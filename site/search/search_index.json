{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\uddec Baysian Inference for Biological ODEs","text":""},{"location":"#bayesian-learning-of-microbial-traits-from-population-time-series-data-a-primer","title":"Bayesian Learning of Microbial Traits from Population Time Series Data: A Primer","text":"<p>Mathematical models are increasingly used to infer traits, interactions, and functional dynamics of microbial systems. The inference process typically begins with a rate-based Ordinary Differential Equation (ODE) model.</p> <p>However, fitting such models to experimental data requires a principled statistical framework that can:</p> <p>Incorporate prior knowledge, Account for measurement noise, and Quantify uncertainty in parameter estimates.</p> <p>Our aim</p> <p>We strive to make the implicit, explicit \u2014 introducing Bayesian inference of ecological ODE models for microbial time series, with a unified workflow in Python (PyMC) and Julia (Turing.jl).</p>"},{"location":"#overview","title":"\ud83d\udcd8 Overview","text":"<p>This repository accompanies our upcoming paper:</p> <p>\u201cBayesian Learning of Microbial Traits from Population Time Series Data: A Primer\u201d Authors: TBD (Link will be posted here when the paper is online.)</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick start","text":"<p>Explore the case studies from the sidebar: - Python (PyMC) \u2192 Case Study 1\u20133 - Julia (Turing.jl) \u2192 Case Study 1\u20132</p> <p>Or jump directly:</p> <ul> <li>Case Study 1 \u2014 Exponential Growth and Death</li> <li>Case Study 2 \u2014 Logistic Growth and Death</li> <li>Case Study 3 \u2014 Monod Growth and Death</li> </ul>"},{"location":"#stack","title":"\ud83e\uddf0 Stack","text":"<ul> <li>Python \u00b7 PyMC \u2014 Probabilistic programming in Python for Bayesian modeling and inference  </li> <li>Julia \u00b7 Turing.jl \u2014 A flexible probabilistic programming language in Julia  </li> </ul>"},{"location":"#contributors","title":"\ud83d\udc69\u200d\ud83d\udd2c Contributors","text":"<p>This primer was developed through a collaborative effort across multiple research groups, combining expertise in microbial ecology, statistical physics, and Bayesian inference.</p> <p>Raunak Dey \u2014 University of Maryland \ud83d\udd17 Website \u00b7 GitHub</p> <p>David Talmy \u2014 University of Tennessee, Knoxville \ud83d\udd17 Website </p> <p>Robert Beach \u2014 University of Tennessee, Knoxville  </p> <p>Kennedi Hambrick \u2014 University of Tennessee, Knoxville  </p> <p>Ioannis Sgouralis \u2014 University of Tennessee, Knoxville \ud83d\udd17 Website</p> <p>Stephen J. Beckett \u2014  University of Maryland </p> <p>Paul Fr\u00e9mont \u2014  University of Maryland </p> <p>David Demory \u2014 CNRS / Universit\u00e9 Paris-Saclay  </p> <p>Eric Carr \u2014 University of Tennessee, Knoxville  </p> <p>Joshua S. Weitz \u2014 University of Maryland  \ud83d\udd17 Website </p> <p>This project connects theory, data, and computation to advance reproducible Bayesian inference for ecological population models.</p>"},{"location":"contributors/","title":"Contributors","text":""},{"location":"contributors/#contributors","title":"\ud83d\udc69\u200d\ud83d\udd2c Contributors","text":"<p>This primer was developed through a collaborative effort across multiple research groups, combining expertise in microbial ecology, statistical physics, and Bayesian inference.</p> <p>Raunak Dey \u2014 University of Maryland \ud83d\udd17 Website \u00b7 GitHub</p> <p>David Talmy \u2014 University of Tennessee, Knoxville \ud83d\udd17 Website </p> <p>Robert Beach \u2014 University of Tennessee, Knoxville  </p> <p>Kennedi Hambrick \u2014 University of Tennessee, Knoxville  </p> <p>Ioannis Sgouralis \u2014 University of Tennessee, Knoxville \ud83d\udd17 Website</p> <p>Stephen J. Beckett \u2014  University of Maryland </p> <p>Paul Fr\u00e9mont \u2014  University of Maryland </p> <p>David Demory \u2014 CNRS / Universit\u00e9 Paris-Saclay  </p> <p>Eric Carr \u2014 University of Tennessee, Knoxville  </p> <p>Joshua S. Weitz \u2014 University of Maryland  \ud83d\udd17 Website </p> <p>This project connects theory, data, and computation to advance reproducible Bayesian inference for ecological population models.</p>"},{"location":"ten-steps/","title":"\ud83e\udded Mini Tutorial: 10 Steps of Bayesian Inference","text":"<p>Individual Bayesian inverse modeling is case-specific, but this page summarizes the common workflow we follow in the primer. We illustrate with simple population models (exponential, logistic, and a Monod resource model).</p> <p>Notation. Data \\(\\mathcal{D}\\), parameters \\(\\theta\\), likelihood \\(\\mathcal{L}(\\mathcal{D}\\mid\\theta)\\), prior \\(P(\\theta)\\), posterior \\(P(\\theta\\mid\\mathcal{D})\\).</p> 1) Visualize the data <p>Begin by plotting time series for each species/variable. - Use log-scaled y-axes when densities span orders of magnitude. - Early visualization guides model choice and highlights measurement scales and noise structure.</p> 2) Choose a dynamical model <p>Qualitative trends inform the structure of your ODEs.</p> <p>Exponential growth &amp; death (Malthusian): $$ \\frac{dP}{dt}=\\mu P, \\qquad \\frac{dP}{dt}=(\\mu-\\delta)P $$ where \\(P\\) is cell density, \\(\\mu\\) growth rate, \\(\\delta\\) death rate.</p> <p>Logistic growth (with death): $$ \\frac{dP}{dt}=rP!\\left(1-\\frac{P}{K}\\right)-\\delta P, \\qquad \\frac{dD}{dt}=\\delta P $$ with intrinsic rate \\(r\\), carrying capacity \\(K\\), live cells \\(P\\), dead cells \\(D\\).</p> <p>Monod (resource-explicit) growth with death: $$ \\begin{aligned} \\frac{dN}{dt}&amp;=-\\frac{Q_N\\,\\mu_{\\max}N}{N+K_S}\\,\\big(P\\times10^6\\big) \\\\ \\frac{dP}{dt}&amp;=\\frac{\\mu_{\\max}N}{N+K_S}\\,P-\\delta P \\\\ \\frac{dD}{dt}&amp;=\\delta P \\end{aligned} $$ where \\(N\\) is nutrient, \\(Q_N\\) cellular quota, \\(\\mu_{\\max}\\) max growth rate, and \\(K_S\\) half-saturation.</p> 3) Define the likelihood <p>The likelihood \\(\\mathcal{L}(\\mathcal{D}\\mid\\theta)\\) measures compatibility of model and data. - Common choice: Gaussian errors (possibly on a log scale) with equal weights across observations. - When variables are latent (unobserved), build the likelihood only over observed variables. - With equal weights, \\(\\log\\mathcal{L}\\) reduces to sum of squared errors plus noise terms.</p> 4) Select parameters to fit <p>Include dynamic parameters, initial conditions, and observation noise. - For strong posterior covariance, consider reparameterization and fit independent parameters only. - Keep units and identifiability in mind.</p> 5) Choose priors <p>Use literature, physics, and bounds to set priors. - Normal / Log-Normal for parameters spanning orders of magnitude. - Uniform for weakly informative ranges. - Half-Normal (or Half-Cauchy) for positive noise scales. - Apply truncation to enforce positivity (e.g., growth/death rates).</p> 6) Pick sampler and ODE solver <ul> <li>For smooth models with gradients, HMC/NUTS is efficient; needs Jacobians/AD support.  </li> <li>For stiff ODEs or tricky gradients, Metropolis (or other gradient-free samplers) with robust solvers (e.g., <code>solve_ivp</code>) can be preferable.  </li> <li>In PyMC, you can use <code>DifferentialEquation</code> for coupled ODE integration within sampling.</li> </ul> 7) Sample the posteriors <p>Bayes\u2019 rule: $$ P(\\theta\\mid\\mathcal{D}) \\propto \\mathcal{L}(\\mathcal{D}\\mid\\theta)\\,P(\\theta) $$ Practical tips: - Prior predictive checks to validate priors before seeing data. - Warm-up/burn-in long enough for adaptation (more for high-dim models). - Draws &amp; chains: target \\(\\ge\\) 1000 effective samples per parameter. - Acceptance rate: tune as needed (e.g., PyMC default ~0.8; can increase toward 0.95 if convergence requires it).</p> 8) Inspect posterior distributions <ul> <li>Use marginals and corner plots to assess shape, multimodality, and correlations.  </li> <li>Strong correlations \u2192 consider reparameterization or model refinement.  </li> <li>Report credible intervals and posterior summaries with context.</li> </ul> 9) Convergence diagnostics <p>Ensure reliable exploration before interpreting results: - \\(\\hat R\\) (Gelman\u2013Rubin) close to 1 (e.g., &lt; 1.01). - Autocorrelation decays quickly (e.g., &lt; 0.1 by ~50 lags). - ESS (bulk &amp; tail) sufficiently large (e.g., &gt; 300).</p> 10) Posterior predictive simulation <p>Propagate posterior draws through the ODE to generate predictive trajectories. - Compare to data for model fit. - Use spread to quantify predictive uncertainty in dynamics.</p>"},{"location":"education/julia/case_study_1/","title":"Case Study 1 \u2014 Birth\u2013death dynamics","text":"In\u00a0[1]: Copied! <pre># For user-defined post processing and plotting functions\ninclude(joinpath(@__DIR__, \"..\", \"..\", \"..\", \"utils\", \"plot_utils.jl\"))\n</pre> # For user-defined post processing and plotting functions include(joinpath(@__DIR__, \"..\", \"..\", \"..\", \"utils\", \"plot_utils.jl\")) <pre>plot_posterior_states_stacked (generic function with 1 method)</pre> <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using Turing. A Julia software package allowing you to fit complex models with ease.</p> In\u00a0[2]: Copied! <pre>## Cell 1 ##\n\nusing CSV, DataFrames\n\ndf = CSV.read(\"../../../case_study_1/python/data/phaeocystis_control.csv\", DataFrame)\n\ntimes    = df.times\ny_obs    = df.cells\nlog_y_obs = log.(y_obs .+ 1e-9)\n</pre> ## Cell 1 ##  using CSV, DataFrames  df = CSV.read(\"../../../case_study_1/python/data/phaeocystis_control.csv\", DataFrame)  times    = df.times y_obs    = df.cells log_y_obs = log.(y_obs .+ 1e-9) <pre>14-element Vector{Float64}:\n 14.304090572782945\n 14.352003928478844\n 14.375126345899698\n 14.386490104550012\n 14.488455031206701\n 14.207552645740298\n 14.580978400103845\n 14.457364444136669\n 14.508657738524219\n 14.797589030376432\n 14.880221294956703\n 14.71159858252091\n 14.73180128983843\n 15.112973705377549</pre> <p>First we will setup the exponential ODE that will adapt to the data. To do this, you will use variables that Turing gives you by 'default'. Let me explain. Notice the 4 parameters being passed in. du, u, p, and t. du is to hold your ODE  system. u are you starting states. p are your parameters. t are your times. Turing and Julia will handle the passing of these under the hood. Just make sure you declare them correctly and in proper order. If you have states 1 and 2 then EVERYWHERE you make sure those states are passed in that exact order. Same with your parameters. Lots of things happen under the hood here for you and things are not always 'explicitly' passed, so make sure when you do pass things your order is always preserved.</p> <p>mum: Growth Rate y: Initial Value</p> <p>$$ \\frac{dy}{dt} = \\mu y $$</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 2 ##\n\nfunction ode(du, u, p, t)\n    mum = p[1]\n    y = u[1]\n    du[1] = mum * y\n    return nothing\nend\n</pre> ## Cell 2 ##  function ode(du, u, p, t)     mum = p[1]     y = u[1]     du[1] = mum * y     return nothing end <pre>ode (generic function with 1 method)</pre> <p>This is where we create the MCMC 'loop': Sample Priors -&gt; Estimate ODE @ times -&gt; check fit</p> <p>We declare the function to be probabilistic model named fit_ode using the @model tag. The inputs are as follows.</p> <p>log_y_obs: Our log transformed observed data. times: The times at which the ODE is evaluated. prob: The prebuilt ODE.</p> <p>Next, priors. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. For $\\mu$ we chose a truncatednormal prior. This says, \"we believe there is a normal distribution about some x, however, it cannot be below some y or above some z\". For N0, we chose a lognormal distribution. This says, \"we believe our value is around some x given a normalesque strictly positive distribution\". For sigma, we chose a halfnormal distribution. This is usually used for your error or standard deviation since neither can be negative and you can tune how quickly it tails off. Below are the priors visualized.</p> <p></p> <p>Next is solving the ODE. This will be a 2 step process.</p> <p>1: remake your problem with newly sampled elements. The inputs are as follows: prob: Premade ODE system. u0: State priors. p: Parameter priors. tspan: first and last of your observed times.</p> <p>2: solve the system at specific time points. the inputs are as follows: pr: remade system Tsit5(): Default. saveat: observed times. abstol / restol: Absolute tolerance / Relative tolerance. Default value chosen.</p> <p>note: See how when you're passing in values to this julia function there are both , and ;? In julia this is to mark the difference between positional arguments (before ;) and keyword arguments (after ;).</p> <p>The final part of the model is getting the likelihood (or fit if you prefer) of the model to the observed data. It's a bit layered here so we'll break it down part by part.</p> <p>log.(Array(sol)[1, :] .+ 1e-9): sol is the solver output. We force it into an array and grab the first row using the [1, :] indexing. Remember to be intentional with your indexing. Finally, we log scale the prediction. log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)): This is the actual likelihood calculation. Ensure the variable preceding the ~ here is the same as the one passed into fit_ode.</p> In\u00a0[5]: Copied! <pre>## Cell 3 ##\n\nusing Turing\n@model function fit_ode(log_y_obs, times, prob)\n    mum   ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)\n    N0    ~ LogNormal(log(1_630_000), 0.1)\n    sigma ~ truncated(Normal(0, 1), 0, Inf)    \n    \n    pr = remake(prob;\n                p = [mum],\n                u0 = [N0],\n                tspan = (times[1], times[end]))\n\n    sol = solve(pr, Tsit5();\n                abstol = 1e-6, reltol = 1e-6,\n                saveat = times)\n\n    log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)\n    log_y_obs ~ arraydist(Normal.(log_y_pred, sigma))\nend\n</pre> ## Cell 3 ##  using Turing @model function fit_ode(log_y_obs, times, prob)     mum   ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)     N0    ~ LogNormal(log(1_630_000), 0.1)     sigma ~ truncated(Normal(0, 1), 0, Inf)              pr = remake(prob;                 p = [mum],                 u0 = [N0],                 tspan = (times[1], times[end]))      sol = solve(pr, Tsit5();                 abstol = 1e-6, reltol = 1e-6,                 saveat = times)      log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)     log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)) end <pre>fit_ode (generic function with 2 methods)</pre> <p>This block is where we declare the ODE with initial values, call the model, and run the sampling.</p> <p>To start,</p> <p>u0: Initial state value(s). Make the inital value(s) from the observed data. p: Initial parameter value(s). Make these a value in the specified range of the prior you declared. Here we chose the center. tspan:  first and last values from your observed times. prob: The prebuild ODE.</p> <p>Next is the call to run the sampling.</p> <p>model: The model created above. NUTS(1000, .95): This is telling you to use the NUTS sampler. 1000 is the amount of 'warm up' iterations it will do under the hood. .95 is the acceptance rate of the 'improved' solutions. MCMCSerial(): Tells the solver to run the chains sequentially. 1000: number of posterior samples to be generated. 3: number of chains. progress: either hides or shows the progress bar. Up to you.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 4 ##\n\nusing DifferentialEquations\n\nu0 = [y_obs[1]]\np = [0.5]\ntspan = (times[1], times[end])\nprob = ODEProblem(ode, u0, tspan, p)\n\nmodel = fit_ode(log_y_obs, times, prob)\nchain = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)\n</pre> ## Cell 4 ##  using DifferentialEquations  u0 = [y_obs[1]] p = [0.5] tspan = (times[1], times[end]) prob = ODEProblem(ode, u0, tspan, p)  model = fit_ode(log_y_obs, times, prob) chain = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false) <pre>\nMethodError: no method matching *(::Vector{Float64}, ::Vector{Float64})\n\nThe function `*` exists, but no method is defined for this combination of argument types.\n\n\n\nClosest candidates are:\n\n  *(::Any, ::Any, !Matched::Any, !Matched::Any...)\n\n   @ Base operators.jl:596\n\n  *(!Matched::ChainRulesCore.NotImplemented, ::Any)\n\n   @ ChainRulesCore C:\\Users\\Whisk\\.julia\\packages\\ChainRulesCore\\Vsbj9\\src\\tangent_arithmetic.jl:37\n\n  *(::Any, !Matched::ChainRulesCore.NotImplemented)\n\n   @ ChainRulesCore C:\\Users\\Whisk\\.julia\\packages\\ChainRulesCore\\Vsbj9\\src\\tangent_arithmetic.jl:38\n\n  ...\n\n\n\n\n\nStacktrace:\n\n  [1] ode(du::Vector{Float64}, u::Vector{Float64}, p::Vector{Float64}, t::Float64)\n\n    @ Main d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W4sZmlsZQ==.jl:6\n\n  [2] (::SciMLBase.Void{typeof(ode)})(::Vector{Float64}, ::Vararg{Any})\n\n    @ SciMLBase C:\\Users\\Whisk\\.julia\\packages\\SciMLBase\\2Ovel\\src\\utils.jl:486\n\n  [3] (::FunctionWrappers.CallWrapper{Nothing})(f::SciMLBase.Void{typeof(ode)}, arg1::Vector{Float64}, arg2::Vector{Float64}, arg3::Vector{Float64}, arg4::Float64)\n\n    @ FunctionWrappers C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappers\\Q5cBx\\src\\FunctionWrappers.jl:65\n\n  [4] macro expansion\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappers\\Q5cBx\\src\\FunctionWrappers.jl:137 [inlined]\n\n  [5] do_ccall\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappers\\Q5cBx\\src\\FunctionWrappers.jl:125 [inlined]\n\n  [6] FunctionWrapper\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappers\\Q5cBx\\src\\FunctionWrappers.jl:144 [inlined]\n\n  [7] _call\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappersWrappers\\9XR0m\\src\\FunctionWrappersWrappers.jl:12 [inlined]\n\n  [8] FunctionWrappersWrapper\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappersWrappers\\9XR0m\\src\\FunctionWrappersWrappers.jl:10 [inlined]\n\n  [9] ODEFunction\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\SciMLBase\\2Ovel\\src\\scimlfunctions.jl:2592 [inlined]\n\n [10] initialize!(integrator::OrdinaryDiffEqCore.ODEIntegrator{Tsit5{typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, true, Vector{Float64}, Nothing, Float64, Vector{Float64}, Float64, Float64, Float64, Float64, Vector{Vector{Float64}}, ODESolution{Float64, 2, Vector{Vector{Float64}}, Nothing, Nothing, Vector{Float64}, Vector{Vector{Vector{Float64}}}, Nothing, ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, Tsit5{typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, OrdinaryDiffEqCore.InterpolationData{ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, Vector{Vector{Float64}}, Vector{Float64}, Vector{Vector{Vector{Float64}}}, Nothing, OrdinaryDiffEqTsit5.Tsit5Cache{Vector{Float64}, Vector{Float64}, Vector{Float64}, typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, Nothing}, SciMLBase.DEStats, Nothing, Nothing, Nothing, Nothing}, ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, OrdinaryDiffEqTsit5.Tsit5Cache{Vector{Float64}, Vector{Float64}, Vector{Float64}, typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, OrdinaryDiffEqCore.DEOptions{Float64, Float64, Float64, Float64, PIController{Rational{Int64}}, typeof(DiffEqBase.ODE_DEFAULT_NORM), typeof(LinearAlgebra.opnorm), Nothing, CallbackSet{Tuple{}, Tuple{}}, typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), DataStructures.BinaryHeap{Float64, DataStructures.FasterForward}, DataStructures.BinaryHeap{Float64, DataStructures.FasterForward}, Nothing, Nothing, Int64, Tuple{}, Vector{Float64}, Tuple{}}, Vector{Float64}, Float64, Nothing, OrdinaryDiffEqCore.DefaultInit, Nothing}, cache::OrdinaryDiffEqTsit5.Tsit5Cache{Vector{Float64}, Vector{Float64}, Vector{Float64}, typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False})\n\n    @ OrdinaryDiffEqTsit5 C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqTsit5\\o07IY\\src\\tsit_perform_step.jl:175\n\n [11] __init(prob::ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, alg::Tsit5{typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, timeseries_init::Tuple{}, ts_init::Tuple{}, ks_init::Tuple{}; saveat::Vector{Float64}, tstops::Tuple{}, d_discontinuities::Tuple{}, save_idxs::Nothing, save_everystep::Bool, save_on::Bool, save_start::Bool, save_end::Nothing, callback::Nothing, dense::Bool, calck::Bool, dt::Float64, dtmin::Float64, dtmax::Float64, force_dtmin::Bool, adaptive::Bool, gamma::Rational{Int64}, abstol::Float64, reltol::Float64, qmin::Rational{Int64}, qmax::Int64, qsteady_min::Int64, qsteady_max::Int64, beta1::Nothing, beta2::Nothing, qoldinit::Rational{Int64}, controller::Nothing, fullnormalize::Bool, failfactor::Int64, maxiters::Int64, internalnorm::typeof(DiffEqBase.ODE_DEFAULT_NORM), internalopnorm::typeof(LinearAlgebra.opnorm), isoutofdomain::typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), unstable_check::typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), verbose::Bool, timeseries_errors::Bool, dense_errors::Bool, advance_to_tstop::Bool, stop_at_next_tstop::Bool, initialize_save::Bool, progress::Bool, progress_steps::Int64, progress_name::String, progress_message::typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), progress_id::Symbol, userdata::Nothing, allow_extrapolation::Bool, initialize_integrator::Bool, alias::ODEAliasSpecifier, initializealg::OrdinaryDiffEqCore.DefaultInit, kwargs::@Kwargs{})\n\n    @ OrdinaryDiffEqCore C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqCore\\f6XR0\\src\\solve.jl:577\n\n [12] __init (repeats 2 times)\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqCore\\f6XR0\\src\\solve.jl:11 [inlined]\n\n [13] #__solve#62\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqCore\\f6XR0\\src\\solve.jl:6 [inlined]\n\n [14] __solve\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqCore\\f6XR0\\src\\solve.jl:1 [inlined]\n\n [15] solve_call(_prob::ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, args::Tsit5{typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}; merge_callbacks::Bool, kwargshandle::Nothing, kwargs::@Kwargs{abstol::Float64, reltol::Float64, saveat::Vector{Float64}})\n\n    @ DiffEqBase C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:127\n\n [16] solve_call\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:84 [inlined]\n\n [17] #solve_up#40\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:681 [inlined]\n\n [18] solve_up\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:658 [inlined]\n\n [19] #solve#38\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:553 [inlined]\n\n [20] fit_ode(__model__::DynamicPPL.Model{typeof(fit_ode), (:log_y_obs, :times, :prob), (), (), Tuple{Vector{Float64}, Vector{Float64}, ODEProblem{Int64, Tuple{Float64, Float64}, true, Float64, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(ode), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}}, Tuple{}, DynamicPPL.SamplingContext{DynamicPPL.SampleFromPrior, DynamicPPL.DefaultContext, Random.TaskLocalRNG}}, __varinfo__::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}}, DynamicPPL.AccumulatorTuple{1, @NamedTuple{Debug::DynamicPPL.DebugUtils.DebugAccumulator}}}, log_y_obs::Vector{Float64}, times::Vector{Float64}, prob::ODEProblem{Int64, Tuple{Float64, Float64}, true, Float64, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(ode), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem})\n\n    @ Main d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_W6sZmlsZQ==.jl:14\n\n [21] _evaluate!!\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\model.jl:921 [inlined]\n\n [22] evaluate_threadunsafe!!\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\model.jl:887 [inlined]\n\n [23] check_model_and_trace(model::DynamicPPL.Model{typeof(fit_ode), (:log_y_obs, :times, :prob), (), (), Tuple{Vector{Float64}, Vector{Float64}, ODEProblem{Int64, Tuple{Float64, Float64}, true, Float64, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(ode), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}}, Tuple{}, DynamicPPL.SamplingContext{DynamicPPL.SampleFromPrior, DynamicPPL.DefaultContext, Random.TaskLocalRNG}}, varinfo::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}; error_on_failure::Bool)\n\n    @ DynamicPPL.DebugUtils C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\debug_utils.jl:429\n\n [24] check_model_and_trace\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\debug_utils.jl:419 [inlined]\n\n [25] check_model\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\debug_utils.jl:452 [inlined]\n\n [26] _check_model\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:6 [inlined]\n\n [27] _check_model\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:9 [inlined]\n\n [28] #sample#107\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:57 [inlined]\n\n [29] sample\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:47 [inlined]\n\n [30] #sample#106\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:42 [inlined]\n\n [31] top-level scope\n\n    @ d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X11sZmlsZQ==.jl:11</pre> <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[6]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :mum   =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),\n    :N0    =&gt; LogNormal(log(1_630_000.0), 0.1),\n    :sigma =&gt; truncated(Normal(0, 1.0), 0.0, Inf)\n)\n\norder = [:mum, :N0, :sigma]\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :mum   =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),     :N0    =&gt; LogNormal(log(1_630_000.0), 0.1),     :sigma =&gt; truncated(Normal(0, 1.0), 0.0, Inf) )  order = [:mum, :N0, :sigma] plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities <pre>\nUndefVarError: `chain` not defined in `Main`\n\nSuggestion: check for spelling errors or missing imports.\n\nHint: a global variable of this name may be made accessible by importing IterTools in the current active module Main\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X13sZmlsZQ==.jl:8</pre> <p>Now we interpret the results. Remember the ODE's from before.</p> <p>$$ \\frac{dy}{dt} = \\mu y $$</p> <p>Take note of x-axis values at the peaks of the PDFs and the variables they represent. The x-axis values at those peaks are what the model found to be the optimal value for the equation. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space well enough.</p> In\u00a0[7]: Copied! <pre>init_syms = [:N0]\nparam_syms = [:mum]\nt_obs = times\ny_obs = y_obs\n\nplt = overlay_posterior_on_observed(\n    chain, ode, t_obs, y_obs;\n    init_syms=init_syms,\n    param_syms=param_syms,\n    which_states=[1],     # choose states to plot\n    n_draws=150,            # how many posterior paths to overlay\n    plot_ribbon=true,       # median \u00b1 CI band\n    ribbon_q=(0.1, 0.9),    # CI limits\n    legend=:topleft,\n    logy=false\n)\ndisplay(plt)\n</pre> init_syms = [:N0] param_syms = [:mum] t_obs = times y_obs = y_obs  plt = overlay_posterior_on_observed(     chain, ode, t_obs, y_obs;     init_syms=init_syms,     param_syms=param_syms,     which_states=[1],     # choose states to plot     n_draws=150,            # how many posterior paths to overlay     plot_ribbon=true,       # median \u00b1 CI band     ribbon_q=(0.1, 0.9),    # CI limits     legend=:topleft,     logy=false ) display(plt) <pre>\nUndefVarError: `chain` not defined in `Main`\n\nSuggestion: check for spelling errors or missing imports.\n\nHint: a global variable of this name may be made accessible by importing IterTools in the current active module Main\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X15sZmlsZQ==.jl:6</pre> <p>This plot is showing how well the range of estimated chains fit the observed data.</p> <p>CONGRATULATIONS! you just ran you first MCMC!</p> In\u00a0[8]: Copied! <pre>## Cell 5 ##\n\nusing CSV, DataFrames\n\ndf = CSV.read(\"../../../case_study_1/python/data/phaeocystis_control.csv\", DataFrame)\n\ntimes    = df.times\ny_obs    = df.cells\nlog_y_obs = log.(y_obs .+ 1e-9)\n</pre> ## Cell 5 ##  using CSV, DataFrames  df = CSV.read(\"../../../case_study_1/python/data/phaeocystis_control.csv\", DataFrame)  times    = df.times y_obs    = df.cells log_y_obs = log.(y_obs .+ 1e-9) <pre>14-element Vector{Float64}:\n 14.304090572782945\n 14.352003928478844\n 14.375126345899698\n 14.386490104550012\n 14.488455031206701\n 14.207552645740298\n 14.580978400103845\n 14.457364444136669\n 14.508657738524219\n 14.797589030376432\n 14.880221294956703\n 14.71159858252091\n 14.73180128983843\n 15.112973705377549</pre> <p>$$ \\frac{dy}{dt} = (\\mu - \\delta)\\, y $$</p> <p>Here we are simply subtracting a delta from mu so that delta can simulate death. In short, we don't always need death (or something else comperable) data. We can simply make our ODE expressive about the data!</p> In\u00a0[9]: Copied! <pre>## Cell 6 ##\n\nfunction ode(du, u, p, t)\n    mum, delta = p\n    y = u\n    du[1] = (mum - delta) * y\n    return nothing\nend\n</pre> ## Cell 6 ##  function ode(du, u, p, t)     mum, delta = p     y = u     du[1] = (mum - delta) * y     return nothing end <pre>ode (generic function with 1 method)</pre> <p>Here is the model. Everything here should make sense. If something is confusing see the explination above cell 4. Still note that we have two likelihoods at the bottom since we are estimating two states.</p> In\u00a0[10]: Copied! <pre>## Cell 7 ##\n\nusing Turing\n@model function fit_ode(log_y_obs, times, prob)\n    mum   ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)\n    delta ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)\n    N0    ~ LogNormal(log(1_630_000), 0.1)\n    sigma ~ truncated(Normal(0, 1), 0, Inf)   \n    \n    pr = remake(prob;\n                p = [mum, delta],\n                u0 = [N0],\n                tspan = (times[1], times[end]))\n    \n    # solve exactly at data times\n    sol = solve(pr, Tsit5();  \n                abstol = 1e-6, reltol = 1e-6,\n                saveat = times)\n\n    # likelihood at observation times\n    log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)\n    log_y_obs ~ arraydist(Normal.(log_y_pred, sigma))\nend\n</pre> ## Cell 7 ##  using Turing @model function fit_ode(log_y_obs, times, prob)     mum   ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)     delta ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)     N0    ~ LogNormal(log(1_630_000), 0.1)     sigma ~ truncated(Normal(0, 1), 0, Inf)             pr = remake(prob;                 p = [mum, delta],                 u0 = [N0],                 tspan = (times[1], times[end]))          # solve exactly at data times     sol = solve(pr, Tsit5();                   abstol = 1e-6, reltol = 1e-6,                 saveat = times)      # likelihood at observation times     log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)     log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)) end <pre>fit_ode (generic function with 2 methods)</pre> <p>Everything here is the same as before! See explination above cell 5 if you forgot something.</p> In\u00a0[11]: Copied! <pre>## Cell 8 ##\n\nusing DifferentialEquations\n\nu0 = [y_obs[1]]\np = [0.5, 0.5]\ntspan = (times[1], times[end])\nprob = ODEProblem(ode, u0, tspan, p)\n\nmodel    = fit_ode(log_y_obs, times, prob)\nchain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 4; progress=false)\n</pre> ## Cell 8 ##  using DifferentialEquations  u0 = [y_obs[1]] p = [0.5, 0.5] tspan = (times[1], times[end]) prob = ODEProblem(ode, u0, tspan, p)  model    = fit_ode(log_y_obs, times, prob) chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 4; progress=false) <pre>\nMethodError: Cannot `convert` an object of type Vector{Float64} to an object of type Float64\n\nThe function `convert` exists, but no method is defined for this combination of argument types.\n\n\n\nClosest candidates are:\n\n  convert(::Type{Float64}, !Matched::AbsoluteLength)\n\n   @ Measures C:\\Users\\Whisk\\.julia\\packages\\Measures\\PKOxJ\\src\\length.jl:12\n\n  convert(::Type{T}, !Matched::CartesianIndex{1}) where T&lt;:Number\n\n   @ Base multidimensional.jl:136\n\n  convert(::Type{&lt;:Number}, !Matched::ChainRulesCore.NotImplemented)\n\n   @ ChainRulesCore C:\\Users\\Whisk\\.julia\\packages\\ChainRulesCore\\Vsbj9\\src\\tangent_types\\notimplemented.jl:68\n\n  ...\n\n\n\n\n\nStacktrace:\n\n  [1] setindex!(A::Vector{Float64}, x::Vector{Float64}, i::Int64)\n\n    @ Base .\\array.jl:987\n\n  [2] ode(du::Vector{Float64}, u::Vector{Float64}, p::Vector{Float64}, t::Float64)\n\n    @ Main d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X23sZmlsZQ==.jl:6\n\n  [3] (::SciMLBase.Void{typeof(ode)})(::Vector{Float64}, ::Vararg{Any})\n\n    @ SciMLBase C:\\Users\\Whisk\\.julia\\packages\\SciMLBase\\2Ovel\\src\\utils.jl:486\n\n  [4] (::FunctionWrappers.CallWrapper{Nothing})(f::SciMLBase.Void{typeof(ode)}, arg1::Vector{Float64}, arg2::Vector{Float64}, arg3::Vector{Float64}, arg4::Float64)\n\n    @ FunctionWrappers C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappers\\Q5cBx\\src\\FunctionWrappers.jl:65\n\n  [5] macro expansion\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappers\\Q5cBx\\src\\FunctionWrappers.jl:137 [inlined]\n\n  [6] do_ccall\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappers\\Q5cBx\\src\\FunctionWrappers.jl:125 [inlined]\n\n  [7] FunctionWrapper\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappers\\Q5cBx\\src\\FunctionWrappers.jl:144 [inlined]\n\n  [8] _call\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappersWrappers\\9XR0m\\src\\FunctionWrappersWrappers.jl:12 [inlined]\n\n  [9] FunctionWrappersWrapper\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\FunctionWrappersWrappers\\9XR0m\\src\\FunctionWrappersWrappers.jl:10 [inlined]\n\n [10] ODEFunction\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\SciMLBase\\2Ovel\\src\\scimlfunctions.jl:2592 [inlined]\n\n [11] initialize!(integrator::OrdinaryDiffEqCore.ODEIntegrator{Tsit5{typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, true, Vector{Float64}, Nothing, Float64, Vector{Float64}, Float64, Float64, Float64, Float64, Vector{Vector{Float64}}, ODESolution{Float64, 2, Vector{Vector{Float64}}, Nothing, Nothing, Vector{Float64}, Vector{Vector{Vector{Float64}}}, Nothing, ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, Tsit5{typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, OrdinaryDiffEqCore.InterpolationData{ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, Vector{Vector{Float64}}, Vector{Float64}, Vector{Vector{Vector{Float64}}}, Nothing, OrdinaryDiffEqTsit5.Tsit5Cache{Vector{Float64}, Vector{Float64}, Vector{Float64}, typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, Nothing}, SciMLBase.DEStats, Nothing, Nothing, Nothing, Nothing}, ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, OrdinaryDiffEqTsit5.Tsit5Cache{Vector{Float64}, Vector{Float64}, Vector{Float64}, typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, OrdinaryDiffEqCore.DEOptions{Float64, Float64, Float64, Float64, PIController{Rational{Int64}}, typeof(DiffEqBase.ODE_DEFAULT_NORM), typeof(LinearAlgebra.opnorm), Nothing, CallbackSet{Tuple{}, Tuple{}}, typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), DataStructures.BinaryHeap{Float64, DataStructures.FasterForward}, DataStructures.BinaryHeap{Float64, DataStructures.FasterForward}, Nothing, Nothing, Int64, Tuple{}, Vector{Float64}, Tuple{}}, Vector{Float64}, Float64, Nothing, OrdinaryDiffEqCore.DefaultInit, Nothing}, cache::OrdinaryDiffEqTsit5.Tsit5Cache{Vector{Float64}, Vector{Float64}, Vector{Float64}, typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False})\n\n    @ OrdinaryDiffEqTsit5 C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqTsit5\\o07IY\\src\\tsit_perform_step.jl:175\n\n [12] __init(prob::ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, alg::Tsit5{typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}, timeseries_init::Tuple{}, ts_init::Tuple{}, ks_init::Tuple{}; saveat::Vector{Float64}, tstops::Tuple{}, d_discontinuities::Tuple{}, save_idxs::Nothing, save_everystep::Bool, save_on::Bool, save_start::Bool, save_end::Nothing, callback::Nothing, dense::Bool, calck::Bool, dt::Float64, dtmin::Float64, dtmax::Float64, force_dtmin::Bool, adaptive::Bool, gamma::Rational{Int64}, abstol::Float64, reltol::Float64, qmin::Rational{Int64}, qmax::Int64, qsteady_min::Int64, qsteady_max::Int64, beta1::Nothing, beta2::Nothing, qoldinit::Rational{Int64}, controller::Nothing, fullnormalize::Bool, failfactor::Int64, maxiters::Int64, internalnorm::typeof(DiffEqBase.ODE_DEFAULT_NORM), internalopnorm::typeof(LinearAlgebra.opnorm), isoutofdomain::typeof(DiffEqBase.ODE_DEFAULT_ISOUTOFDOMAIN), unstable_check::typeof(DiffEqBase.ODE_DEFAULT_UNSTABLE_CHECK), verbose::Bool, timeseries_errors::Bool, dense_errors::Bool, advance_to_tstop::Bool, stop_at_next_tstop::Bool, initialize_save::Bool, progress::Bool, progress_steps::Int64, progress_name::String, progress_message::typeof(DiffEqBase.ODE_DEFAULT_PROG_MESSAGE), progress_id::Symbol, userdata::Nothing, allow_extrapolation::Bool, initialize_integrator::Bool, alias::ODEAliasSpecifier, initializealg::OrdinaryDiffEqCore.DefaultInit, kwargs::@Kwargs{})\n\n    @ OrdinaryDiffEqCore C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqCore\\f6XR0\\src\\solve.jl:577\n\n [13] __init (repeats 2 times)\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqCore\\f6XR0\\src\\solve.jl:11 [inlined]\n\n [14] #__solve#62\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqCore\\f6XR0\\src\\solve.jl:6 [inlined]\n\n [15] __solve\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\OrdinaryDiffEqCore\\f6XR0\\src\\solve.jl:1 [inlined]\n\n [16] solve_call(_prob::ODEProblem{Vector{Float64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, FunctionWrappersWrappers.FunctionWrappersWrapper{Tuple{FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{Float64}, Vector{Float64}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Float64}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}, FunctionWrappers.FunctionWrapper{Nothing, Tuple{Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}, Vector{Float64}, ForwardDiff.Dual{ForwardDiff.Tag{DiffEqBase.OrdinaryDiffEqTag, Float64}, Float64, 1}}}}, false}, LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}, args::Tsit5{typeof(OrdinaryDiffEqCore.trivial_limiter!), typeof(OrdinaryDiffEqCore.trivial_limiter!), Static.False}; merge_callbacks::Bool, kwargshandle::Nothing, kwargs::@Kwargs{abstol::Float64, reltol::Float64, saveat::Vector{Float64}})\n\n    @ DiffEqBase C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:127\n\n [17] solve_call\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:84 [inlined]\n\n [18] #solve_up#40\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:681 [inlined]\n\n [19] solve_up\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:658 [inlined]\n\n [20] #solve#38\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DiffEqBase\\Rfwzp\\src\\solve.jl:553 [inlined]\n\n [21] fit_ode(__model__::DynamicPPL.Model{typeof(fit_ode), (:log_y_obs, :times, :prob), (), (), Tuple{Vector{Float64}, Vector{Float64}, ODEProblem{Vector{Int64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(ode), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}}, Tuple{}, DynamicPPL.SamplingContext{DynamicPPL.SampleFromPrior, DynamicPPL.DefaultContext, Random.TaskLocalRNG}}, __varinfo__::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}}, DynamicPPL.AccumulatorTuple{1, @NamedTuple{Debug::DynamicPPL.DebugUtils.DebugAccumulator}}}, log_y_obs::Vector{Float64}, times::Vector{Float64}, prob::ODEProblem{Vector{Int64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(ode), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem})\n\n    @ Main d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X25sZmlsZQ==.jl:16\n\n [22] _evaluate!!\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\model.jl:921 [inlined]\n\n [23] evaluate_threadunsafe!!\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\model.jl:887 [inlined]\n\n [24] check_model_and_trace(model::DynamicPPL.Model{typeof(fit_ode), (:log_y_obs, :times, :prob), (), (), Tuple{Vector{Float64}, Vector{Float64}, ODEProblem{Vector{Int64}, Tuple{Float64, Float64}, true, Vector{Float64}, ODEFunction{true, SciMLBase.AutoSpecialize, typeof(ode), LinearAlgebra.UniformScaling{Bool}, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, Nothing, typeof(SciMLBase.DEFAULT_OBSERVED), Nothing, Nothing, Nothing, Nothing}, @Kwargs{}, SciMLBase.StandardODEProblem}}, Tuple{}, DynamicPPL.SamplingContext{DynamicPPL.SampleFromPrior, DynamicPPL.DefaultContext, Random.TaskLocalRNG}}, varinfo::DynamicPPL.VarInfo{DynamicPPL.Metadata{Dict{AbstractPPL.VarName, Int64}, Vector{Distribution}, Vector{AbstractPPL.VarName}, Vector{Real}}, DynamicPPL.AccumulatorTuple{3, @NamedTuple{LogPrior::DynamicPPL.LogPriorAccumulator{Float64}, LogJacobian::DynamicPPL.LogJacobianAccumulator{Float64}, LogLikelihood::DynamicPPL.LogLikelihoodAccumulator{Float64}}}}; error_on_failure::Bool)\n\n    @ DynamicPPL.DebugUtils C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\debug_utils.jl:429\n\n [25] check_model_and_trace\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\debug_utils.jl:419 [inlined]\n\n [26] check_model\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\DynamicPPL\\bXCZJ\\src\\debug_utils.jl:452 [inlined]\n\n [27] _check_model\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:6 [inlined]\n\n [28] _check_model\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:9 [inlined]\n\n [29] #sample#107\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:57 [inlined]\n\n [30] sample\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:47 [inlined]\n\n [31] #sample#106\n\n    @ C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\abstractmcmc.jl:42 [inlined]\n\n [32] top-level scope\n\n    @ d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X30sZmlsZQ==.jl:11</pre> In\u00a0[12]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :mum   =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),\n    :delta =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),\n    :N0    =&gt; LogNormal(log(1_630_000.0), 0.1),\n    :sigma =&gt; truncated(Normal(0, 1.0), 0.0, Inf)\n)\n\norder = [:mum, :delta, :N0, :sigma]\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :mum   =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),     :delta =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),     :N0    =&gt; LogNormal(log(1_630_000.0), 0.1),     :sigma =&gt; truncated(Normal(0, 1.0), 0.0, Inf) )  order = [:mum, :delta, :N0, :sigma] plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities <pre>\nUndefVarError: `chain` not defined in `Main`\n\nSuggestion: check for spelling errors or missing imports.\n\nHint: a global variable of this name may be made accessible by importing IterTools in the current active module Main\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X31sZmlsZQ==.jl:9</pre> In\u00a0[13]: Copied! <pre>init_syms = [:N0]\nparam_syms = [:mum, :delta]\ny_obs = y_obs\nt_obs = times\n\nplt = overlay_posterior_on_observed(\n    chain, ode, t_obs, y_obs;\n    init_syms=init_syms,\n    param_syms=param_syms,\n    which_states=[1],     # choose states to plot\n    n_draws=150,            # how many posterior paths to overlay\n    plot_ribbon=true,       # median \u00b1 CI band\n    ribbon_q=(0.1, 0.9),    # CI limits\n    legend=:topleft,\n    logy=false\n)\ndisplay(plt)\n</pre> init_syms = [:N0] param_syms = [:mum, :delta] y_obs = y_obs t_obs = times  plt = overlay_posterior_on_observed(     chain, ode, t_obs, y_obs;     init_syms=init_syms,     param_syms=param_syms,     which_states=[1],     # choose states to plot     n_draws=150,            # how many posterior paths to overlay     plot_ribbon=true,       # median \u00b1 CI band     ribbon_q=(0.1, 0.9),    # CI limits     legend=:topleft,     logy=false ) display(plt) <pre>\nUndefVarError: `chain` not defined in `Main`\n\nSuggestion: check for spelling errors or missing imports.\n\nHint: a global variable of this name may be made accessible by importing IterTools in the current active module Main\n\n\n\nStacktrace:\n\n [1] top-level scope\n\n   @ d:\\a-work-folder\\Modeling\\MCMCwithODEs_primer\\docs\\education\\julia\\jl_notebook_cell_df34fa98e69747e1a8f8a730347b8e2f_X32sZmlsZQ==.jl:6</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"education/julia/case_study_1/#case-study-1-exponential-growth","title":"Case Study 1: Exponential Growth\u00b6","text":"<p>We are going to establish an exponential model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! We will read our data from a CSV into a DataFrame. It's also smart to handle any necessary data conversions during this step. You may not need to and that's just fine! It all depends on what you are looking for.</p> <p>Note: The \"+ 1e-9\" is merely a fail-safe against divide-by-zero errors when doing a log transform. Adding a value infitesimally near 0 makes no impact on the data itself and prevents the aformentioned error. Good quick trick to know!</p>"},{"location":"education/julia/case_study_1/#case-study-1-exponential-growth-and-death","title":"Case Study 1: Exponential Growth and Death\u00b6","text":"<p>Now on to a more complex model. We will be using the same data as before.</p>"},{"location":"education/julia/case_study_2/","title":"Case Study 2 \u2014 Logistic growth and death","text":"In\u00a0[\u00a0]: Copied! <pre># For user-defined post processing and plotting functions\ninclude(joinpath(@__DIR__, \"..\", \"..\", \"..\", \"utils\", \"plot_utils.jl\"))\n</pre> # For user-defined post processing and plotting functions include(joinpath(@__DIR__, \"..\", \"..\", \"..\", \"utils\", \"plot_utils.jl\")) <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using Turing. A Julia software package allowing you to fit complex models with ease.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 1 ## \n\nusing CSV, DataFrames\n\n# from your data-prep cell\ndf = CSV.read(\"../../../case_study_2/python/data/total_cells.csv\", DataFrame)\n\ntimes    = df[end-14:end, :1]\ny_obs    = df[end-14:end, :2] * 1e6\nlog_y_obs = log.(y_obs .+ 1e-9)\n</pre> ## Cell 1 ##   using CSV, DataFrames  # from your data-prep cell df = CSV.read(\"../../../case_study_2/python/data/total_cells.csv\", DataFrame)  times    = df[end-14:end, :1] y_obs    = df[end-14:end, :2] * 1e6 log_y_obs = log.(y_obs .+ 1e-9) <p>First we will setup the logistic ODE that will adapt to the data. To do this, you will use variables that Turing gives you by 'default'. Let me explain. Notice the 4 parameters being passed in. du, u, p, and t. du is to hold your ODE system. u are you starting states. p are your parameters. t are your times. Turing and Julia will handle the passing of these under the hood. Just make sure you declare them correctly and in proper order. If you have states 1 and 2 then EVERYWHERE you make sure those states are passed in that exact order. Same with your parameters. Lots of things happen under the hood here for you and things are not always 'explicitly' passed, so make sure when you do pass things your order is always preserved.</p> <p>P: Initial Value r: Growith Rate K: Carrying Capacity</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P $$</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 2 ##\n\nfunction ode(du, u, p, t)\n    P = u\n    r, K = p\n\n    du[1] = r * (1 - P / K) * P\n\n    return nothing\nend\n</pre> ## Cell 2 ##  function ode(du, u, p, t)     P = u     r, K = p      du[1] = r * (1 - P / K) * P      return nothing end <p>This is where we create the MCMC 'loop': Sample Priors -&gt; Estimate ODE @ times -&gt; check fit</p> <p>We declare the function to be probabilistic model named fit_ode using the @model tag. The inputs are as follows.</p> <p>log_y_obs: Our log transformed observed data. times: The times at which the ODE is evaluated. prob: The prebuilt ODE.</p> <p>Next, priors. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. With these uniform priors we are essentially saying. We believe the proper value for the model will be equally likely within this range and WILL NOT be outside it. Your half normal distribution is a PDF that is only positive and extends to infinity. This is usually used for your error or standard deviation since neither can be negative and you can tune how quickly it tails off. Below are the priors visualized.</p> <p></p> <p>Next is solving the ODE. This will be a 2 step process.</p> <p>1: remake your problem with newly sampled elements. The inputs are as follows: prob: Premade ODE system. u0: State priors. p: Parameter priors. tspan: first and last of your observed times.</p> <p>2: solve the system at specific time points. the inputs are as follows: pr: remade system Tsit5(): Default. saveat: observed times. abstol / restol: Absolute tolerance / Relative tolerance. Default value chosen.</p> <p>note: See how when you're passing in values to this julia function there are both , and ;? In julia this is to mark the difference between positional arguments (before ;) and keyword arguments (after ;).</p> <p>The final part of the model is getting the likelihood (or fit if you prefer) of the model to the observed data. It's a bit layered here so we'll break it down part by part.</p> <p>log.(Array(sol)[1, :] .+ 1e-9): sol is the solver output. We force it into an array and grab the first row using the [1, :] indexing. Remember to be intentional with your indexing. Finally, we log scale the prediction. log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)): This is the actual likelihood calculation. Ensure the variable preceding the ~ here is the same as the one passed into fit_ode.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 3 ##\n\nusing Turing\n@model function fit_ode(log_y_obs, times, prob)\n    r ~ Uniform(0.5, 1.0)\n    K ~ Uniform(1e6, 4e7)\n\n    P0 ~ Uniform(1e5, 3e5)\n    \n    sigma ~ truncated(Normal(0, 3), 0, Inf)\n\n    pr = remake(prob;\n                p = [r, K],\n                u0 = [P0],\n                tspan = (times[1], times[end]))\n\n    sol = solve(pr, Tsit5();\n                abstol = 1e-6, reltol = 1e-6,\n                saveat = times)\n\n    log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)\n    log_y_obs ~ arraydist(Normal.(log_y_pred, sigma))\nend\n</pre> ## Cell 3 ##  using Turing @model function fit_ode(log_y_obs, times, prob)     r ~ Uniform(0.5, 1.0)     K ~ Uniform(1e6, 4e7)      P0 ~ Uniform(1e5, 3e5)          sigma ~ truncated(Normal(0, 3), 0, Inf)      pr = remake(prob;                 p = [r, K],                 u0 = [P0],                 tspan = (times[1], times[end]))      sol = solve(pr, Tsit5();                 abstol = 1e-6, reltol = 1e-6,                 saveat = times)      log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)     log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)) end <p>This block is where we declare the ODE with some starter values, call the model, and run the sampling.</p> <p>To start,</p> <p>u0: Initial state value(s). Make the inital value(s) from the observed data. p: Initial parameter value(s). Make these a value in the specified range of the prior you declared. Here we chose the low end of the uniform distribution. tspan:  first and last values from your observed times. prob: The prebuild ODE.</p> <p>Next is the call to run the sampling.</p> <p>model: The model created above. NUTS(1000, .95): This is telling you to use the NUTS sampler. 1000 is the amount of 'warm up' iterations it will do under the hood. .95 is the acceptance rate of the 'improved' solutions. MCMCSerial(): Tells the solver to run the chains sequentially. 1000: number of posterior samples to be generated. 3: number of chains. progress: either hides or shows the progress bar. Up to you.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 4 ##\n\nusing DifferentialEquations\n\nu0 = [y_obs[1]]\np = [0.5, 1e6]\ntspan = (times[1], times[end])\nprob = ODEProblem(ode, u0, tspan, p)\n\nmodel    = fit_ode(log_y_obs, times, prob)\nchain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 4; progress=false)\n</pre> ## Cell 4 ##  using DifferentialEquations  u0 = [y_obs[1]] p = [0.5, 1e6] tspan = (times[1], times[end]) prob = ODEProblem(ode, u0, tspan, p)  model    = fit_ode(log_y_obs, times, prob) chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 4; progress=false) <pre>\u250c Info: Found initial step size\n\u2502   \u03f5 = 0.05\n\u2514 @ Turing.Inference C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\hmc.jl:215\n\u250c Info: Found initial step size\n\u2502   \u03f5 = 0.4\n\u2514 @ Turing.Inference C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\hmc.jl:215\n\u250c Info: Found initial step size\n\u2502   \u03f5 = 0.4\n\u2514 @ Turing.Inference C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\hmc.jl:215\n\u250c Info: Found initial step size\n\u2502   \u03f5 = 0.20078125000000002\n\u2514 @ Turing.Inference C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\hmc.jl:215\n</pre> <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[6]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :r     =&gt; Uniform(0.5, 1.0),\n    :K     =&gt; Uniform(1e6, 4e7),\n    :P0    =&gt; Uniform(1e5, 3e5),\n    :sigma =&gt; truncated(Normal(0, 3), 0, Inf),\n)\n\norder = [:r, :K, :P0, :sigma]\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :r     =&gt; Uniform(0.5, 1.0),     :K     =&gt; Uniform(1e6, 4e7),     :P0    =&gt; Uniform(1e5, 3e5),     :sigma =&gt; truncated(Normal(0, 3), 0, Inf), )  order = [:r, :K, :P0, :sigma] plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities <p>Now we interpret the results. Remember the ODE's from before.</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P $$</p> <p>Take note of x-axis values at the peaks of the PDFs and the variables they represent. The x-axis values at those peaks are what the model found to be the optimal value for the equation. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space well enough.</p> In\u00a0[7]: Copied! <pre>init_syms = [:P0]\nparam_syms = [:r, :K]\nt_obs = times\ny_obs = y_obs\n\nplt = overlay_posterior_on_observed(\n    chain, ode, t_obs, y_obs;\n    init_syms=init_syms,\n    param_syms=param_syms,\n    which_states=[1],     # choose states to plot\n    n_draws=150,            # how many posterior paths to overlay\n    plot_ribbon=true,       # median \u00b1 CI band\n    legend=:topleft,\n    ribbon_q=(0.1, 0.9),    # CI limits\n    logy=false\n)\ndisplay(plt)\n</pre> init_syms = [:P0] param_syms = [:r, :K] t_obs = times y_obs = y_obs  plt = overlay_posterior_on_observed(     chain, ode, t_obs, y_obs;     init_syms=init_syms,     param_syms=param_syms,     which_states=[1],     # choose states to plot     n_draws=150,            # how many posterior paths to overlay     plot_ribbon=true,       # median \u00b1 CI band     legend=:topleft,     ribbon_q=(0.1, 0.9),    # CI limits     logy=false ) display(plt) <p>This plot is showing how well the range of estimated chains fit the observed data.</p> <p>CONGRATULATIONS! you just ran you first MCMC!</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 5 ##\n\nusing CSV, DataFrames\n\ncells = CSV.read(\"../../../case_study_2/python/data/total_cells.csv\", DataFrame)\ndeath = CSV.read(\"../../../case_study_2/python/data/death_percentage.csv\", DataFrame)\n\ncells_times  = cells[end-14:end, :1]\ncells_obs    = cells[end-14:end, :2] * 1e6\nlog_cells_obs = log.(cells_obs .+ 1e-9)\n\ndeath_times = death[end-14:end, :1]\ndeath_obs   = death[end-14:end, :2] .* (cells_obs ./ 100)\nlog_death_obs = log.(death_obs .+ 1e-9)\n</pre> ## Cell 5 ##  using CSV, DataFrames  cells = CSV.read(\"../../../case_study_2/python/data/total_cells.csv\", DataFrame) death = CSV.read(\"../../../case_study_2/python/data/death_percentage.csv\", DataFrame)  cells_times  = cells[end-14:end, :1] cells_obs    = cells[end-14:end, :2] * 1e6 log_cells_obs = log.(cells_obs .+ 1e-9)  death_times = death[end-14:end, :1] death_obs   = death[end-14:end, :2] .* (cells_obs ./ 100) log_death_obs = log.(death_obs .+ 1e-9) <p>ODE for living cells accounting for cell death</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P - \\delta P $$</p> <p>ODE for dead cells</p> <p>$$ \\frac{dD}{dt} = \\delta P $$</p> <p>We follow the same rules as before when creating this ODE. It's very easy to create more complex models with multiple equations. Remember your odering! Equations should be in the same order as the states they correspond to. BE CAREFUL WITH THE ORDER OF EVERYTHING. IT ALL MATTERS.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 6 ##\n\nfunction ode(du, u, p, t)\n    P, D = u\n    r, K, delta = p\n\n    du[1] = r * (1 - P / K) * P - delta * P\n    du[2] = delta * P\n\n    return nothing\nend\n</pre> ## Cell 6 ##  function ode(du, u, p, t)     P, D = u     r, K, delta = p      du[1] = r * (1 - P / K) * P - delta * P     du[2] = delta * P      return nothing end <p>Here is the model. Everything here should make sense. If something is confusing see the explination above cell 4. Still note that we have two likelihoods at the bottom since we are estimating two states.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 7 ##\n\nusing Turing\n\n@model function fit_ode(log_cells_obs, log_death_obs, times, prob)\n    r ~ Uniform(0.5, 1.0)\n    K ~ Uniform(1e6, 4e7)\n    delta ~ Uniform(0.0, 0.15)\n\n    P0 ~ Uniform(1e5, 3e5)\n    D0 ~ Uniform(1e4, 7e4)\n\n    sigma_live ~ truncated(Normal(0, 3), 0, Inf)\n    sigma_dead ~ truncated(Normal(0, 3), 0, Inf)\n\n    pr = remake(prob;\n                p = [r, K, delta],\n                u0 = [P0, D0],\n                tspan = (times[1], times[end]))\n    \n    # solve exactly at data times\n    sol = solve(pr, Tsit5();\n                abstol = 1e-6, reltol = 1e-6,\n                saveat = times)\n    S = Array(sol) \n\n    log_cells_pred = log.((S[1, :] + S[2, :]) .+ 1e-9)\n    log_death_pred = log.(S[2, :] .+ 1e-9)\n\n    log_cells_obs ~ arraydist(Normal.(log_cells_pred, sigma_live))\n    log_death_obs ~ arraydist(Normal.(log_death_pred, sigma_dead))\n\nend\n</pre> ## Cell 7 ##  using Turing  @model function fit_ode(log_cells_obs, log_death_obs, times, prob)     r ~ Uniform(0.5, 1.0)     K ~ Uniform(1e6, 4e7)     delta ~ Uniform(0.0, 0.15)      P0 ~ Uniform(1e5, 3e5)     D0 ~ Uniform(1e4, 7e4)      sigma_live ~ truncated(Normal(0, 3), 0, Inf)     sigma_dead ~ truncated(Normal(0, 3), 0, Inf)      pr = remake(prob;                 p = [r, K, delta],                 u0 = [P0, D0],                 tspan = (times[1], times[end]))          # solve exactly at data times     sol = solve(pr, Tsit5();                 abstol = 1e-6, reltol = 1e-6,                 saveat = times)     S = Array(sol)       log_cells_pred = log.((S[1, :] + S[2, :]) .+ 1e-9)     log_death_pred = log.(S[2, :] .+ 1e-9)      log_cells_obs ~ arraydist(Normal.(log_cells_pred, sigma_live))     log_death_obs ~ arraydist(Normal.(log_death_pred, sigma_dead))  end <p>Everything here is that same as before! See the explination above cell 5 if you forgot something.</p> In\u00a0[14]: Copied! <pre>## Cell 8 ##\n\nusing DifferentialEquations\n\nu0 = [log_cells_obs[1], log_death_obs[1]]\np = [0.5, 1e6, 0.0] \ntspan = (cells_times[1], cells_times[end])\nprob = ODEProblem(ode, u0, tspan, p)\n\nmodel    = fit_ode(log_cells_obs, log_death_obs, cells_times, prob)\nchain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 4; progress=false)\n</pre> ## Cell 8 ##  using DifferentialEquations  u0 = [log_cells_obs[1], log_death_obs[1]] p = [0.5, 1e6, 0.0]  tspan = (cells_times[1], cells_times[end]) prob = ODEProblem(ode, u0, tspan, p)  model    = fit_ode(log_cells_obs, log_death_obs, cells_times, prob) chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 4; progress=false)   <pre>\u250c Info: Found initial step size\n\u2502   \u03f5 = 0.025\n\u2514 @ Turing.Inference C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\hmc.jl:215\n\u250c Info: Found initial step size\n\u2502   \u03f5 = 0.05\n\u2514 @ Turing.Inference C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\hmc.jl:215\n\u250c Info: Found initial step size\n\u2502   \u03f5 = 0.2\n\u2514 @ Turing.Inference C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\hmc.jl:215\n\u250c Info: Found initial step size\n\u2502   \u03f5 = 0.2\n\u2514 @ Turing.Inference C:\\Users\\Whisk\\.julia\\packages\\Turing\\Avpxw\\src\\mcmc\\hmc.jl:215\n</pre> <pre>Chains MCMC chain (1000\u00d721\u00d74 Array{Float64, 3}):\n\nIterations        = 1001:1:2000\nNumber of chains  = 4\nSamples per chain = 1000\nWall duration     = 4.36 seconds\nCompute duration  = 4.3 seconds\nparameters        = r, K, delta, P0, D0, sigma_live, sigma_dead\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, lp, logprior, loglikelihood\n\nUse `describe(chains)` for summary statistics and quantiles.\n</pre> In\u00a0[15]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :r     =&gt; Uniform(0.5, 1.0),\n    :K     =&gt; Uniform(1e6, 4e7),\n    :delta =&gt; Uniform(0.0, 0.15),\n    :P0    =&gt; Uniform(1e5, 3e5),\n    :D0    =&gt; Uniform(1e4, 7e4),\n    :sigma_live =&gt; truncated(Normal(0, 3), 0, Inf),\n    :sigma_dead =&gt; truncated(Normal(0, 3), 0, Inf)\n)\n\norder = [:r, :K, :delta, :P0, :D0, :sigma_live, :sigma_dead]\n\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :r     =&gt; Uniform(0.5, 1.0),     :K     =&gt; Uniform(1e6, 4e7),     :delta =&gt; Uniform(0.0, 0.15),     :P0    =&gt; Uniform(1e5, 3e5),     :D0    =&gt; Uniform(1e4, 7e4),     :sigma_live =&gt; truncated(Normal(0, 3), 0, Inf),     :sigma_dead =&gt; truncated(Normal(0, 3), 0, Inf) )  order = [:r, :K, :delta, :P0, :D0, :sigma_live, :sigma_dead]  plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities In\u00a0[16]: Copied! <pre>init_syms = [:P0, :D0]\nparam_syms = [:r, :K, :delta]\nt_obs = cells_times\ny_obs = hcat(cells_obs, death_obs)\n\nplt = overlay_posterior_on_observed(\n    chain, ode, t_obs, y_obs;\n    init_syms=init_syms,\n    param_syms=param_syms,\n    which_states=[1, 2],     # choose states to plot\n    pred_transforms=[u -&gt; u[1] + u[2], u -&gt; u[2]], # column1=total(P+D), column2=dead(D)\n    legend=:topleft,\n    n_draws=150,            # how many posterior paths to overlay\n    plot_ribbon=true,       # median \u00b1 CI band\n    ribbon_q=(0.1, 0.9),    # CI limits\n    logy=false\n)\ndisplay(plt)\n</pre> init_syms = [:P0, :D0] param_syms = [:r, :K, :delta] t_obs = cells_times y_obs = hcat(cells_obs, death_obs)  plt = overlay_posterior_on_observed(     chain, ode, t_obs, y_obs;     init_syms=init_syms,     param_syms=param_syms,     which_states=[1, 2],     # choose states to plot     pred_transforms=[u -&gt; u[1] + u[2], u -&gt; u[2]], # column1=total(P+D), column2=dead(D)     legend=:topleft,     n_draws=150,            # how many posterior paths to overlay     plot_ribbon=true,       # median \u00b1 CI band     ribbon_q=(0.1, 0.9),    # CI limits     logy=false ) display(plt) In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"education/julia/case_study_2/#case-study-2-logistic-growth","title":"Case Study 2: Logistic Growth\u00b6","text":"<p>We are going to establish a logistic model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! You want to read your data into a DataFrame. It's also smart to handle any necessary data conversions during this step. You may not need to and that's just fine! It all depends on what you are looking for.</p> <p>Note: The \"+ 1e-9\" is merely a fail-safe against divide-by-zero errors when doing a log transform. Adding a value infitesimally near 0 makes no impact on the data itself and prevents the aformentioned error. Good quick trick to know!</p>"},{"location":"education/julia/case_study_2/#case-study-2-logistic-growth-and-death","title":"Case Study 2: Logistic Growth and Death\u00b6","text":"<p>Now on to a more complex model. We will be using the same data as before as well as some data on dead cells. We will also being using two ODEs!</p>"},{"location":"education/julia/case_study_3/","title":"Case Study 3 \u2014 Resource explicit Monod growth and death","text":"In\u00a0[\u00a0]: Copied! <pre># For user-defined post processing and plotting functions\ninclude(joinpath(@__DIR__, \"..\", \"..\", \"..\", \"utils\", \"plot_utils.jl\"))\n</pre> # For user-defined post processing and plotting functions include(joinpath(@__DIR__, \"..\", \"..\", \"..\", \"utils\", \"plot_utils.jl\")) <pre>plot_posterior_states_stacked (generic function with 1 method)</pre> <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using Turing. A Julia software package allowing you to fit complex models with ease.</p> In\u00a0[1]: Copied! <pre>using Distributed\naddprocs(4) # 4 processes 4 chains\n</pre> using Distributed addprocs(4) # 4 processes 4 chains <pre>4-element Vector{Int64}:\n 2\n 3\n 4\n 5</pre> <p>Second, we will setup the ODE system that will adapt to the data. To do this, you will use variables that Turing gives you by 'default' (du, u, p, t).</p> <p>mu_max: Maximum Specific Growth Rate Ks: Half-Saturation Constant Qn: Nutrient Consumption per Cell delta: Death Rate P_m3: Unit Conversion</p> <p>$$ \\begin{aligned}   \\mu(N) &amp;= \\mu_{\\text{max}} \\frac{N}{K_s + N} \\\\ \\frac{dN}{dt} &amp;= - Q_n \\, \\mu(N) \\, P_{\\text{m}^3} \\\\ \\frac{dP}{dt} &amp;= \\mu(N) \\, P - \\delta \\, P \\\\ \\frac{dD}{dt} &amp;= \\delta \\, P \\\\ \\end{aligned}   $$</p> In\u00a0[3]: Copied! <pre>@everywhere function ode(du, u, p, t)\n    N, P, D = u\n    mum, Ks, Qn, delta = p\n    P_m3 = P * 1e6 # convert from cells/mL to cells/m^3\n\n    mu = mum * N / (Ks + N)\n    du[1] = -Qn * mu * P_m3\n    du[2] = mu * P - delta * P\n    du[3] = delta * P\n\n    return nothing\nend\n</pre> @everywhere function ode(du, u, p, t)     N, P, D = u     mum, Ks, Qn, delta = p     P_m3 = P * 1e6 # convert from cells/mL to cells/m^3      mu = mum * N / (Ks + N)     du[1] = -Qn * mu * P_m3     du[2] = mu * P - delta * P     du[3] = delta * P      return nothing end <p>Third, the model. We declare the function to be a probabilistic model named fit_ode using the @model tag. The inputs are as follows.</p> <p>logP_obs: Total cell counts logD_obs: Dead cell counts times: The times at which the ODE system is evaluated prob: the prebuilt ODE</p> <p>Next, proirs. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. For our parameters we choose uniform priors. These uniform priors say, \"we believe the proper value for the model will be equally likely within this range and WILL NOT be outside of it\". For our initial conditions we choose a lognormal distribution. This says, \"we believe our value is around some x given a normalesque strictly positive distribution\". For our sigma terms (error), we choose a halfnormal distribution. This is usually used for your error or standard deviation since neither can be negative and you can tune how quickly it tails off. Below are the priors visualized.</p> <p></p> <p>Next is solving the ODE. This will be a 2 step process.</p> <p>1: remake your problem with newly sampled elements. The inputs are as follows: prob: Premade ODE system. u0: State priors. p: Parameter priors. tspan: first and last of your observed times.</p> <p>2: solve the system at specific time points. The inputs are as follows: pr: Remade system. Rodas5(): An ODE solver designed for stiff systems. saveat: Observed times. abstol / restol: Absolute tolerance / Relative tolerance. Default value chosen. sensealg: This is for cases where you want to use a specific algorithm to compute gradients. This is a standard choice from SciMLSensitivity. There are many at your disposal depending on your criteria.</p> <p>note: See how when you're passing in values to this julia function there are both , and ;? In julia this is to mark the difference between positional arguments (before ;) and keyword arguments (after ;).</p> <p>log.(Array(sol)[x, :] .+ 1e-9): sol is the solver output. We force it into an array and grab a row using the [x, :] indexing. Remember to be intentional with your indexing. Finally, we log scale the prediction. log_obs ~ arraydist(Normal.(log_pred, sigma)): This is the actual likelihood calculation. Ensure the variable preceding the ~ here is the same as the one passed into fit_ode.</p> In\u00a0[\u00a0]: Copied! <pre>@everywhere using Turing, Distributions, SciMLSensitivity\n\n@everywhere @model function fit_ode(logP_obs, logD_obs, times, prob)\n    mum ~ Uniform(0.4, 0.7)\n    Ks ~ Uniform(0.05, 0.2)\n    Qn ~ Uniform(1e-10, 7e-10)\n    delta ~ Uniform(0.01, 0.09)\n\n    # N0 isn't a 'prior' distribution. It is deterministic. Meaing, N0 will be a \n    # variable tracked by the model, but at each iteration it exists as a function\n    # of Qn. \n    N0 := 1000 + ((500 / 1.8e-10) * (Qn - 3.2e-10))\n\n    P0 ~ LogNormal(12.2175, 0.1)\n    D0 ~ LogNormal(10.2804, 0.1)\n\n    sigma_live ~ truncated(Normal(0, 1), 0, Inf)\n    sigma_dead ~ truncated(Normal(0, 1), 0, Inf)\n\n    pr = remake(prob,\n                u0 = [N0, P0, D0],\n                p = [mum, Ks, Qn, delta],\n                tspan = (times[1], times[end]))\n                \n    sol = solve(pr, Rodas5();\n                saveat = times,\n                abstol = 1e-6, reltol = 1e-6,\n                sensealg = InterpolatingAdjoint(autojacvec = ZygoteVJP()))\n    S = Array(sol)\n\n    logP_pred = log.((S[2, :] + S[3, :]) .+ 1e-9) # total. 2 is live, 3 is dead!\n    logD_pred = log.(S[3, :] .+ 1e-9)\n\n    logP_obs ~ arraydist(Normal.(logP_pred, sigma_live))\n    logD_obs ~ arraydist(Normal.(logD_pred, sigma_dead))\nend\n</pre> @everywhere using Turing, Distributions, SciMLSensitivity  @everywhere @model function fit_ode(logP_obs, logD_obs, times, prob)     mum ~ Uniform(0.4, 0.7)     Ks ~ Uniform(0.05, 0.2)     Qn ~ Uniform(1e-10, 7e-10)     delta ~ Uniform(0.01, 0.09)      # N0 isn't a 'prior' distribution. It is deterministic. Meaing, N0 will be a      # variable tracked by the model, but at each iteration it exists as a function     # of Qn.      N0 := 1000 + ((500 / 1.8e-10) * (Qn - 3.2e-10))      P0 ~ LogNormal(12.2175, 0.1)     D0 ~ LogNormal(10.2804, 0.1)      sigma_live ~ truncated(Normal(0, 1), 0, Inf)     sigma_dead ~ truncated(Normal(0, 1), 0, Inf)      pr = remake(prob,                 u0 = [N0, P0, D0],                 p = [mum, Ks, Qn, delta],                 tspan = (times[1], times[end]))                      sol = solve(pr, Rodas5();                 saveat = times,                 abstol = 1e-6, reltol = 1e-6,                 sensealg = InterpolatingAdjoint(autojacvec = ZygoteVJP()))     S = Array(sol)      logP_pred = log.((S[2, :] + S[3, :]) .+ 1e-9) # total. 2 is live, 3 is dead!     logD_pred = log.(S[3, :] .+ 1e-9)      logP_obs ~ arraydist(Normal.(logP_pred, sigma_live))     logD_obs ~ arraydist(Normal.(logD_pred, sigma_dead)) end <p>Fourth, dependencies. You want to establish all your dependencies at once within the @everywhere begin / end block. If you don't do this, you will have to type @everywhere in front of every single variable. You need to call this only once. Multiple calls in this fashion (at least while working in jupyter notebooks) during personal testing resulted in agregious slowdowns during the distributed computation phase.</p> <p>We'll walk through how the priming variables were chosen. Qn_low: The bottom end of the uniform prior established for the variable. N0_init: The variable in the model is a deterministic value based on Qn. The same math is applied to give the priming value. The rest of u0: First values of the series of observed data. p: Bottom ends of their respective priors. tspan: First and last observed times. prob: Primed ODE system.</p> <p>Final step is to log transform the data and pass it to the model.</p> In\u00a0[5]: Copied! <pre>@everywhere using CSV, DataFrames, DifferentialEquations\n\n@everywhere begin\n    cells = CSV.read(\"../../../case_study_2/python/data/total_cells.csv\", DataFrame)\n    death = CSV.read(\"../../../case_study_2/python/data/death_percentage.csv\", DataFrame)\n\n    cells_times  = cells[end-14:end, :1]\n    cells_obs    = cells[end-14:end, :2] * 1e6\n\n    death_times = death[end-14:end, :1]\n    death_obs   = death[end-14:end, :2] .* (cells_obs ./ 100)\n\n    Qn_low = 1e-10\n    N0_init = 1000 + ((500 / 1.8e-10) * (Qn_low - 3.2e-10))\n    u0 = [N0_init, cells_obs[1], death_obs[1]]\n    p = [0.4, 0.05, 1e-10, 0.01] \n    tspan = (cells_times[1], cells_times[end])\n    prob = ODEProblem(ode, u0, tspan, p)\n\n    log_cells_obs = log.(cells_obs .+ 1e-9)\n    log_death_obs = log.(death_obs .+ 1e-9)\n    model = fit_ode(log_cells_obs, log_death_obs, cells_times, prob)\nend\n</pre> @everywhere using CSV, DataFrames, DifferentialEquations  @everywhere begin     cells = CSV.read(\"../../../case_study_2/python/data/total_cells.csv\", DataFrame)     death = CSV.read(\"../../../case_study_2/python/data/death_percentage.csv\", DataFrame)      cells_times  = cells[end-14:end, :1]     cells_obs    = cells[end-14:end, :2] * 1e6      death_times = death[end-14:end, :1]     death_obs   = death[end-14:end, :2] .* (cells_obs ./ 100)      Qn_low = 1e-10     N0_init = 1000 + ((500 / 1.8e-10) * (Qn_low - 3.2e-10))     u0 = [N0_init, cells_obs[1], death_obs[1]]     p = [0.4, 0.05, 1e-10, 0.01]      tspan = (cells_times[1], cells_times[end])     prob = ODEProblem(ode, u0, tspan, p)      log_cells_obs = log.(cells_obs .+ 1e-9)     log_death_obs = log.(death_obs .+ 1e-9)     model = fit_ode(log_cells_obs, log_death_obs, cells_times, prob) end <p>Fifth and finally this is the command to run the processes.</p> <p>model: The model created above. NUTS(1000, .95): This is telling you to use the NUTS sampler. 1000 is the amount of 'warm up' iterations it will do under the hood. .95 is the acceptance rate of the 'improved' solutions. MCMCDistributed(): Tells the solver to run the chains in parallel on the 4 spawned processes. 1000: Number of posterior samples to be generated. 4: Number of chains. Need to match the process count. progress: Either hides or shows the progress bar. Up to you.</p> In\u00a0[6]: Copied! <pre>chain = sample(model, NUTS(1000, .95), MCMCDistributed(), 1000, 4; progress=true)\n</pre> chain = sample(model, NUTS(1000, .95), MCMCDistributed(), 1000, 4; progress=true) <pre>Sampling (4 processes)   0%|\u2588                           |  ETA: N/A\n</pre> <pre>      From worker 2:\t\u250c Info: Found initial step size\n      From worker 2:\t\u2514   \u03f5 = 0.0359619140625\n      From worker 3:\t\u250c Info: Found initial step size\n      From worker 3:\t\u2514   \u03f5 = 0.2\n      From worker 5:\t\u250c Info: Found initial step size\n      From worker 5:\t\u2514   \u03f5 = 0.05\n      From worker 4:\t\u250c Info: Found initial step size\n      From worker 4:\t\u2514   \u03f5 = 0.00625\n</pre> <pre>Sampling (4 processes)   0%|\u2588                           |  ETA: 2:04:51\nSampling (4 processes)   1%|\u2588                           |  ETA: 1:05:50\nSampling (4 processes)   2%|\u2588                           |  ETA: 0:47:08\nSampling (4 processes)   2%|\u2588                           |  ETA: 0:36:36\nSampling (4 processes)   2%|\u2588                           |  ETA: 0:31:34\nSampling (4 processes)   3%|\u2588                           |  ETA: 0:26:57\nSampling (4 processes)   4%|\u2588                           |  ETA: 0:25:02\nSampling (4 processes)   4%|\u2588\u2588                          |  ETA: 0:23:05\nSampling (4 processes)   4%|\u2588\u2588                          |  ETA: 0:21:27\nSampling (4 processes)   5%|\u2588\u2588                          |  ETA: 0:19:22\nSampling (4 processes)   6%|\u2588\u2588                          |  ETA: 0:17:50\nSampling (4 processes)   6%|\u2588\u2588                          |  ETA: 0:16:37\nSampling (4 processes)   6%|\u2588\u2588                          |  ETA: 0:15:27\nSampling (4 processes)   7%|\u2588\u2588                          |  ETA: 0:14:33\nSampling (4 processes)   8%|\u2588\u2588\u2588                         |  ETA: 0:13:37\nSampling (4 processes)   8%|\u2588\u2588\u2588                         |  ETA: 0:12:51\nSampling (4 processes)   8%|\u2588\u2588\u2588                         |  ETA: 0:12:09\nSampling (4 processes)   9%|\u2588\u2588\u2588                         |  ETA: 0:11:31\nSampling (4 processes)  10%|\u2588\u2588\u2588                         |  ETA: 0:10:58\nSampling (4 processes)  10%|\u2588\u2588\u2588                         |  ETA: 0:10:26\nSampling (4 processes)  10%|\u2588\u2588\u2588                         |  ETA: 0:09:58\nSampling (4 processes)  11%|\u2588\u2588\u2588\u2588                        |  ETA: 0:09:34\nSampling (4 processes)  12%|\u2588\u2588\u2588\u2588                        |  ETA: 0:09:12\nSampling (4 processes)  12%|\u2588\u2588\u2588\u2588                        |  ETA: 0:08:53\nSampling (4 processes)  12%|\u2588\u2588\u2588\u2588                        |  ETA: 0:08:32\nSampling (4 processes)  13%|\u2588\u2588\u2588\u2588                        |  ETA: 0:08:18\nSampling (4 processes)  14%|\u2588\u2588\u2588\u2588                        |  ETA: 0:07:58\nSampling (4 processes)  14%|\u2588\u2588\u2588\u2588                        |  ETA: 0:07:44\nSampling (4 processes)  14%|\u2588\u2588\u2588\u2588\u2588                       |  ETA: 0:07:27\nSampling (4 processes)  15%|\u2588\u2588\u2588\u2588\u2588                       |  ETA: 0:07:14\nSampling (4 processes)  16%|\u2588\u2588\u2588\u2588\u2588                       |  ETA: 0:07:01\nSampling (4 processes)  16%|\u2588\u2588\u2588\u2588\u2588                       |  ETA: 0:06:50\nSampling (4 processes)  16%|\u2588\u2588\u2588\u2588\u2588                       |  ETA: 0:06:37\nSampling (4 processes)  17%|\u2588\u2588\u2588\u2588\u2588                       |  ETA: 0:06:27\nSampling (4 processes)  18%|\u2588\u2588\u2588\u2588\u2588                       |  ETA: 0:06:15\nSampling (4 processes)  18%|\u2588\u2588\u2588\u2588\u2588\u2588                      |  ETA: 0:06:06\nSampling (4 processes)  18%|\u2588\u2588\u2588\u2588\u2588\u2588                      |  ETA: 0:05:57\nSampling (4 processes)  19%|\u2588\u2588\u2588\u2588\u2588\u2588                      |  ETA: 0:05:49\nSampling (4 processes)  20%|\u2588\u2588\u2588\u2588\u2588\u2588                      |  ETA: 0:05:39\nSampling (4 processes)  20%|\u2588\u2588\u2588\u2588\u2588\u2588                      |  ETA: 0:05:33\nSampling (4 processes)  20%|\u2588\u2588\u2588\u2588\u2588\u2588                      |  ETA: 0:05:24\nSampling (4 processes)  21%|\u2588\u2588\u2588\u2588\u2588\u2588                      |  ETA: 0:05:17\nSampling (4 processes)  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588                     |  ETA: 0:05:09\nSampling (4 processes)  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588                     |  ETA: 0:05:04\nSampling (4 processes)  22%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588                     |  ETA: 0:04:56\nSampling (4 processes)  23%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588                     |  ETA: 0:04:52\nSampling (4 processes)  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588                     |  ETA: 0:04:44\nSampling (4 processes)  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588                     |  ETA: 0:04:40\nSampling (4 processes)  24%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588                     |  ETA: 0:04:34\nSampling (4 processes)  25%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    |  ETA: 0:04:29\nSampling (4 processes)  26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    |  ETA: 0:04:23\nSampling (4 processes)  26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    |  ETA: 0:04:18\nSampling (4 processes)  26%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    |  ETA: 0:04:13\nSampling (4 processes)  27%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    |  ETA: 0:04:09\nSampling (4 processes)  28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    |  ETA: 0:04:04\nSampling (4 processes)  28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    |  ETA: 0:04:00\nSampling (4 processes)  28%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                    |  ETA: 0:03:55\nSampling (4 processes)  29%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   |  ETA: 0:03:51\nSampling (4 processes)  30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   |  ETA: 0:03:47\nSampling (4 processes)  30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   |  ETA: 0:03:43\nSampling (4 processes)  30%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   |  ETA: 0:03:39\nSampling (4 processes)  31%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   |  ETA: 0:03:35\nSampling (4 processes)  32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   |  ETA: 0:03:32\nSampling (4 processes)  32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                   |  ETA: 0:03:28\nSampling (4 processes)  32%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  |  ETA: 0:03:25\nSampling (4 processes)  33%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  |  ETA: 0:03:21\nSampling (4 processes)  34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  |  ETA: 0:03:19\nSampling (4 processes)  34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  |  ETA: 0:03:15\nSampling (4 processes)  34%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  |  ETA: 0:03:12\nSampling (4 processes)  35%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  |  ETA: 0:03:09\nSampling (4 processes)  36%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                  |  ETA: 0:03:06\nSampling (4 processes)  36%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 |  ETA: 0:03:03\nSampling (4 processes)  36%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 |  ETA: 0:03:00\nSampling (4 processes)  37%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 |  ETA: 0:02:57\nSampling (4 processes)  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 |  ETA: 0:02:54\nSampling (4 processes)  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 |  ETA: 0:02:52\nSampling (4 processes)  38%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 |  ETA: 0:02:49\nSampling (4 processes)  39%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                 |  ETA: 0:02:47\nSampling (4 processes)  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                |  ETA: 0:02:44\nSampling (4 processes)  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                |  ETA: 0:02:41\nSampling (4 processes)  40%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                |  ETA: 0:02:39\nSampling (4 processes)  41%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                |  ETA: 0:02:36\nSampling (4 processes)  42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                |  ETA: 0:02:34\nSampling (4 processes)  42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                |  ETA: 0:02:32\nSampling (4 processes)  42%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588                |  ETA: 0:02:30\nSampling (4 processes)  43%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               |  ETA: 0:02:28\nSampling (4 processes)  44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               |  ETA: 0:02:25\nSampling (4 processes)  44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               |  ETA: 0:02:23\nSampling (4 processes)  44%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               |  ETA: 0:02:21\nSampling (4 processes)  45%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               |  ETA: 0:02:19\nSampling (4 processes)  46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               |  ETA: 0:02:17\nSampling (4 processes)  46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588               |  ETA: 0:02:15\nSampling (4 processes)  46%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              |  ETA: 0:02:13\nSampling (4 processes)  47%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              |  ETA: 0:02:11\nSampling (4 processes)  48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              |  ETA: 0:02:09\nSampling (4 processes)  48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              |  ETA: 0:02:08\nSampling (4 processes)  48%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              |  ETA: 0:02:06\nSampling (4 processes)  49%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              |  ETA: 0:02:04\nSampling (4 processes)  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588              |  ETA: 0:02:02\nSampling (4 processes)  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             |  ETA: 0:02:00\nSampling (4 processes)  50%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             |  ETA: 0:01:58\nSampling (4 processes)  51%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             |  ETA: 0:01:57\nSampling (4 processes)  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             |  ETA: 0:01:55\nSampling (4 processes)  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             |  ETA: 0:01:53\nSampling (4 processes)  52%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             |  ETA: 0:01:52\nSampling (4 processes)  53%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             |  ETA: 0:01:50\nSampling (4 processes)  54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588             |  ETA: 0:01:48\nSampling (4 processes)  54%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            |  ETA: 0:01:47\nSampling (4 processes)  55%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            |  ETA: 0:01:45\nSampling (4 processes)  55%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            |  ETA: 0:01:44\nSampling (4 processes)  56%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            |  ETA: 0:01:42\nSampling (4 processes)  56%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            |  ETA: 0:01:40\nSampling (4 processes)  56%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            |  ETA: 0:01:39\nSampling (4 processes)  57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588            |  ETA: 0:01:37\nSampling (4 processes)  57%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           |  ETA: 0:01:36\nSampling (4 processes)  58%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           |  ETA: 0:01:34\nSampling (4 processes)  58%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           |  ETA: 0:01:33\nSampling (4 processes)  59%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           |  ETA: 0:01:32\nSampling (4 processes)  60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           |  ETA: 0:01:30\nSampling (4 processes)  60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           |  ETA: 0:01:29\nSampling (4 processes)  60%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588           |  ETA: 0:01:27\nSampling (4 processes)  61%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          |  ETA: 0:01:26\nSampling (4 processes)  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          |  ETA: 0:01:25\nSampling (4 processes)  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          |  ETA: 0:01:23\nSampling (4 processes)  62%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          |  ETA: 0:01:22\nSampling (4 processes)  63%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          |  ETA: 0:01:20\nSampling (4 processes)  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          |  ETA: 0:01:19\nSampling (4 processes)  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588          |  ETA: 0:01:18\nSampling (4 processes)  64%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         |  ETA: 0:01:17\nSampling (4 processes)  65%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         |  ETA: 0:01:15\nSampling (4 processes)  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         |  ETA: 0:01:14\nSampling (4 processes)  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         |  ETA: 0:01:13\nSampling (4 processes)  66%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         |  ETA: 0:01:11\nSampling (4 processes)  67%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         |  ETA: 0:01:10\nSampling (4 processes)  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588         |  ETA: 0:01:09\nSampling (4 processes)  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        |  ETA: 0:01:08\nSampling (4 processes)  68%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        |  ETA: 0:01:06\nSampling (4 processes)  69%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        |  ETA: 0:01:05\nSampling (4 processes)  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        |  ETA: 0:01:04\nSampling (4 processes)  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        |  ETA: 0:01:03\nSampling (4 processes)  70%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        |  ETA: 0:01:02\nSampling (4 processes)  71%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588        |  ETA: 0:01:01\nSampling (4 processes)  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       |  ETA: 0:00:59\nSampling (4 processes)  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       |  ETA: 0:00:58\nSampling (4 processes)  72%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       |  ETA: 0:00:57\nSampling (4 processes)  73%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       |  ETA: 0:00:56\nSampling (4 processes)  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       |  ETA: 0:00:55\nSampling (4 processes)  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       |  ETA: 0:00:54\nSampling (4 processes)  74%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588       |  ETA: 0:00:52\nSampling (4 processes)  75%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      |  ETA: 0:00:51\nSampling (4 processes)  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      |  ETA: 0:00:50\nSampling (4 processes)  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      |  ETA: 0:00:49\nSampling (4 processes)  76%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      |  ETA: 0:00:48\nSampling (4 processes)  77%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      |  ETA: 0:00:47\nSampling (4 processes)  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      |  ETA: 0:00:46\nSampling (4 processes)  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      |  ETA: 0:00:45\nSampling (4 processes)  78%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588      |  ETA: 0:00:44\nSampling (4 processes)  79%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     |  ETA: 0:00:42\nSampling (4 processes)  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     |  ETA: 0:00:41\nSampling (4 processes)  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     |  ETA: 0:00:40\nSampling (4 processes)  80%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     |  ETA: 0:00:39\nSampling (4 processes)  81%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     |  ETA: 0:00:38\nSampling (4 processes)  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     |  ETA: 0:00:37\nSampling (4 processes)  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588     |  ETA: 0:00:36\nSampling (4 processes)  82%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    |  ETA: 0:00:35\nSampling (4 processes)  83%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    |  ETA: 0:00:34\nSampling (4 processes)  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    |  ETA: 0:00:33\nSampling (4 processes)  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    |  ETA: 0:00:32\nSampling (4 processes)  84%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    |  ETA: 0:00:31\nSampling (4 processes)  85%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    |  ETA: 0:00:30\nSampling (4 processes)  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588    |  ETA: 0:00:29\nSampling (4 processes)  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   |  ETA: 0:00:28\nSampling (4 processes)  86%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   |  ETA: 0:00:27\nSampling (4 processes)  87%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   |  ETA: 0:00:26\nSampling (4 processes)  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   |  ETA: 0:00:25\nSampling (4 processes)  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   |  ETA: 0:00:23\nSampling (4 processes)  88%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   |  ETA: 0:00:22\nSampling (4 processes)  89%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588   |  ETA: 0:00:21\nSampling (4 processes)  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  |  ETA: 0:00:20\nSampling (4 processes)  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  |  ETA: 0:00:19\nSampling (4 processes)  90%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  |  ETA: 0:00:18\nSampling (4 processes)  91%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  |  ETA: 0:00:17\nSampling (4 processes)  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  |  ETA: 0:00:16\nSampling (4 processes)  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  |  ETA: 0:00:15\nSampling (4 processes)  92%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588  |  ETA: 0:00:14\nSampling (4 processes)  93%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |  ETA: 0:00:13\nSampling (4 processes)  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |  ETA: 0:00:12\nSampling (4 processes)  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |  ETA: 0:00:12\nSampling (4 processes)  94%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |  ETA: 0:00:11\nSampling (4 processes)  95%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |  ETA: 0:00:10\nSampling (4 processes)  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |  ETA: 0:00:09\nSampling (4 processes)  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588 |  ETA: 0:00:08\nSampling (4 processes)  96%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  ETA: 0:00:07\nSampling (4 processes)  97%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  ETA: 0:00:06\nSampling (4 processes)  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  ETA: 0:00:05\nSampling (4 processes)  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  ETA: 0:00:04\nSampling (4 processes)  98%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  ETA: 0:00:03\nSampling (4 processes)  99%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  ETA: 0:00:02\nSampling (4 processes) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588|  ETA: 0:00:01\nSampling (4 processes) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| Time: 0:03:10\nSampling (4 processes) 100%|\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588\u2588| Time: 0:03:15\n</pre> <pre>Chains MCMC chain (1000\u00d723\u00d74 Array{Float64, 3}):\n\nIterations        = 1001:1:2000\nNumber of chains  = 4\nSamples per chain = 1000\nWall duration     = 185.17 seconds\nCompute duration  = 727.25 seconds\nparameters        = mum, Ks, Qn, delta, N0, P0, D0, sigma_live, sigma_dead\ninternals         = n_steps, is_accept, acceptance_rate, log_density, hamiltonian_energy, hamiltonian_energy_error, max_hamiltonian_energy_error, tree_depth, numerical_error, step_size, nom_step_size, lp, logprior, loglikelihood\n\nUse `describe(chains)` for summary statistics and quantiles.\n</pre> <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[7]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :mum =&gt; Uniform(0.4, 0.7),\n    :Ks =&gt; Uniform(.05, 0.2),\n    :Qn =&gt; Uniform(1e-10, 7e-10),\n    :delta =&gt; Uniform(0.01, 0.09),\n    :P0 =&gt; LogNormal(12.2175, 0.1),\n    :D0 =&gt; LogNormal(10.2804, 0.1),\n    :sigma_live =&gt; truncated(Normal(0, 1), 0, Inf),\n    :sigma_dead =&gt; truncated(Normal(0, 1), 0, Inf)\n)\n\norder = [:mum, :Ks, :Qn, :delta, :P0, :D0, :sigma_live, :sigma_dead]\n\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :mum =&gt; Uniform(0.4, 0.7),     :Ks =&gt; Uniform(.05, 0.2),     :Qn =&gt; Uniform(1e-10, 7e-10),     :delta =&gt; Uniform(0.01, 0.09),     :P0 =&gt; LogNormal(12.2175, 0.1),     :D0 =&gt; LogNormal(10.2804, 0.1),     :sigma_live =&gt; truncated(Normal(0, 1), 0, Inf),     :sigma_dead =&gt; truncated(Normal(0, 1), 0, Inf) )  order = [:mum, :Ks, :Qn, :delta, :P0, :D0, :sigma_live, :sigma_dead]  plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities In\u00a0[14]: Copied! <pre>fig = plot_posterior_states_stacked(\n    chain, prob, cells_times;\n    n_draws=150, ribbon_q=(0.05,0.95),\n    obs_total=cells_obs, obs_dead=death_obs\n)\ndisplay(fig)\n</pre> fig = plot_posterior_states_stacked(     chain, prob, cells_times;     n_draws=150, ribbon_q=(0.05,0.95),     obs_total=cells_obs, obs_dead=death_obs ) display(fig) <p>Now we interpret the results. Remember the ODE system from before.</p> <p>$$ \\begin{aligned}   \\mu(N) &amp;= \\mu_{\\text{max}} \\frac{N}{K_s + N} \\\\ \\frac{dN}{dt} &amp;= - Q_n \\, \\mu(N) \\, P_{\\text{m}^3} \\\\ \\frac{dP}{dt} &amp;= \\mu(N) \\, P - \\delta \\, P \\\\ \\frac{dD}{dt} &amp;= \\delta \\, P \\\\ \\end{aligned}   $$</p> <p>Here we will look at the above plots in tandem. Take note of the x-axis values at the peaks of the PDF's and the variables they represent. Those are the optimal values for their respective variables for the system. The left hand column is a frequency plot. This is just to make sure the model is exploring the parameter space. Now, see that not all the varibles are 'fit'. Ks doesn't have a peak, its flat. If you see this alone it may seem problematic. This is why looking at posterior draws over the data is important. We can see that our model fits the data very well, therefore the flat Ks estimate means the model is not dependent on that value. Which is fine! This is the importance of clear post processing.</p>"},{"location":"education/julia/case_study_3/#case-study-3-monod-growth-model-with-explicit-resources","title":"Case Study 3: Monod Growth Model with Explicit Resources\u00b6","text":"<p>We are going to establish a Monod Growth model in this case study. The ODE will be defined in the near future. The Monod Growth model results is a stiff ODE system so we have to employ a couple tricks to get this model to work efficiently. This will be sectioned into 5 general parts.</p> <p>1: Spawn processes 2: Establish ODE system 3: Create Model 4: Load processes' dependencies 5: Run</p> <p>First is the introduction of distributed processing to allow chains to run in parallel. This portion is simple. Just establish some processes as follows. The amount of processes you spawn need to match the amount of chains you plan to run. Your main process is process 1. When you spawn 4 more you will get additional processes 2, 3, 4, and 5. These will sit idly and wait for the sample() call in step 5 to compute one chain per process in parallel. Note, that functions and libraries need to have the @everywhere tag so they are implemented in all the spawned processes not just process one (main).</p>"},{"location":"education/python/case_study_1/","title":"Case Study 1 \u2014 Birth\u2013death dynamics","text":"In\u00a0[1]: Copied! <pre># For user-defined post processing and plotting functions\nimport sys\nsys.path.insert(0, \"../../..\")\nfrom utils.plot_utils_v2 import *\n</pre> # For user-defined post processing and plotting functions import sys sys.path.insert(0, \"../../..\") from utils.plot_utils_v2 import * <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using PYMC. A python software package allowing you to fit complex models with ease.</p> In\u00a0[2]: Copied! <pre>## Cell 1 ##\n\nimport pandas as pd\nimport numpy as np\n\ndata = pd.read_csv(\"../../../case_study_1/python/data/phaeocystis_control.csv\")\ntime = data['times'].values\nobs = data['cells'].values\nlog_obs = np.log(obs + 1e-9)\n</pre> ## Cell 1 ##  import pandas as pd import numpy as np  data = pd.read_csv(\"../../../case_study_1/python/data/phaeocystis_control.csv\") time = data['times'].values obs = data['cells'].values log_obs = np.log(obs + 1e-9) <p>First things first, the ODE system. We will setup the logistic ODE that will adapt to the data. To do this, you will use variables that PyMC gives you by 'default'. Let me explain. Note that in Cell 2 there are 3 parameters being passed in (y, t, params). You will also not see these variables being explicitly passed in this way anywhere. Here is what happens. When you create the ODE function in Cell 2, it gets passed to the the 'cell_model' in Cell 3. Now look to where the 'cell_model' is called in cell 4. There it passes priors into Y0 and theta. The connection is Y0 goes to y, and theta goes to params. The order you hand over the priors matters so call them in the correct order when creating the ODE equation. Never forget to be mindful of your prior's orientations when you pass them!</p> <p>mum: Growth Rate y: Initial Value</p> <p>$$ \\frac{dy}{dt} = \\mu y $$</p> In\u00a0[3]: Copied! <pre>## Cell 2 ##\n\ndef ode(y, t, params):\n    mum = params[0]\n    return [mum * y[0]]\n</pre> ## Cell 2 ##  def ode(y, t, params):     mum = params[0]     return [mum * y[0]] <p>Here is the model we spoke about earlier. Here is what goes into each variable.</p> <p>func: the ODE function you just created times: the time steps from your data n_states: possible states count n_theta: Unique parameter count t0: inital time step (should almost always be 0)</p> In\u00a0[4]: Copied! <pre>## Cell 3 ##\n\nimport pymc as pm\ncell_model = pm.ode.DifferentialEquation(\n    func=ode,\n    times=data['times'].values,\n    n_states=1,\n    n_theta=1,\n    t0=0\n)\n</pre> ## Cell 3 ##  import pymc as pm cell_model = pm.ode.DifferentialEquation(     func=ode,     times=data['times'].values,     n_states=1,     n_theta=1,     t0=0 ) <p>This is where we create the MCMC 'loop': Sample Priors -&gt; Estimate ODE @ times -&gt; check fit</p> <p>First, priors. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. For $\\mu$ we chose a truncated normal prior. This says, \"we believe there is a normal distribution about some x, however, it cannot be below some y or above some z\". For N0, we chose a lognormal distribution. This says, \"we believe our value is around some x given a normalesque strictly positive distribution\". For sigma, we chose a halfnormal distribution. This is usually used for your error or standard deviation. This is usually used for your error or standard deviation since neither can be negative and you can tune how quickly it tails off. Below are the priors visualized.</p> <p></p> <p>After the priors are declared the next function call will be for the solution of the model given its current estimates. We spoke about how to pass the priors in previously. Be careful during this step!</p> <p>The final function is for the likelihood estimation. This is where the model tests how well it estimated the observed data. We pass it our log transformed estimated data, the sigma value from the iteration's sample, and log transformed observed data. Now just let PyMC work its magic!</p> In\u00a0[5]: Copied! <pre>## Cell 4 ##\n\nwith pm.Model() as model:\n    mum = pm.TruncatedNormal('mum', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)\n    N0 = pm.Lognormal('N0', mu=log_obs[0], sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", 1)\n\n    sol = cell_model(y0=[N0], theta=[mum])\n\n    pm.Normal(\"Y_obs\", \n              mu=pm.math.log(sol[:, 0] + 1e-9), \n              sigma=sigma, \n              observed=log_obs)\n</pre> ## Cell 4 ##  with pm.Model() as model:     mum = pm.TruncatedNormal('mum', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)     N0 = pm.Lognormal('N0', mu=log_obs[0], sigma=0.1)     sigma = pm.HalfNormal(\"sigma\", 1)      sol = cell_model(y0=[N0], theta=[mum])      pm.Normal(\"Y_obs\",                mu=pm.math.log(sol[:, 0] + 1e-9),                sigma=sigma,                observed=log_obs) <p>This is how you call and run the model!</p> <p>draws: poterior values to be generated. tune: Some under the hood PyMC magic that 'warms it up'. Keep this between 500 and 2000 depending on how complex your model is. chains / cores: Amount of chains you want to compute in parallel. Keep these values the same for most basic usage. PyMC recommends you run at least 4 chains. return_inferencedata: returns an ARVIZ inferencedata object which we'll use for easy reading of our output. target_accept: the % of times you accept the improved likelihood at each step of the chain. This is your exploration vs exploitation balance.</p> In\u00a0[6]: Copied! <pre>## CELL 5 ##\n\nwith model:\n    trace = pm.sample(draws=1000, \n                      tune=1000, \n                      chains=4, \n                      cores=4,\n                      return_inferencedata=True,\n                      target_accept=0.95)\n</pre> ## CELL 5 ##  with model:     trace = pm.sample(draws=1000,                        tune=1000,                        chains=4,                        cores=4,                       return_inferencedata=True,                       target_accept=0.95) <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mum, N0, sigma]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 40 seconds.\n</pre> <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[8]: Copied! <pre>plot_trace(trace=trace,\n           model=model,\n           uni=[],\n           var_names_map={'N0':'Initial density/ml','mum': 'Growth Rate \u03bc', 'sigma': 'Std Dev \u03c3'},\n           var_order=['mum','N0','sigma'],\n           fontname='Arial',\n           fontsize=12,\n           num_prior_samples=2000,\n           save_path='figures/normal_growth_chains.png')\n</pre> plot_trace(trace=trace,            model=model,            uni=[],            var_names_map={'N0':'Initial density/ml','mum': 'Growth Rate \u03bc', 'sigma': 'Std Dev \u03c3'},            var_order=['mum','N0','sigma'],            fontname='Arial',            fontsize=12,            num_prior_samples=2000,            save_path='figures/normal_growth_chains.png') <pre>Sampling: [N0, Y_obs, mum, sigma]\n</pre> <pre>mum\nN0\nsigma\n</pre> Out[8]: <pre>array([[&lt;Axes: title={'center': 'Growth Rate \u03bc'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Growth Rate \u03bc'}&gt;],\n       [&lt;Axes: title={'center': 'Initial density/ml'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Initial density/ml'}&gt;],\n       [&lt;Axes: title={'center': 'Std Dev \u03c3'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Std Dev \u03c3'}&gt;]], dtype=object)</pre> <p>Now we interpret the results. Remember the ODE's from before.</p> <p>$$ \\frac{dy}{dt} = \\mu y $$</p> <p>Take note of x-axis values at the peaks of the PDFs and the variables they represent. The x-axis values at those peaks are what the model found to be the optimal value for the equation. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space well enough.</p> In\u00a0[9]: Copied! <pre>dataset_postprocessing = {\n\"Cells\": [\n        {\"time\": time, \"values\": obs},  # replicate 1\n    ]\n}\n\ndef ode_solution2data(solution):\n    total = solution[:, 0]  # Assuming the first column is the total cell count\n    return {\n        \"total\": total\n    }\n\nposterior_dynamics(dataset=dataset_postprocessing,\n                   trace=trace,\n                   model=model,\n                   n_plots=100,\n                   burn_in=50,\n                   num_variables=1,\n                   ode_fn=ode,\n                   ode2data_fn=ode_solution2data,\n                   save_path=\"figures/vardi_logistic_growth_dynamics.png\",\n                   # the key of this dict is the variable name in the dataset_postprocessing\n                   var_properties={\n                       \"Cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}\n                  },\n                   suptitle=\"Posterior Predictive Dynamics\",\n                   color_lines='green')\n</pre> dataset_postprocessing = { \"Cells\": [         {\"time\": time, \"values\": obs},  # replicate 1     ] }  def ode_solution2data(solution):     total = solution[:, 0]  # Assuming the first column is the total cell count     return {         \"total\": total     }  posterior_dynamics(dataset=dataset_postprocessing,                    trace=trace,                    model=model,                    n_plots=100,                    burn_in=50,                    num_variables=1,                    ode_fn=ode,                    ode2data_fn=ode_solution2data,                    save_path=\"figures/vardi_logistic_growth_dynamics.png\",                    # the key of this dict is the variable name in the dataset_postprocessing                    var_properties={                        \"Cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}                   },                    suptitle=\"Posterior Predictive Dynamics\",                    color_lines='green') Out[9]: <pre>(&lt;Figure size 500x500 with 1 Axes&gt;,\n [&lt;Axes: title={'center': 'Total'}, xlabel='Time (days)', ylabel='Total cell density (/ml)'&gt;])</pre> <p>This plot is showing how well the range of estimated chains fit the observed data.</p> <p>CONGRATULATIONS! you just ran you first MCMC!</p> In\u00a0[10]: Copied! <pre>## Cell 6 ##\n\nimport pandas as pd\nimport numpy as np\ndata = pd.read_csv(\"../../../case_study_1/python/data/phaeocystis_control.csv\")\ntime = data['times'].values\nobs = data['cells'].values\nlog_obs = np.log(obs + 1e-9)\n</pre> ## Cell 6 ##  import pandas as pd import numpy as np data = pd.read_csv(\"../../../case_study_1/python/data/phaeocystis_control.csv\") time = data['times'].values obs = data['cells'].values log_obs = np.log(obs + 1e-9) <p>$$ \\frac{dy}{dt} = (\\mu - \\delta)\\, y $$</p> <p>Here we are simply subtracting a delta from mu so that delta can simulate death. In short, we don't always need death (or something else comperable) data. We can simply make our ODE expressive about the data!</p> In\u00a0[11]: Copied! <pre>## Cell 7 ##\n\ndef ode(y, t, params):\n    mum,delta = params[0],params[1]\n    return [(mum-delta) * y[0]]\n</pre> ## Cell 7 ##  def ode(y, t, params):     mum,delta = params[0],params[1]     return [(mum-delta) * y[0]] <p>Here we have our new model setup. We have two unique parameters (mum, delta) so we'll set n_theta to 2.</p> In\u00a0[12]: Copied! <pre>## Cell 8 ##\n\nimport pymc as pm\ncell_model = pm.ode.DifferentialEquation(\n    func=ode,\n    times=data['times'].values,\n    n_states=1,\n    n_theta=2,\n    t0=0\n)\n</pre> ## Cell 8 ##  import pymc as pm cell_model = pm.ode.DifferentialEquation(     func=ode,     times=data['times'].values,     n_states=1,     n_theta=2,     t0=0 ) <p>Here is the model. Everything here should make sense. If something is confusing see the explination above cell 4. Still note that we have two likelihoods at the bottom since we are estimating two states.</p> In\u00a0[13]: Copied! <pre>## Cell 9 ##\n\nimport numpy as np\nwith pm.Model() as model:\n    mum = pm.TruncatedNormal('mum', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)\n    delta = pm.TruncatedNormal('delta', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)\n    N0 = pm.Lognormal('N0', mu=log_obs[0], sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", 1)\n\n    sol = cell_model(y0=[N0], theta=[mum, delta])\n\n    pm.Normal(\"Y_obs\", \n              mu=pm.math.log(sol[:, 0] + 1e-9), \n              sigma=sigma, \n              observed=log_obs)\n</pre> ## Cell 9 ##  import numpy as np with pm.Model() as model:     mum = pm.TruncatedNormal('mum', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)     delta = pm.TruncatedNormal('delta', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)     N0 = pm.Lognormal('N0', mu=log_obs[0], sigma=0.1)     sigma = pm.HalfNormal(\"sigma\", 1)      sol = cell_model(y0=[N0], theta=[mum, delta])      pm.Normal(\"Y_obs\",                mu=pm.math.log(sol[:, 0] + 1e-9),                sigma=sigma,                observed=log_obs) <p>Everything here is the same as before! See explination above cell 5 if you forgot something.</p> In\u00a0[14]: Copied! <pre>## Cell 10 ##\n\nwith model:\n    trace = pm.sample(draws=1000, \n                      tune=1000, \n                      chains=4, \n                      cores=4,\n                      return_inferencedata=True,\n                      target_accept=0.95)\n</pre> ## Cell 10 ##  with model:     trace = pm.sample(draws=1000,                        tune=1000,                        chains=4,                        cores=4,                       return_inferencedata=True,                       target_accept=0.95) <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [mum, delta, N0, sigma]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 131 seconds.\n</pre> In\u00a0[15]: Copied! <pre>plot_trace(trace=trace,\n           model=model,\n           uni=[],\n           var_names_map={'N0':'Initial density/ml','mum': 'Growth Rate \u03bc (/day)', 'delta': 'Death Rate \u03b4 (/day)','sigma': 'Std Dev \u03c3'},\n           var_order=['mum','delta','N0','sigma'],\n           fontname='Arial',\n           fontsize=12,\n           num_prior_samples=2000,\n           save_path='figures/normal_growthdeath_chains.png')\n</pre> plot_trace(trace=trace,            model=model,            uni=[],            var_names_map={'N0':'Initial density/ml','mum': 'Growth Rate \u03bc (/day)', 'delta': 'Death Rate \u03b4 (/day)','sigma': 'Std Dev \u03c3'},            var_order=['mum','delta','N0','sigma'],            fontname='Arial',            fontsize=12,            num_prior_samples=2000,            save_path='figures/normal_growthdeath_chains.png') <pre>Sampling: [N0, Y_obs, delta, mum, sigma]\n</pre> <pre>mum\ndelta\nN0\nsigma\n</pre> Out[15]: <pre>array([[&lt;Axes: title={'center': 'Growth Rate \u03bc (/day)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Growth Rate \u03bc (/day)'}&gt;],\n       [&lt;Axes: title={'center': 'Death Rate \u03b4 (/day)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Death Rate \u03b4 (/day)'}&gt;],\n       [&lt;Axes: title={'center': 'Initial density/ml'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Initial density/ml'}&gt;],\n       [&lt;Axes: title={'center': 'Std Dev \u03c3'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Std Dev \u03c3'}&gt;]], dtype=object)</pre> In\u00a0[16]: Copied! <pre>dataset_postprocessing = {\n\"Cells\": [\n        {\"time\": time, \"values\": obs},  # replicate 1\n    ]\n}\n\ndef ode_solution2data(solution):    \n    total = solution[:, 0]  # Assuming the first column is the total cell count\n    return {\n        \"total\": total\n    }\n\nposterior_dynamics(dataset=dataset_postprocessing,\n                   trace=trace,\n                   model=model,\n                   n_plots=100,\n                   burn_in=50,\n                   num_variables=1,\n                   ode_fn=ode,\n                   ode2data_fn=ode_solution2data,\n                   save_path=\"figures/vardi_logistic_growthdeath_dynamics.png\",\n                   # the key of this dict is the variable name in the dataset_postprocessing\n                   var_properties={\n                       \"Cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}\n                   },\n                   suptitle=\"Posterior Predictive Dynamics\",\n                   color_lines='red')\n</pre> dataset_postprocessing = { \"Cells\": [         {\"time\": time, \"values\": obs},  # replicate 1     ] }  def ode_solution2data(solution):         total = solution[:, 0]  # Assuming the first column is the total cell count     return {         \"total\": total     }  posterior_dynamics(dataset=dataset_postprocessing,                    trace=trace,                    model=model,                    n_plots=100,                    burn_in=50,                    num_variables=1,                    ode_fn=ode,                    ode2data_fn=ode_solution2data,                    save_path=\"figures/vardi_logistic_growthdeath_dynamics.png\",                    # the key of this dict is the variable name in the dataset_postprocessing                    var_properties={                        \"Cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}                    },                    suptitle=\"Posterior Predictive Dynamics\",                    color_lines='red') Out[16]: <pre>(&lt;Figure size 500x500 with 1 Axes&gt;,\n [&lt;Axes: title={'center': 'Total'}, xlabel='Time (days)', ylabel='Total cell density (/ml)'&gt;])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"education/python/case_study_1/#case-study-1-exponential-growth","title":"Case Study 1: Exponential Growth\u00b6","text":"<p>We are going to establish an exponential model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! Pandas is the defacto python package for reading in data. It's also smart to handle any necessary data conversions during this step. Our data spans many magnitudes, so we will go ahead and perform our log conversion now. You may not need to and that's just fine! It all depends on what you are looking for.</p> <p>Note: The \"+ 1e-9\" is merely a fail-safe against divide-by-zero errors when doing a log transform. Adding a value infitesimally near 0 makes no impact on the data itself and prevents the aformentioned error. Good quick trick to know!</p>"},{"location":"education/python/case_study_1/#case-study-1-exponential-growth-and-death","title":"Case Study 1: Exponential Growth and Death\u00b6","text":"<p>Now on to a more complex model. We will be using the same data as before.</p>"},{"location":"education/python/case_study_2/","title":"Case Study 2 \u2014 Logistic growth and death","text":"In\u00a0[1]: Copied! <pre># For user-defined post processing and plotting functions\nimport sys\nsys.path.insert(0, \"../../..\")\nfrom utils.plot_utils_v2 import *\n</pre> # For user-defined post processing and plotting functions import sys sys.path.insert(0, \"../../..\") from utils.plot_utils_v2 import * <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using PYMC. A python software package allowing you to fit complex models with ease.</p> In\u00a0[2]: Copied! <pre>## Cell 1 ##\n\nimport pandas as pd\n\n# import alive\ndataset = pd.read_csv(\"../../../case_study_2/python/data/total_cells.csv\")\ncells = dataset.tail(15)\ncells_time = cells['Time (days)'].values\ncells_density = 1e6*cells[' Density (1e6/ml)'].values\nlog_cells_density = np.log(cells_density + 1e-9)\n</pre> ## Cell 1 ##  import pandas as pd  # import alive dataset = pd.read_csv(\"../../../case_study_2/python/data/total_cells.csv\") cells = dataset.tail(15) cells_time = cells['Time (days)'].values cells_density = 1e6*cells[' Density (1e6/ml)'].values log_cells_density = np.log(cells_density + 1e-9) <p>First things first, the ODE system. We will setup the logistic ODE that will adapt to the data. To do this, you will use variables that PyMC gives you by 'default'. Let me explain. Note that in Cell 2 there are 3 parameters being passed in (y, t, params). You will not see these variables being explicitly passed in this way anywhere. Here is what happens. When you create the ODE function in Cell 2, it gets passed to the the 'cell_model' in Cell 3. Now look to where the 'cell_model' is called in cell 4. There it passes priors into Y0 and theta. The connection is Y0 goes to y, and theta goes to params. The order you hand over the priors matters so call them in the correct order when creating the ODE equation. Never forget to be mindful of your prior's orientations when you pass them!</p> <p>P: Initial Value r: Growith Rate K: Carrying Capacity</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P $$</p> In\u00a0[3]: Copied! <pre>## Cell 2 ##\n\ndef logistic_growth(y, t, params):\n    P = y[0]\n    \n    r = params[0]\n    K = params[1]\n    \n    return r * (1 - P / K) * P\n</pre> ## Cell 2 ##  def logistic_growth(y, t, params):     P = y[0]          r = params[0]     K = params[1]          return r * (1 - P / K) * P  <p>Here is the model we spoke about earlier. Here is what goes into each variable.</p> <p>func: the ODE function you just created times: the time steps from your data n_states: possible states count n_theta: Unique parameter count t0: inital time step (should almost always be 0)</p> In\u00a0[4]: Copied! <pre>## Cell 3 ##\n\nimport pymc as pm\ncell_model = pm.ode.DifferentialEquation(\n    func=logistic_growth,\n    times=cells_time,\n    n_states=1,\n    n_theta=2,\n    t0=0\n)\n</pre> ## Cell 3 ##  import pymc as pm cell_model = pm.ode.DifferentialEquation(     func=logistic_growth,     times=cells_time,     n_states=1,     n_theta=2,     t0=0 ) <p>This is where we create the MCMC 'loop': Sample Priors -&gt; Estimate ODE @ times -&gt; check fit</p> <p>First, priors. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. For our parameters and initial condition we choose uniform priors. These uniform priors say, \"we believe the proper value for the model will be equally likely within this range and WILL NOT be outside of it\". For our sigma terms (error), we choose a halfnormal distribution. This is usually used for your error or standard deviation since neither can be negative and you can tune how quickly it tails off. Below are the priors visualized.</p> <p></p> <p>After the priors are declared the next function call will be for the solution of the model given its current estimates. We spoke about how to pass the priors in previously. Be careful during this step!</p> <p>The final function is for the likelihood estimation. This is where the model tests how well it estimated the observed data. We pass it our log transformed estimated data, the sigma value from the iteration's sample, and log transformed observed data. Now just let PyMC work its magic!</p> In\u00a0[5]: Copied! <pre>## Cell 4 ##\n\nimport numpy as np\nwith pm.Model() as model:\n    r = pm.Uniform(r\"$r$ (growth rate)\", lower=0.5, upper=1)\n    K = pm.Uniform(r\"$K$ (carrying capacity)\" , lower=1e6, upper=4e7)\n    P0 = pm.Uniform(r\"$P_0$ (init. live)\", lower=1e5, upper=3e5)\n    sigma_live = pm.HalfNormal(r\"$\\sigma_L$\", 3)\n\n    sol = cell_model(y0=[P0], theta=[r,K])\n    \n    pm.Normal(\"Y_total\", \n              mu=pm.math.log(sol[:, 0] + 1e-9),\n              sigma=sigma_live,\n              observed=log_cells_density)\n</pre> ## Cell 4 ##  import numpy as np with pm.Model() as model:     r = pm.Uniform(r\"$r$ (growth rate)\", lower=0.5, upper=1)     K = pm.Uniform(r\"$K$ (carrying capacity)\" , lower=1e6, upper=4e7)     P0 = pm.Uniform(r\"$P_0$ (init. live)\", lower=1e5, upper=3e5)     sigma_live = pm.HalfNormal(r\"$\\sigma_L$\", 3)      sol = cell_model(y0=[P0], theta=[r,K])          pm.Normal(\"Y_total\",                mu=pm.math.log(sol[:, 0] + 1e-9),               sigma=sigma_live,               observed=log_cells_density)  <p>This is how you call and run the model!</p> <p>draws: poterior values to be generated. tune: Some under the hood PyMC magic that 'warms it up'. Keep this between 500 and 2000 depending on how complex your model is. chains / cores: Amount of chains you want to compute in parallel. Keep these values the same for most basic usage. PyMC recommends you run at least 4 chains. return_inferencedata: returns an ARVIZ inferencedata object which we'll use for easy reading of our output. target_accept: the % of times you accept the improved likelihood at each step of the chain. This is your exploration vs exploitation balance.</p> In\u00a0[6]: Copied! <pre>## Cell 5 ##\n\nwith model:\n    trace = pm.sample(draws=1000, \n                      tune=1000, \n                      chains=4, \n                      cores=4,\n                      return_inferencedata=True, \n                      target_accept=0.95)\n</pre> ## Cell 5 ##  with model:     trace = pm.sample(draws=1000,                        tune=1000,                        chains=4,                        cores=4,                       return_inferencedata=True,                        target_accept=0.95)  <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [$r$ (growth rate), $K$ (carrying capacity), $P_0$ (init. live), $\\sigma_L$]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 222 seconds.\n</pre> <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[7]: Copied! <pre>plot_trace(trace=trace,\n           model=model,\n           uni=[\"$r$ (growth rate)\", \"$K$ (carrying capacity)\", \"$P_0$ (init. live)\"],\n           fontname='Arial',\n           fontsize=12,\n           num_prior_samples=200,\n           save_path='figures/vardi_logistic_growth_chains.png')\n</pre> plot_trace(trace=trace,            model=model,            uni=[\"$r$ (growth rate)\", \"$K$ (carrying capacity)\", \"$P_0$ (init. live)\"],            fontname='Arial',            fontsize=12,            num_prior_samples=200,            save_path='figures/vardi_logistic_growth_chains.png') <pre>Sampling: [$K$ (carrying capacity), $P_0$ (init. live), $\\sigma_L$, $r$ (growth rate), Y_total]\n</pre> <pre>$r$ (growth rate)\n$K$ (carrying capacity)\n$P_0$ (init. live)\n$\\sigma_L$\n</pre> Out[7]: <pre>array([[&lt;Axes: title={'center': '$r$ (growth rate)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$r$ (growth rate)'}&gt;],\n       [&lt;Axes: title={'center': '$K$ (carrying capacity)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$K$ (carrying capacity)'}&gt;],\n       [&lt;Axes: title={'center': '$P_0$ (init. live)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$P_0$ (init. live)'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\sigma_L$'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$\\\\sigma_L$'}&gt;]], dtype=object)</pre> <p>Now we interpret the results. Remember the ODE's from before.</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P $$</p> <p>Take note of x-axis values at the peaks of the PDFs and the variables they represent. The x-axis values at those peaks are what the model found to be the optimal value for the equation. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space well enough.</p> In\u00a0[8]: Copied! <pre>dataset_postprocessing = {\n\"Total cells\": [\n        {\"time\": cells_time, \"values\":  cells_density},  # replicate 1\n    ]\n}\n\ndef ode_solution2data(solution):\n    total = solution[:, 0]  # Assuming the first column is the total cell count\n    return {\n        \"total\": total\n    }\n\nposterior_dynamics(dataset=dataset_postprocessing,\n                   trace=trace,\n                   model=model,\n                   n_plots=100,\n                   burn_in=50,\n                   num_variables=1,\n                   ode_fn=logistic_growth,\n                   ode2data_fn=ode_solution2data,\n                   save_path=\"figures/vardi_logistic_growth_dynamics.png\",\n                   var_properties={\"Total cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}},\n                   suptitle=\"Posterior Predictive Dynamics\")\n</pre> dataset_postprocessing = { \"Total cells\": [         {\"time\": cells_time, \"values\":  cells_density},  # replicate 1     ] }  def ode_solution2data(solution):     total = solution[:, 0]  # Assuming the first column is the total cell count     return {         \"total\": total     }  posterior_dynamics(dataset=dataset_postprocessing,                    trace=trace,                    model=model,                    n_plots=100,                    burn_in=50,                    num_variables=1,                    ode_fn=logistic_growth,                    ode2data_fn=ode_solution2data,                    save_path=\"figures/vardi_logistic_growth_dynamics.png\",                    var_properties={\"Total cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}},                    suptitle=\"Posterior Predictive Dynamics\") Out[8]: <pre>(&lt;Figure size 500x500 with 1 Axes&gt;,\n [&lt;Axes: title={'center': 'Total'}, xlabel='Time (days)', ylabel='Total cell density (/ml)'&gt;])</pre> <p>This plot is showing how well the range of estimated chains fit the observed data.</p> <p>CONGRATULATIONS! you just ran you first MCMC!</p> In\u00a0[9]: Copied! <pre>## Cell 6 ##\n\nimport pandas as pd\n\n# import alive\ndataset = pd.read_csv(\"../../../case_study_2/python/data/total_cells.csv\")\ncells = dataset.tail(15)\ncells_time = cells['Time (days)'].values\ncells_density = 1e6*cells[' Density (1e6/ml)'].values\nlog_cells_density = np.log(cells_density + 1e-9)\n\n# import dead\ndeath_dataset = pd.read_csv(\"../../../case_study_2/python/data/death_percentage.csv\")\ndeath = death_dataset.tail(15)\ndead_time = death['Time (days)'].values\ndead_density = death[' Dead percentage '].values*cells_density/100\nlog_dead_density = np.log(dead_density + 1e-9)\n</pre> ## Cell 6 ##  import pandas as pd  # import alive dataset = pd.read_csv(\"../../../case_study_2/python/data/total_cells.csv\") cells = dataset.tail(15) cells_time = cells['Time (days)'].values cells_density = 1e6*cells[' Density (1e6/ml)'].values log_cells_density = np.log(cells_density + 1e-9)  # import dead death_dataset = pd.read_csv(\"../../../case_study_2/python/data/death_percentage.csv\") death = death_dataset.tail(15) dead_time = death['Time (days)'].values dead_density = death[' Dead percentage '].values*cells_density/100 log_dead_density = np.log(dead_density + 1e-9)  <p>ODE for living cells accounting for cell death</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P - \\delta P $$</p> <p>ODE for dead cells</p> <p>$$ \\frac{dD}{dt} = \\delta P $$</p> <p>We follow the same rules as before when creating this ODE. The new part of this is how we return the equation to PYMC (As an ODE system). Note that in our previous ODE we returned an equation of lists to to PyMC that looked like</p> <p>[] = [] * (1 = [] / []) * []</p> <p>Since PyMC expects a matrix as a return value, it was able to accept the 1-D column right there since that is still a matrix.</p> <p>We take advantage of this when we have multiple equations. All you have to do is expand the matrix.</p> <p>x[[],[]] x[0] = [] * (1 = [] / []) * [] - [] * [] x[1] = [] * [] give x to PyMC</p> In\u00a0[10]: Copied! <pre>## Cell 7 ##\n\ndef logistic_growth_death(y, t, params):\n    P = y[0]\n    r = params[0]\n    K = params[1]\n    delta = params[2]\n\n    dydt = []\n    dydt.append(r * (1 - P / K) * P - delta * P)\n    dydt.append(delta * P)\n    return dydt\n</pre> ## Cell 7 ##  def logistic_growth_death(y, t, params):     P = y[0]     r = params[0]     K = params[1]     delta = params[2]      dydt = []     dydt.append(r * (1 - P / K) * P - delta * P)     dydt.append(delta * P)     return dydt <p>Here we have our new model setup. We have 3 unique parameters (r, K, delta) so we'll set n_theta to 3. n_states will now be 2 since we now have two ODE's we are tracking two states (alive, dead).</p> <p>note: We will end up with 5 priors for the 5 variables just mentioned. Plus two more for sigma_live and sigma_dead. Totaling 7.</p> In\u00a0[11]: Copied! <pre>## Cell 8 ##\nimport pymc as pm\ncell_model = pm.ode.DifferentialEquation(\n    func=logistic_growth_death,\n    times=cells_time,\n    n_states=2,\n    n_theta=3, \n    t0=0\n)\n</pre> ## Cell 8 ## import pymc as pm cell_model = pm.ode.DifferentialEquation(     func=logistic_growth_death,     times=cells_time,     n_states=2,     n_theta=3,      t0=0 ) <p>Here is the model. Everything here should make sense. If something is confusing see the explination above cell 4. Still note that we have two likelihoods at the bottom since we are estimating two states.</p> In\u00a0[12]: Copied! <pre>## Cell 10 ##\nimport numpy as np\nwith pm.Model() as model:\n    r = pm.Uniform(r\"$r$ (growth rate)\", lower=0.5, upper=1)\n    K = pm.Uniform(r\"$K$ (carrying capacity)\" , lower=1e6, upper=4e7)\n    delta = pm.Uniform(r\"$\\delta$ (death rate)\", lower=0.0, upper=0.15)\n    P0 = pm.Uniform(r\"$P_0$ (init. live)\", lower=1e5, upper=3e5)\n    D0 = pm.Uniform(r\"$D_0$ (init. dead)\", lower=1e4, upper=7e4)\n\n    sigma_live = pm.HalfNormal(r\"$\\sigma_L$\", 3)\n    sigma_dead = pm.HalfNormal(r\"$\\sigma_D$\", 3)\n\n    sol = cell_model(y0=[P0,D0], theta=[r,K,delta])\n    total_solution = sol[:,0] + sol[:,1]\n    dead_solution = sol[:,1]\n\n    pm.Normal(\"Y_live\", mu=pm.math.log(total_solution + 1e-9),sigma=sigma_live,\n            observed=log_cells_density)\n        \n    pm.Normal(\"Y_dead\", mu=pm.math.log(dead_solution + 1e-9),sigma=sigma_dead,\n            observed=log_dead_density)\n</pre> ## Cell 10 ## import numpy as np with pm.Model() as model:     r = pm.Uniform(r\"$r$ (growth rate)\", lower=0.5, upper=1)     K = pm.Uniform(r\"$K$ (carrying capacity)\" , lower=1e6, upper=4e7)     delta = pm.Uniform(r\"$\\delta$ (death rate)\", lower=0.0, upper=0.15)     P0 = pm.Uniform(r\"$P_0$ (init. live)\", lower=1e5, upper=3e5)     D0 = pm.Uniform(r\"$D_0$ (init. dead)\", lower=1e4, upper=7e4)      sigma_live = pm.HalfNormal(r\"$\\sigma_L$\", 3)     sigma_dead = pm.HalfNormal(r\"$\\sigma_D$\", 3)      sol = cell_model(y0=[P0,D0], theta=[r,K,delta])     total_solution = sol[:,0] + sol[:,1]     dead_solution = sol[:,1]      pm.Normal(\"Y_live\", mu=pm.math.log(total_solution + 1e-9),sigma=sigma_live,             observed=log_cells_density)              pm.Normal(\"Y_dead\", mu=pm.math.log(dead_solution + 1e-9),sigma=sigma_dead,             observed=log_dead_density) <p>Everything here is that same as before! See the explination above cell 5 if you forgot something.</p> In\u00a0[13]: Copied! <pre>## Cell 11 ##\n\nwith model:\n    trace = pm.sample(draws=1000, \n                      tune=1000, \n                      chains=4, \n                      cores=4, \n                      return_inferencedata=True, \n                      target_accept=0.95)\n</pre> ## Cell 11 ##  with model:     trace = pm.sample(draws=1000,                        tune=1000,                        chains=4,                        cores=4,                        return_inferencedata=True,                        target_accept=0.95)  <pre>Initializing NUTS using jitter+adapt_diag...\nMultiprocess sampling (4 chains in 4 jobs)\nNUTS: [$r$ (growth rate), $K$ (carrying capacity), $\\delta$ (death rate), $P_0$ (init. live), $D_0$ (init. dead), $\\sigma_L$, $\\sigma_D$]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 633 seconds.\n</pre> In\u00a0[14]: Copied! <pre>plot_trace(trace=trace,\n           model=model,\n           uni=[\"$r$ (growth rate)\", \"$K$ (carrying capacity)\", \"$\\delta$ (death rate)\", \"$P_0$ (init. live)\", \"$D_0$ (init. dead)\"],\n           fontname='Arial',\n           fontsize=12,\n           num_prior_samples=2000,\n           save_path='figures/vardi_growth_death_chains.png')\n</pre> plot_trace(trace=trace,            model=model,            uni=[\"$r$ (growth rate)\", \"$K$ (carrying capacity)\", \"$\\delta$ (death rate)\", \"$P_0$ (init. live)\", \"$D_0$ (init. dead)\"],            fontname='Arial',            fontsize=12,            num_prior_samples=2000,            save_path='figures/vardi_growth_death_chains.png') <pre>&lt;&gt;:3: SyntaxWarning: invalid escape sequence '\\d'\n&lt;&gt;:3: SyntaxWarning: invalid escape sequence '\\d'\nC:\\Users\\Whisk\\AppData\\Local\\Temp\\ipykernel_39824\\4226420622.py:3: SyntaxWarning: invalid escape sequence '\\d'\n  uni=[\"$r$ (growth rate)\", \"$K$ (carrying capacity)\", \"$\\delta$ (death rate)\", \"$P_0$ (init. live)\", \"$D_0$ (init. dead)\"],\nSampling: [$D_0$ (init. dead), $K$ (carrying capacity), $P_0$ (init. live), $\\delta$ (death rate), $\\sigma_D$, $\\sigma_L$, $r$ (growth rate), Y_dead, Y_live]\n</pre> <pre>$r$ (growth rate)\n$K$ (carrying capacity)\n$\\delta$ (death rate)\n$P_0$ (init. live)\n$D_0$ (init. dead)\n$\\sigma_L$\n$\\sigma_D$\n</pre> Out[14]: <pre>array([[&lt;Axes: title={'center': '$r$ (growth rate)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$r$ (growth rate)'}&gt;],\n       [&lt;Axes: title={'center': '$K$ (carrying capacity)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$K$ (carrying capacity)'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\delta$ (death rate)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$\\\\delta$ (death rate)'}&gt;],\n       [&lt;Axes: title={'center': '$P_0$ (init. live)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$P_0$ (init. live)'}&gt;],\n       [&lt;Axes: title={'center': '$D_0$ (init. dead)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$D_0$ (init. dead)'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\sigma_L$'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$\\\\sigma_L$'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\sigma_D$'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$\\\\sigma_D$'}&gt;]], dtype=object)</pre> In\u00a0[15]: Copied! <pre>dataset_postprocessing = {\n\"Total cells\": [\n        {\"time\": cells_time, \"values\":  cells_density},  # replicate 1\n    ],\n\"Dead cells\": [\n        {\"time\": dead_time, \"values\": dead_density},  # replicate 1\n    ]\n}\n\ndef ode_solution2data(solution):\n    live = solution[:, 0]\n    dead = solution[:, 1]\n    total = live + dead\n    return {\n        \"total\": total,\n        \"dead\": dead\n    }\n\nposterior_dynamics(dataset=dataset_postprocessing,\n                   trace=trace,\n                   model=model,\n                   n_plots=100,\n                   burn_in=50,\n                   num_variables=2,\n                   ode_fn=logistic_growth_death,\n                   ode2data_fn=ode_solution2data,\n                   save_path=\"figures/vardi_growth_death_dynamics.png\",\n                   var_properties={\n                       \"Total cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\", \"sol_key\": \"total\",\"log\": True},\n                       \"Dead cells\": {\"label\": \"Dead\", \"color\": \"black\", \"ylabel\": \"Dead cell density (/ml)\", \"xlabel\":\"Time (days)\", \"sol_key\": \"dead\",\"log\": True},\n                   },\n                   suptitle=\"Posterior Predictive Dynamics\")\n</pre> dataset_postprocessing = { \"Total cells\": [         {\"time\": cells_time, \"values\":  cells_density},  # replicate 1     ], \"Dead cells\": [         {\"time\": dead_time, \"values\": dead_density},  # replicate 1     ] }  def ode_solution2data(solution):     live = solution[:, 0]     dead = solution[:, 1]     total = live + dead     return {         \"total\": total,         \"dead\": dead     }  posterior_dynamics(dataset=dataset_postprocessing,                    trace=trace,                    model=model,                    n_plots=100,                    burn_in=50,                    num_variables=2,                    ode_fn=logistic_growth_death,                    ode2data_fn=ode_solution2data,                    save_path=\"figures/vardi_growth_death_dynamics.png\",                    var_properties={                        \"Total cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\", \"sol_key\": \"total\",\"log\": True},                        \"Dead cells\": {\"label\": \"Dead\", \"color\": \"black\", \"ylabel\": \"Dead cell density (/ml)\", \"xlabel\":\"Time (days)\", \"sol_key\": \"dead\",\"log\": True},                    },                    suptitle=\"Posterior Predictive Dynamics\") Out[15]: <pre>(&lt;Figure size 1000x500 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'Total'}, xlabel='Time (days)', ylabel='Total cell density (/ml)'&gt;,\n        &lt;Axes: title={'center': 'Dead'}, xlabel='Time (days)', ylabel='Dead cell density (/ml)'&gt;],\n       dtype=object))</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"education/python/case_study_2/#case-study-2-logistic-growth","title":"Case Study 2: Logistic Growth\u00b6","text":"<p>We are going to establish a logistic model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! Pandas is the defacto python package for reading in data. It's also smart to handle any necessary data conversions during this step. Our data spans many magnitudes, so we will go ahead and perform our log conversion now. You may not need to and that's just fine! You may not need to and that's just fine! It all depends on what you are looking for.</p> <p>Note: The \"+ 1e-9\" is merely a fail-safe against divide-by-zero errors when doing a log transform. Adding a value infitesimally near 0 makes no impact on the data itself and prevents the aformentioned error. Good quick trick to know!</p>"},{"location":"education/python/case_study_2/#case-study-2-logistic-growth-and-death","title":"Case Study 2: Logistic Growth and Death\u00b6","text":"<p>Now on to a more complex model. We will be using the same data as before alongside new dead cell data. We will also being using two ODEs!</p>"},{"location":"education/python/case_study_3/","title":"Case Study 3 \u2014 Resource explicit Monod growth and death","text":"In\u00a0[1]: Copied! <pre># For user-defined post processing and plotting functions\nimport sys\nsys.path.insert(0, \"../../..\")\nfrom utils.plot_utils_v2 import *\n</pre> # For user-defined post processing and plotting functions import sys sys.path.insert(0, \"../../..\") from utils.plot_utils_v2 import * <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using PYMC. A python software package allowing you to fit complex models with ease.</p> In\u00a0[2]: Copied! <pre>## Cell 1 ##\n\nimport pandas as pd\nimport numpy as np\n\ndataset = pd.read_csv(\"../../../case_study_3/python/data/total_cells.csv\")\nehux_cells = dataset.tail(15)\nehux_total_time = ehux_cells['Time (days)'].values\nehux_total_density = 1e6*ehux_cells[' Density (1e6/ml)'].values\nlog_ehux_total_density = np.log(ehux_total_density + 1e-9)\n\ndeath_dataset = pd.read_csv(\"../../../case_study_3/python/data/death_percentage.csv\")\nehux_death = death_dataset.tail(15)\nehux_death_time = ehux_death['Time (days)'].values\nehux_dead_density = ehux_death[' Dead percentage '].values*ehux_total_density/100\nlog_ehux_dead_density = np.log(ehux_dead_density + 1e-9)\n</pre> ## Cell 1 ##  import pandas as pd import numpy as np  dataset = pd.read_csv(\"../../../case_study_3/python/data/total_cells.csv\") ehux_cells = dataset.tail(15) ehux_total_time = ehux_cells['Time (days)'].values ehux_total_density = 1e6*ehux_cells[' Density (1e6/ml)'].values log_ehux_total_density = np.log(ehux_total_density + 1e-9)  death_dataset = pd.read_csv(\"../../../case_study_3/python/data/death_percentage.csv\") ehux_death = death_dataset.tail(15) ehux_death_time = ehux_death['Time (days)'].values ehux_dead_density = ehux_death[' Dead percentage '].values*ehux_total_density/100 log_ehux_dead_density = np.log(ehux_dead_density + 1e-9) <p>Here is the first thing that needs to be done. We will setup the ODE system that will adapt to the data. To do this, you will use variables that PyMC gives you by 'default' (y, t, params).</p> <p>mu_max: Maximum Specific Growth Rate Ks: Half-Saturation Constant Qn: Nutrient Consumption per Cell delta: Death Rate P_m3: Unit Conversion</p> <p>$$ \\begin{aligned}   \\mu(N) &amp;= \\mu_{\\text{max}} \\frac{N}{K_s + N} \\\\ \\frac{dN}{dt} &amp;= - Q_n \\, \\mu(N) \\, P_{\\text{m}^3} \\\\ \\frac{dP}{dt} &amp;= \\mu(N) \\, P - \\delta \\, P \\\\ \\frac{dD}{dt} &amp;= \\delta \\, P \\\\ \\end{aligned}   $$</p> In\u00a0[3]: Copied! <pre>## Cell 2 ##\n\ndef general_case(t, y, params):\n    N, P, D = y\n    mu_max = params[0]\n    Ks = params[1]\n    Qn = params[2]\n    delta = params[3]\n    P_m3 = P * 1e6  # cells/mL \u2192 cells/m\u00b3\n\n    mu = mu_max * N / (N + Ks)\n    dNdt = -Qn * mu * P_m3\n    dPdt = mu * P - delta * P\n    dDdt = delta * P\n   \n    return [dNdt, dPdt, dDdt]\n</pre> ## Cell 2 ##  def general_case(t, y, params):     N, P, D = y     mu_max = params[0]     Ks = params[1]     Qn = params[2]     delta = params[3]     P_m3 = P * 1e6  # cells/mL \u2192 cells/m\u00b3      mu = mu_max * N / (N + Ks)     dNdt = -Qn * mu * P_m3     dPdt = mu * P - delta * P     dDdt = delta * P         return [dNdt, dPdt, dDdt]  <p>Here is where this case becomes unique. This ODE system is stiff. This happens when rates of change within a system vary drastically. This will cause the process to lock up if you don't use a proper sampler / solver. Here is how we chose to solve the issue.</p> <p>We are still going to use PyMC, however, instead of using the integrated solver we will use an external SciPy ODE solver to get around the stiffness issue. We declare a custom operator (Op) with one input vector (itypes) and one output matrix (otypes). It will be initialized with your observed times. perform is the default call for the runtime computation of the Op. See in Cell 5 how the parameters and inital values are passed to the Op. The first 4 are the parameters and last 3 are the initial conditions (theta and y0).</p> <p>The solve_ivp call should follow this form. The inputs to the function are as follows:</p> <p>fun: The ODE function(s) t_span: first and last time y0: Initial Values t_eval: exact time points the ODE will be evaluated at method: This is up to the user but for our case we chose LSODA. It auto-detects stiffness and switches between Adams (non-stiff) and BDF (stiff) methods.</p> <p>Lastly, the output is transposed to match the form PyMC expects.</p> In\u00a0[4]: Copied! <pre>## Cell 3 ##\n\nimport pytensor.tensor as pt\nfrom pytensor.graph.op import Op\nfrom scipy.integrate import solve_ivp\n\nclass SolveIVPWrapper(Op):\n    itypes = [pt.dvector]  # theta + y0\n    otypes = [pt.dmatrix]  # solution: (len(t), 3)\n\n    def __init__(self, times):\n        self.times = times\n\n    def perform(self, node, inputs, outputs):\n        theta_y0, = inputs\n        theta = theta_y0[:4] # first 4\n        y0 = theta_y0[4:] # last 3\n\n        sol = solve_ivp(\n            fun=lambda t, y: general_case(t, y, theta),\n            t_span=(self.times[0], self.times[-1]),\n            y0=y0,\n            t_eval=self.times,\n            method=\"LSODA\"\n        )\n\n        if not sol.success:\n            raise RuntimeError(\"ODE solver failed:\", sol.message)\n\n        outputs[0][0] = sol.y.T  # shape: (time, 3)\n</pre> ## Cell 3 ##  import pytensor.tensor as pt from pytensor.graph.op import Op from scipy.integrate import solve_ivp  class SolveIVPWrapper(Op):     itypes = [pt.dvector]  # theta + y0     otypes = [pt.dmatrix]  # solution: (len(t), 3)      def __init__(self, times):         self.times = times      def perform(self, node, inputs, outputs):         theta_y0, = inputs         theta = theta_y0[:4] # first 4         y0 = theta_y0[4:] # last 3          sol = solve_ivp(             fun=lambda t, y: general_case(t, y, theta),             t_span=(self.times[0], self.times[-1]),             y0=y0,             t_eval=self.times,             method=\"LSODA\"         )          if not sol.success:             raise RuntimeError(\"ODE solver failed:\", sol.message)          outputs[0][0] = sol.y.T  # shape: (time, 3) <p>This is where we create the MCMC 'loop': Sample Priors -&gt; Estimate ODE @ times -&gt; check fit</p> <p>First, proirs. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. For our parameters we choose uniform priors. These uniform priors say, \"we believe the proper value for the model will be equally likely within this range and WILL NOT be outside of it\". For our initial conditions we choose a lognormal distribution. This says, \"we believe our value is around some x given a normalesque strictly positive distribution\". For our sigma terms (error), we choose a halfnormal distribution. This is usually used for your error or standard deviation since neither can be negative and you can tune how quickly it tails off. Below are the priors visualized.</p> <p></p> <p>After the priors are declared the next function call will be for the solution of the model given its current estimates. Pay attention to the order in which you pass your priors!</p> <p>The final functions are the likelihood estimations of the model's states. This is where the model tests how well it estimated the observed data. We pass it our log transformed estimated data, the sigma values from the iteration's sample, and log transformed observed data. Now just let PyMC simply work its magic!</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 5 ##\n\nimport pymc as pm\n\node_op = SolveIVPWrapper(ehux_total_time)\n\nwith pm.Model() as model:\n    mu_max = pm.Uniform(\"mu_max\", 0.4, 0.7)\n    Ks = pm.Uniform(\"Ks\", 0.05, 0.2)\n    Qn = pm.Uniform(\"Qn\", 1e-10, 7e-10)\n    delta = pm.Uniform(\"delta\", 0.01, 0.09)\n\n    # N0 isn't a 'prior' distribution. It is deterministic. Meaing, N0 will be a \n    # variable tracked by the model, but at each iteration it exists as a function\n    # of Qn. \n    N0 = pm.Deterministic(\"N0\", 1000 + ((500 / 1.8e-10) * (Qn - 3.2e-10)))\n    \n    P0 = pm.LogNormal(\"P0\", mu=12.2175, sigma=0.1)\n    D0 = pm.LogNormal(\"D0\", mu=10.2804, sigma=0.1)\n\n    sigma_live = pm.HalfNormal(\"sigma_live\", 1)\n    sigma_dead = pm.HalfNormal(\"sigma_dead\", 1)\n\n    # Solve ODE\n    sol = ode_op(pt.stack([mu_max, Ks, Qn, delta, N0, P0, D0]))\n\n    total = sol[:, 1] + sol[:, 2]\n    dead = sol[:, 2]\n\n    pm.Normal(\"Y_total\", mu=pm.math.log(total + 1e-9), sigma=sigma_live, observed=log_ehux_total_density)\n    pm.Normal(\"Y_dead\", mu=pm.math.log(dead + 1e-9), sigma=sigma_dead, observed=log_ehux_dead_density)\n</pre> ## Cell 5 ##  import pymc as pm  ode_op = SolveIVPWrapper(ehux_total_time)  with pm.Model() as model:     mu_max = pm.Uniform(\"mu_max\", 0.4, 0.7)     Ks = pm.Uniform(\"Ks\", 0.05, 0.2)     Qn = pm.Uniform(\"Qn\", 1e-10, 7e-10)     delta = pm.Uniform(\"delta\", 0.01, 0.09)      # N0 isn't a 'prior' distribution. It is deterministic. Meaing, N0 will be a      # variable tracked by the model, but at each iteration it exists as a function     # of Qn.      N0 = pm.Deterministic(\"N0\", 1000 + ((500 / 1.8e-10) * (Qn - 3.2e-10)))          P0 = pm.LogNormal(\"P0\", mu=12.2175, sigma=0.1)     D0 = pm.LogNormal(\"D0\", mu=10.2804, sigma=0.1)      sigma_live = pm.HalfNormal(\"sigma_live\", 1)     sigma_dead = pm.HalfNormal(\"sigma_dead\", 1)      # Solve ODE     sol = ode_op(pt.stack([mu_max, Ks, Qn, delta, N0, P0, D0]))      total = sol[:, 1] + sol[:, 2]     dead = sol[:, 2]      pm.Normal(\"Y_total\", mu=pm.math.log(total + 1e-9), sigma=sigma_live, observed=log_ehux_total_density)     pm.Normal(\"Y_dead\", mu=pm.math.log(dead + 1e-9), sigma=sigma_dead, observed=log_ehux_dead_density) <p>This is how you call and run the model!</p> <p>draws: poterior values to be generated. tune: Some under the hood PyMC magic that 'warms it up'. Keep this between 500 and 2000 depending on how complex your model is. chains / cores: Amount of chains you want to compute in parallel. Keep these values the same for most basic usage. PyMC recommends you run at least 4 chains. step: By default PyMC uses NUTS (No U-Turn Sampler). If you want to use something other than NUTS this is where you declare it. return_inferencedata: returns an ARVIZ inferencedata object which we'll use for easy reading of our output.</p> In\u00a0[10]: Copied! <pre>## Cell 6 ##\n\nwith model:\n    trace = pm.sample(draws=10000, \n                      tune=1000, \n                      chains=4, \n                      cores=4,\n                      step=pm.Metropolis(),\n                      return_inferencedata=True)\n</pre> ## Cell 6 ##  with model:     trace = pm.sample(draws=10000,                        tune=1000,                        chains=4,                        cores=4,                       step=pm.Metropolis(),                       return_inferencedata=True) <pre>Multiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;Metropolis: [mu_max]\n&gt;Metropolis: [Ks]\n&gt;Metropolis: [Qn]\n&gt;Metropolis: [delta]\n&gt;Metropolis: [P0]\n&gt;Metropolis: [D0]\n&gt;Metropolis: [sigma_live]\n&gt;Metropolis: [sigma_dead]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 10_000 draw iterations (4_000 + 40_000 draws total) took 416 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\n</pre> <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[13]: Copied! <pre>plot_trace(trace=trace,\n           model=model,\n           uni=[\"mu_max\", \"Ks\", \"Qn\", \"delta\"],\n           fontname='Arial',\n           fontsize=12,\n           num_prior_samples=2000,\n           var_names_map={'mu_max': 'Maximum Growth Rate \u03bc (/day)', 'Ks': 'Saturation Constant Ks (ml/cell)', 'delta': 'Death Rate \u03b4 (/day)', 'Qn': 'Nutrient Quota Qn (ml/cell)', 'P0': 'Initial Live Density (/ml)', 'D0': 'Initial Dead Density (/ml)','sigma_live': '\u03c3 for live cells', 'sigma_dead': '\u03c3 for dead cells'},\n           var_order=['mu_max', 'Ks', 'Qn', 'delta','P0','D0','sigma_live','sigma_dead'],\n           save_path='figures/vardi_general_chains_reparam.png')\n</pre> plot_trace(trace=trace,            model=model,            uni=[\"mu_max\", \"Ks\", \"Qn\", \"delta\"],            fontname='Arial',            fontsize=12,            num_prior_samples=2000,            var_names_map={'mu_max': 'Maximum Growth Rate \u03bc (/day)', 'Ks': 'Saturation Constant Ks (ml/cell)', 'delta': 'Death Rate \u03b4 (/day)', 'Qn': 'Nutrient Quota Qn (ml/cell)', 'P0': 'Initial Live Density (/ml)', 'D0': 'Initial Dead Density (/ml)','sigma_live': '\u03c3 for live cells', 'sigma_dead': '\u03c3 for dead cells'},            var_order=['mu_max', 'Ks', 'Qn', 'delta','P0','D0','sigma_live','sigma_dead'],            save_path='figures/vardi_general_chains_reparam.png') <pre>Sampling: [D0, Ks, P0, Qn, Y_dead, Y_total, delta, mu_max, sigma_dead, sigma_live]\n</pre> <pre>mu_max\nKs\nQn\ndelta\nP0\nD0\nsigma_live\nsigma_dead\n</pre> Out[13]: <pre>array([[&lt;Axes: title={'center': 'Maximum Growth Rate \u03bc (/day)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Maximum Growth Rate \u03bc (/day)'}&gt;],\n       [&lt;Axes: title={'center': 'Saturation Constant Ks (ml/cell)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Saturation Constant Ks (ml/cell)'}&gt;],\n       [&lt;Axes: title={'center': 'Nutrient Quota Qn (ml/cell)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Nutrient Quota Qn (ml/cell)'}&gt;],\n       [&lt;Axes: title={'center': 'Death Rate \u03b4 (/day)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Death Rate \u03b4 (/day)'}&gt;],\n       [&lt;Axes: title={'center': 'Initial Live Density (/ml)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Initial Live Density (/ml)'}&gt;],\n       [&lt;Axes: title={'center': 'Initial Dead Density (/ml)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Initial Dead Density (/ml)'}&gt;],\n       [&lt;Axes: title={'center': '\u03c3 for live cells'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '\u03c3 for live cells'}&gt;],\n       [&lt;Axes: title={'center': '\u03c3 for dead cells'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '\u03c3 for dead cells'}&gt;]], dtype=object)</pre> In\u00a0[12]: Copied! <pre>def ode_solution2data(solution):\n    live = solution[:, 1]\n    dead = solution[:, 2]\n    total = live + dead\n    return {\n        \"total\": total,\n        \"dead\": dead\n    }\n\ncase_3_posteriors(trace, \n                  ehux_total_time, \n                  ehux_death_time, \n                  ehux_total_density, \n                  ehux_dead_density, \n                  general_case, \n                  ode_solution2data)\n</pre> def ode_solution2data(solution):     live = solution[:, 1]     dead = solution[:, 2]     total = live + dead     return {         \"total\": total,         \"dead\": dead     }  case_3_posteriors(trace,                    ehux_total_time,                    ehux_death_time,                    ehux_total_density,                    ehux_dead_density,                    general_case,                    ode_solution2data) <p>Now we interpret the results. Remember the ODE system from before.</p> <p>$$ \\begin{aligned}   \\mu(N) &amp;= \\mu_{\\text{max}} \\frac{N}{K_s + N} \\\\ \\frac{dN}{dt} &amp;= - Q_n \\, \\mu(N) \\, P_{\\text{m}^3} \\\\ \\frac{dP}{dt} &amp;= \\mu(N) \\, P - \\delta \\, P \\\\ \\frac{dD}{dt} &amp;= \\delta \\, P \\\\ \\end{aligned}   $$</p> <p>Here we will look at the above plots in tandem. Take note of the x-axis values at the peaks of the PDF's and the variables they represent. Those are the optimal values for their variables for the system. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space. Now, see that not all the varibles are 'fit'. Ks doesn't have a peak, its flat. If you see this alone it may seem problematic. This is why looking at posterior draws over the data is important. We can see that our model fits the data very well, therefore the flat Ks estimate means the model is not dependent on that value. Which is fine! This is the importance of clear post processing.</p> In\u00a0[9]: Copied! <pre>with model:\n    trace = pm.sample(draws=1000, \n                      tune=1000, \n                      chains=4, \n                      target_accept=0.95,\n                      return_inferencedata=True, \n                      cores=4)\n    \nplot_trace(trace=trace,\n           model=model,\n           uni=[\"mu_max\", \"Ks\", \"Qn\", \"delta\"],\n           fontname='Arial',\n           fontsize=12,\n           num_prior_samples=2000,\n           var_names_map={'mu_max': 'Maximum Growth Rate \u03bc (/day)', 'delta': 'Death Rate \u03b4 (/day)', 'Qn': 'Nutrient Quota Qn (ml/cell)', 'P0': 'Initial Live Density (/ml)', 'D0': 'Initial Dead Density (/ml)','sigma_live': '\u03c3 for live cells', 'sigma_dead': '\u03c3 for dead cells'},\n           var_order=['mu_max', 'Ks', 'Qn', 'delta','P0','D0','sigma_live','sigma_dead'],\n           save_path='figures/vardi_general_chains_reparam.png')\n\ndef ode_solution2data(solution):\n    live = solution[:, 1]\n    dead = solution[:, 2]\n    total = live + dead\n    return {\n        \"total\": total,\n        \"dead\": dead\n    }\n\ncase_3_posteriors(trace, \n                  ehux_total_time, \n                  ehux_death_time, \n                  ehux_total_density, \n                  ehux_dead_density, \n                  general_case, \n                  ode_solution2data)\n</pre> with model:     trace = pm.sample(draws=1000,                        tune=1000,                        chains=4,                        target_accept=0.95,                       return_inferencedata=True,                        cores=4)      plot_trace(trace=trace,            model=model,            uni=[\"mu_max\", \"Ks\", \"Qn\", \"delta\"],            fontname='Arial',            fontsize=12,            num_prior_samples=2000,            var_names_map={'mu_max': 'Maximum Growth Rate \u03bc (/day)', 'delta': 'Death Rate \u03b4 (/day)', 'Qn': 'Nutrient Quota Qn (ml/cell)', 'P0': 'Initial Live Density (/ml)', 'D0': 'Initial Dead Density (/ml)','sigma_live': '\u03c3 for live cells', 'sigma_dead': '\u03c3 for dead cells'},            var_order=['mu_max', 'Ks', 'Qn', 'delta','P0','D0','sigma_live','sigma_dead'],            save_path='figures/vardi_general_chains_reparam.png')  def ode_solution2data(solution):     live = solution[:, 1]     dead = solution[:, 2]     total = live + dead     return {         \"total\": total,         \"dead\": dead     }  case_3_posteriors(trace,                    ehux_total_time,                    ehux_death_time,                    ehux_total_density,                    ehux_dead_density,                    general_case,                    ode_solution2data) <pre>Multiprocess sampling (4 chains in 4 jobs)\nCompoundStep\n&gt;CompoundStep\n&gt;&gt;Slice: [mu_max]\n&gt;&gt;Slice: [Ks]\n&gt;&gt;Slice: [Qn]\n&gt;&gt;Slice: [delta]\n&gt;&gt;Slice: [P0]\n&gt;&gt;Slice: [D0]\n&gt;NUTS: [sigma_live, sigma_dead]\n</pre> <pre>Output()</pre> <pre></pre> <pre>Sampling 4 chains for 1_000 tune and 1_000 draw iterations (4_000 + 4_000 draws total) took 165 seconds.\nThe rhat statistic is larger than 1.01 for some parameters. This indicates problems during sampling. See https://arxiv.org/abs/1903.08008 for details\nThe effective sample size per chain is smaller than 100 for some parameters.  A higher number is needed for reliable rhat and ess computation. See https://arxiv.org/abs/1903.08008 for details\nSampling: [D0, Ks, P0, Qn, Y_dead, Y_total, delta, mu_max, sigma_dead, sigma_live]\n</pre> <pre>mu_max\nKs\nQn\ndelta\nP0\nD0\nsigma_live\nsigma_dead\n</pre>"},{"location":"education/python/case_study_3/#case-study-3-monod-growth-model-with-explicit-resources","title":"Case Study 3: Monod Growth Model with Explicit Resources\u00b6","text":"<p>We are going to establish a Monod Growth model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! Pandas is the defacto python package for reading in data. It's also smart to handle any necessary data conversions during this step. Our data spans many magnitudes, so we will go ahead and perform our log conversion now. You may not need to and that's just fine! You may not need to and that's just fine! It all depends on what you are looking for.</p> <p>Note: The \"+ 1e-9\" is merely a fail-safe against divide-by-zero errors when doing a log transform. Adding a value infitesimally near 0 makes no impact on the data itself and prevents the aformentioned error. Good quick trick to know!</p>"},{"location":"education/python/case_study_3/#additional-nuts-output","title":"Additional NUTS Output\u00b6","text":"<p>Using NUTS is still possible and can result in appealing estimates. However, take note of the warning displayed by PyMC. True this is just a warning, but warnings are NEVER to be ignored. If there is a proper justification and knowledge behind your decision, you are obviously free to make it. The rest of the time, always take care of warnings.</p>"}]}