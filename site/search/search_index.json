{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83e\uddec MCMCwithODEs_primer","text":""},{"location":"#bayesian-learning-of-microbial-traits-from-population-time-series-data-a-primer","title":"Bayesian Learning of Microbial Traits from Population Time Series Data: A Primer","text":"<p>Mathematical models are increasingly used to infer traits, interactions, and functional dynamics of microbial systems. The inference process typically begins with a rate-based Ordinary Differential Equation (ODE) model.</p> <p>However, fitting such models to experimental data requires a principled statistical framework that can:</p> <p>Incorporate prior knowledge, Account for measurement noise, and Quantify uncertainty in parameter estimates.</p> <p>Our aim</p> <p>We strive to make the implicit, explicit \u2014 introducing Bayesian inference of ecological ODE models for microbial time series, with a unified workflow in Python (PyMC) and Julia (Turing.jl).</p>"},{"location":"#overview","title":"\ud83d\udcd8 Overview","text":"<p>This repository accompanies our upcoming paper:</p> <p>\u201cBayesian Learning of Microbial Traits from Population Time Series Data: A Primer\u201d Authors: TBD (Link will be posted here when the paper is online.)</p>"},{"location":"#quick-start","title":"\ud83d\ude80 Quick start","text":"<p>Explore the case studies from the sidebar: - Python (PyMC) \u2192 Case Study 1\u20133 - Julia (Turing.jl) \u2192 Case Study 1\u20132</p> <p>Or jump directly:</p> <ul> <li>Case Study 1 \u2014 Exponential Growth and Death</li> <li>Case Study 2 \u2014 Logistic Growth and Death</li> <li>Case Study 3 \u2014 Monod Growth and Death</li> </ul>"},{"location":"#stack","title":"\ud83e\uddf0 Stack","text":"<ul> <li>Python \u00b7 PyMC \u2014 Probabilistic programming in Python for Bayesian modeling and inference  </li> <li>Julia \u00b7 Turing.jl \u2014 A flexible probabilistic programming language in Julia  </li> </ul>"},{"location":"#contributors","title":"\ud83d\udc69\u200d\ud83d\udd2c Contributors","text":"<p>This primer was developed through a collaborative effort across multiple research groups, combining expertise in microbial ecology, statistical physics, and Bayesian inference.</p> <p>Raunak Dey \u2014 University of Maryland \ud83d\udd17 Website \u00b7 GitHub</p> <p>David Talmy \u2014 University of Tennessee, Knoxville \ud83d\udd17 Website </p> <p>Robert Beach \u2014 University of Tennessee, Knoxville  </p> <p>Kennedi Hambrick \u2014 University of Tennessee, Knoxville  </p> <p>Ioannis Sgouralis \u2014 University of Tennessee, Knoxville \ud83d\udd17 Website</p> <p>Stephen J. Beckett \u2014  University of Maryland </p> <p>Paul Fr\u00e9mont \u2014  University of Maryland </p> <p>David Demory \u2014 CNRS / Universit\u00e9 Paris-Saclay  </p> <p>Eric Carr \u2014 University of Tennessee, Knoxville  </p> <p>Joshua S. Weitz \u2014 University of Maryland  \ud83d\udd17 Website </p> <p>This project connects theory, data, and computation to advance reproducible Bayesian inference for ecological population models.</p>"},{"location":"contributors/","title":"Contributors","text":""},{"location":"contributors/#contributors","title":"\ud83d\udc69\u200d\ud83d\udd2c Contributors","text":"<p>This primer was developed through a collaborative effort across multiple research groups, combining expertise in microbial ecology, statistical physics, and Bayesian inference.</p> <p>Raunak Dey \u2014 University of Maryland \ud83d\udd17 Website \u00b7 GitHub</p> <p>David Talmy \u2014 University of Tennessee, Knoxville \ud83d\udd17 Website </p> <p>Robert Beach \u2014 University of Tennessee, Knoxville  </p> <p>Kennedi Hambrick \u2014 University of Tennessee, Knoxville  </p> <p>Ioannis Sgouralis \u2014 University of Tennessee, Knoxville \ud83d\udd17 Website</p> <p>Stephen J. Beckett \u2014  University of Maryland </p> <p>Paul Fr\u00e9mont \u2014  University of Maryland </p> <p>David Demory \u2014 CNRS / Universit\u00e9 Paris-Saclay  </p> <p>Eric Carr \u2014 University of Tennessee, Knoxville  </p> <p>Joshua S. Weitz \u2014 University of Maryland  \ud83d\udd17 Website </p> <p>This project connects theory, data, and computation to advance reproducible Bayesian inference for ecological population models.</p>"},{"location":"education/julia/case_study_1/","title":"Case Study 1 \u2014 Birth\u2013death dynamics","text":"In\u00a0[\u00a0]: Copied! <pre># For user-defined post processing and plotting functions\ninclude(joinpath(@__DIR__, \"..\", \"..\", \"utils\", \"plot_utils.jl\"))\n</pre> # For user-defined post processing and plotting functions include(joinpath(@__DIR__, \"..\", \"..\", \"utils\", \"plot_utils.jl\")) <pre>overlay_posterior_on_observed (generic function with 1 method)</pre> <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using Turing. A Julia software package allowing you to fit complex models with ease.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 1 ##\n\nusing CSV, DataFrames\n\ndf = CSV.read(\"../../case_study_1/python/data/phaeocystis_control.csv\", DataFrame)\n\ntimes    = df.times\ny_obs    = df.cells\n\nlog_y_obs = log.(y_obs .+ 1e-9)\n\nnothing\n</pre> ## Cell 1 ##  using CSV, DataFrames  df = CSV.read(\"../../case_study_1/python/data/phaeocystis_control.csv\", DataFrame)  times    = df.times y_obs    = df.cells  log_y_obs = log.(y_obs .+ 1e-9)  nothing <p>Here is the first thing that needs to be done. We will setup the logistic ODE that will adapt to the data. To do this, you will use variables that Turing gives you by 'default'. Let me explain. Notice the 4 parameters being passed in. du, u, p, and t. du will be the array your functions go in. u are you starting states. p are your parameters. t are your times. Turing and Julia will handle the passing of these under the hood. Just make sure you declare them correctly and remember the order of things. If you have states 1 and 2 then EVERYWHERE you make sure anytime those states are passed they are in that exact order. Same things with your parameters. Lots of things happen under the hood here for you, so things are not always 'explicitly' passed, so make sure when you do pass things your order is always preserved.</p> <p>mum: Growth Rate y: Initial Value</p> <p>$$ \\frac{dy}{dt} = \\mu y $$</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 2 ##\n\nfunction ode(du, u, p, t)\n    mum = p[1]\n    y = u[1]\n    du[1] = mum * y\n    return nothing\nend\n\nnothing\n</pre> ## Cell 2 ##  function ode(du, u, p, t)     mum = p[1]     y = u[1]     du[1] = mum * y     return nothing end  nothing <p>This is where we start to use Turing. We declare the function to be probabilistic model named fit_ode using the @model tag. The inputs are as follows.</p> <p>log_y_obs: Our log transformed observed data. times: The times at which the ODE is evaluated. prob: The prebuilt ODE.</p> <p>Next come where we assign out priors. Choosing priors is an interesting combination of art and science. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. For $\\mu$ we chose a truncatednormal prior. This says, we believe there is a normal distribution about some x, however, it cannot be above y or below z. For N0, we chose a lognormal distribution. This says we believe our value is around some x given a normalesque strictly positive distribution. For sigma, we chose a halfnormal distribution. This is usually used for your error or standard deviation. This is saying we know we can not have negative error, and we believe the error will be closer to 0 than farther away, however, we are not explicitly capping it off in the positive direction.</p> <p></p> <p>The next part are instructions on how to solve the ODE. The following is what gets passed into solve.</p> <p>prob: The prebuilt ODE. Tsit5(): An ODE solver. What solver you use is generally up to you. u0: State priors. p: Parameter Priors. saveat: all the times from the observed times.</p> <p>note: See how when you're passing in values to this julia function there are both , and ;? In julia this is to mark the difference between positional arguments (before ;) and keyword arguments (after ;).</p> <p>The final part of the model is getting the likelihood (or fit if you prefer) of the model to the observed data. It's a bit layered here so we'll break it down part by part.</p> <p>log.(Array(sol)[1, :] .+ 1e-9): sol is the solver output. We force it into an array using Array and grab the first row using the [1, :] indexing. That indexing will match whatever state you want. If you have multiple states then row 1, 2, 3... will correspond to the order that the states are passed into u0. Last we put the output on a log scale. Note that seeing a . connected to a mathematical operation makes it an element-wise array operation. The .+ 1e-9 is an array addition of a value inconsequentially near 0 so there are no log(0) errors. log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)): For simplicity this will be a little hand wavey. At this step you have the log scaled y_pred and the sigma distribution. Give those to arraydist(Normal.()) and it will tell the model how well it's guess was. In other words, this line is the model saying each observed log value is normal around the predicted log-value with standard deviation sigma. Ensure the variable preceding the ~ here is the same as the one passed into fit_ode.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 3 ##\n\nusing Turing\n@model function fit_ode(log_y_obs, times, prob)\n    mum   ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)\n    N0    ~ LogNormal(log(1_630_000), 0.1)\n    sigma ~ truncated(Normal(0, 1), 0, Inf)    \n    \n    # solve exactly at data times\n    sol = solve(prob, Tsit5();\n                u0 = [N0], p = mum,  \n                saveat = times)\n\n    # likelihood at observation times\n    log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)\n    log_y_obs ~ arraydist(Normal.(log_y_pred, sigma))\nend\n\nnothing\n</pre> ## Cell 3 ##  using Turing @model function fit_ode(log_y_obs, times, prob)     mum   ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)     N0    ~ LogNormal(log(1_630_000), 0.1)     sigma ~ truncated(Normal(0, 1), 0, Inf)              # solve exactly at data times     sol = solve(prob, Tsit5();                 u0 = [N0], p = mum,                   saveat = times)      # likelihood at observation times     log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)     log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)) end  nothing <p>This block is where we declare the ODE with some starter values, call the model, and run the sampling.</p> <p>To start,</p> <p>u0: Initial state value(s). Make the inital value(s) from the observed data. p: Initial parameter value(s). Make these a value in the specified range of the prior you declared. Here we chose the center. tspan:  first and last values from your observed times. prob: The prebuild ODE.</p> <p>Next is the call to run the sampling. Here are what the arguments mean.</p> <p>model: The model created above. NUTS(1000, .95): This is telling you to use the NUTS sampler. 1000 is the amount of 'warm up' iterations it will do under the hood. .95 is the acceptance rate of the 'improved' solutions. MCMCSerial(): Tells the solver to run the chains sequentially. 1000: number of posterior samples to be generated. 3: number of chains. progress: either hides or shows the progress bar. Up to you.</p> <p>Here we aren't going to run the sampling for time purposes. Instead we have the chain saved VIA Julia serialization and will call it to show the postprocessing.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 4 ##\n\nusing DifferentialEquations\n\nu0 = [y_obs[1]]\np = [0.5]\ntspan = (times[1], times[end])\nprob = ODEProblem(ode, u0, tspan, p)\n\nmodel    = fit_ode(log_y_obs, times, prob)\n# chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)\n\nusing Serialization\nchain = deserialize(\"chains/case_study_1_p1.jls\")::Chains  # load\n\nnothing\n</pre> ## Cell 4 ##  using DifferentialEquations  u0 = [y_obs[1]] p = [0.5] tspan = (times[1], times[end]) prob = ODEProblem(ode, u0, tspan, p)  model    = fit_ode(log_y_obs, times, prob) # chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)  using Serialization chain = deserialize(\"chains/case_study_1_p1.jls\")::Chains  # load  nothing <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[7]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :mum   =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),\n    :N0    =&gt; LogNormal(log(1_630_000.0), 0.1),\n    :sigma =&gt; truncated(Normal(0, 1.0), 0.0, Inf)\n)\n\norder = [:mum, :N0, :sigma]\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :mum   =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),     :N0    =&gt; LogNormal(log(1_630_000.0), 0.1),     :sigma =&gt; truncated(Normal(0, 1.0), 0.0, Inf) )  order = [:mum, :N0, :sigma] plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities <p>Now we interpret the results. Remember the ODE's from before.</p> <p>$$ \\frac{dy}{dt} = \\mu y $$</p> <p>Take note of x-axis values at the peaks of the PDFs and the variables they represent. The x-axis values at those peaks are what the model found to be the optimal value for the equation. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space well enough.</p> In\u00a0[8]: Copied! <pre>init_syms = [:N0]\nparam_syms = [:mum]\nt_obs = times\ny_obs = y_obs\n\nplt = overlay_posterior_on_observed(\n    chain, ode, t_obs, y_obs;\n    init_syms=init_syms,\n    param_syms=param_syms,\n    which_states=[1],     # choose states to plot\n    n_draws=150,            # how many posterior paths to overlay\n    plot_ribbon=true,       # median \u00b1 CI band\n    ribbon_q=(0.1, 0.9),    # CI limits\n    legend=:topleft,\n    logy=false\n)\ndisplay(plt)\n</pre> init_syms = [:N0] param_syms = [:mum] t_obs = times y_obs = y_obs  plt = overlay_posterior_on_observed(     chain, ode, t_obs, y_obs;     init_syms=init_syms,     param_syms=param_syms,     which_states=[1],     # choose states to plot     n_draws=150,            # how many posterior paths to overlay     plot_ribbon=true,       # median \u00b1 CI band     ribbon_q=(0.1, 0.9),    # CI limits     legend=:topleft,     logy=false ) display(plt) <p>This plot is showing how well the range of estimated chains fit the observed data.</p> <p>CONGRATULATIONS! you just ran you first MCMC!</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 5 ##\n\nusing CSV, DataFrames\n\ndf = CSV.read(\"../../case_study_1/python/data/phaeocystis_control.csv\", DataFrame)\n\ntimes    = df.times\ny_obs    = df.cells\n\nnothing\n</pre> ## Cell 5 ##  using CSV, DataFrames  df = CSV.read(\"../../case_study_1/python/data/phaeocystis_control.csv\", DataFrame)  times    = df.times y_obs    = df.cells  nothing <p>$$ \\frac{dy}{dt} = (\\mu - \\delta)\\, y $$</p> <p>Here we are simply subtracting a delta from mu so that delta can simulate death. In short, we don't always need death (or something else comperable) data. We can simply make our ODE expressive about the data!</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 6 ##\n\nfunction ode(du, u, p, t)\n    mum = p[1] \n    delta = p[2]\n    y = u[1]\n    du[1] = (mum - delta) * y\n    return nothing\nend\n\nnothing\n</pre> ## Cell 6 ##  function ode(du, u, p, t)     mum = p[1]      delta = p[2]     y = u[1]     du[1] = (mum - delta) * y     return nothing end  nothing <p>Here is the model. Everything here should make sense. If something is confusing see the explination above cell 4. Still note that we have two likelihoods at the bottom since we are estimating two states.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 7 ##\n\nusing Turing\n@model function fit_ode(log_y_obs, times, prob)\n    mum   ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)\n    delta ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)\n    N0    ~ LogNormal(log(1_630_000), 0.1)\n    sigma ~ truncated(Normal(0, 1), 0, Inf)    \n    \n    # solve exactly at data times\n    sol = solve(prob, Tsit5();\n                u0 = [N0], p = [mum, delta],  \n                saveat = times)\n\n    # likelihood at observation times\n    log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)\n    log_y_obs ~ arraydist(Normal.(log_y_pred, sigma))\nend\n\nnothing\n</pre> ## Cell 7 ##  using Turing @model function fit_ode(log_y_obs, times, prob)     mum   ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)     delta ~ truncated(Normal(0.5, 0.3), 0.0, 1.0)     N0    ~ LogNormal(log(1_630_000), 0.1)     sigma ~ truncated(Normal(0, 1), 0, Inf)              # solve exactly at data times     sol = solve(prob, Tsit5();                 u0 = [N0], p = [mum, delta],                   saveat = times)      # likelihood at observation times     log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)     log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)) end  nothing <p>Everything here is the same as before! See explination above cell 5 if you forgot something.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 8 ##\n\nusing DifferentialEquations\n\nu0 = [y_obs[1]]\np = [0.5, 0.5]\ntspan = (times[1], times[end])\nprob = ODEProblem(ode, u0, tspan, p)\n\nlog_y_obs = log.(y_obs .+ 1e-9)\nmodel    = fit_ode(log_y_obs, times, prob)\n# chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)\n\nusing Serialization\nchain = deserialize(\"chains/case_study_1_p2.jls\")::Chains \n\nnothing\n</pre> ## Cell 8 ##  using DifferentialEquations  u0 = [y_obs[1]] p = [0.5, 0.5] tspan = (times[1], times[end]) prob = ODEProblem(ode, u0, tspan, p)  log_y_obs = log.(y_obs .+ 1e-9) model    = fit_ode(log_y_obs, times, prob) # chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)  using Serialization chain = deserialize(\"chains/case_study_1_p2.jls\")::Chains   nothing In\u00a0[\u00a0]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :mum   =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),\n    :delta =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),\n    :N0    =&gt; LogNormal(log(1_630_000.0), 0.1),\n    :sigma =&gt; truncated(Normal(0, 1.0), 0.0, Inf)\n)\n\norder = [:mum, :delta, :N0, :sigma]\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :mum   =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),     :delta =&gt; truncated(Normal(0.5, 0.3), 0.0, 1.0),     :N0    =&gt; LogNormal(log(1_630_000.0), 0.1),     :sigma =&gt; truncated(Normal(0, 1.0), 0.0, Inf) )  order = [:mum, :delta, :N0, :sigma] plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities In\u00a0[15]: Copied! <pre>init_syms = [:N0]\nparam_syms = [:mum, :delta]\ny_obs = y_obs\nt_obs = times\n\nplt = overlay_posterior_on_observed(\n    chain, ode, t_obs, y_obs;\n    init_syms=init_syms,\n    param_syms=param_syms,\n    which_states=[1],     # choose states to plot\n    n_draws=150,            # how many posterior paths to overlay\n    plot_ribbon=true,       # median \u00b1 CI band\n    ribbon_q=(0.1, 0.9),    # CI limits\n    legend=:topleft,\n    logy=false\n)\ndisplay(plt)\n</pre> init_syms = [:N0] param_syms = [:mum, :delta] y_obs = y_obs t_obs = times  plt = overlay_posterior_on_observed(     chain, ode, t_obs, y_obs;     init_syms=init_syms,     param_syms=param_syms,     which_states=[1],     # choose states to plot     n_draws=150,            # how many posterior paths to overlay     plot_ribbon=true,       # median \u00b1 CI band     ribbon_q=(0.1, 0.9),    # CI limits     legend=:topleft,     logy=false ) display(plt) In\u00a0[47]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"education/julia/case_study_1/#case-study-1-exponential-growth","title":"Case Study 1: Exponential Growth\u00b6","text":"<p>We are going to establish an exponential model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! We will read our data from a CSV into a DataFrame. It's also smart to handle any necessary data conversions during this step. You may not need to and that's just fine! It all depends on what you are looking for.</p>"},{"location":"education/julia/case_study_1/#case-study-1-exponential-growth-and-death","title":"Case Study 1: Exponential Growth and Death\u00b6","text":"<p>Now on to a more complex model. We will be using the same data as before.</p>"},{"location":"education/julia/case_study_1/#the-end","title":"The End\u00b6","text":""},{"location":"education/julia/case_study_2/","title":"Case Study 2 \u2014 Priors, noise models, diagnostics","text":"In\u00a0[\u00a0]: Copied! <pre># For user-defined post processing and plotting functions\ninclude(joinpath(@__DIR__, \"..\", \"..\", \"utils\", \"plot_utils.jl\"))\n</pre> # For user-defined post processing and plotting functions include(joinpath(@__DIR__, \"..\", \"..\", \"utils\", \"plot_utils.jl\")) <pre>overlay_posterior_on_observed (generic function with 1 method)</pre> <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using Turing. A Julia software package allowing you to fit complex models with ease.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 1 ## \n\nusing CSV, DataFrames\n\n# from your data-prep cell\ndf = CSV.read(\"../../case_study_2/python/data/total_cells.csv\", DataFrame)\n\ntimes    = df[end-14:end, :1]\ny_obs    = df[end-14:end, :2] * 1e6\n\nnothing\n</pre> ## Cell 1 ##   using CSV, DataFrames  # from your data-prep cell df = CSV.read(\"../../case_study_2/python/data/total_cells.csv\", DataFrame)  times    = df[end-14:end, :1] y_obs    = df[end-14:end, :2] * 1e6  nothing <p>Here is the first thing that needs to be done. We will setup the logistic ODE that will adapt to the data. To do this, you will use variables that Turing gives you by 'default'. Let me explain. Notice the 4 parameters being passed in. du, u, p, and t. du will be the array your functions go in. u are you starting states. p are your parameters. t are your times. Turing and Julia will handle the passing of these under the hood. Just make sure you declare them correctly and remember the order of things. If you have states 1 and 2 then EVERYWHERE you make sure anytime those states are passed they are in that exact order. Same things with your parameters. Lots of things happen under the hood here for you, so things are not always 'explicitly' passed, so make sure when you do pass things your order is always preserved.</p> <p>P: Initial Value r: Growith Rate K: Carrying Capacity</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P $$</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 2 ##\n\nfunction ode(du, u, p, t)\n    P = u[1]\n\n    r = p[1]\n    K = p[2]\n\n    du[1] = r * (1 - P / K) * P\n\n    return nothing\nend\n\nnothing\n</pre> ## Cell 2 ##  function ode(du, u, p, t)     P = u[1]      r = p[1]     K = p[2]      du[1] = r * (1 - P / K) * P      return nothing end  nothing <p>This is where we start to use Turing. We declare the function to be probabilistic model named fit_ode using the @model tag. The inputs are as follows.</p> <p>log_y_obs: Our log transformed observed data. times: The times at which the ODE is evaluated. prob: The prebuilt ODE.</p> <p>Next is where we assign our priors. Choosing priors is an interesting combination of art and science. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. With these uniform priors we are essentially saying. We believe the proper value for the model will be equally likely within this range and WILL NOT be outside this range. Your half normal distribution is a PDF that is only posotive and extends to infinity. This is usually used for your error or standard deviation. This is saying we know we can not have negative error, and we believe the error will be closer to 0 than farther away, however, we are not explicitly capping it off in the positive direction. Below are the priors visualized.</p> <p></p> <p>The next part are instructions on how to solve the ODE. The following is what gets passed into solve.</p> <p>prob: The prebuilt ODE. Tsit5(): An ODE solver. What solver you use is generally up to you. u0: State priors. p: Parameter Priors. saveat: all the times from the observed times.</p> <p>note: See how when you're passing in values to this julia function there are both , and ;? In julia this is to mark the difference between positional arguments (before ;) and keyword arguments (after ;).</p> <p>The final part of the model is getting the likelihood (or fit if you prefer) of the model to the observed data. It's a bit layered here so we'll break it down part by part.</p> <p>log.(Array(sol)[1, :] .+ 1e-9): sol is the solver output. We force it into an array using Array and grab the first row using the [1, :] indexing. That indexing will match whatever state you want. If you have multiple states then row 1, 2, 3... will correspond to the order that the states are passed into u0. Last we put the output on a log scale. Note that seeing a . connected to a mathematical operation makes it an element-wise array operation. The .+ 1e-9 is an array addition of a value inconsequentially near 0 so there are no log(0) errors. log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)): For simplicity this will be a little hand wavey. At this step you have the log scaled y_pred and the sigma distribution. Give those to arraydist(Normal.()) and it will tell the model how well it's guess was. In other words, this line is the model saying each observed log value is normal around the predicted log-value with standard deviation sigma. Ensure the variable preceding the ~ here is the same as the one passed into fit_ode.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 3 ##\n\nusing Turing\n@model function fit_ode(log_y_obs, times, prob)\n    r ~ Uniform(0.5, 1.0)\n    K ~ Uniform(1e6, 4e7)\n\n    P0 ~ Uniform(1e5, 3e5)\n    \n    sigma ~ truncated(Normal(0, 3), 0, Inf)\n\n    sol = solve(prob, Tsit5();\n                u0 = [P0], p = [r, K], \n                saveat = times)\n\n    log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)\n    log_y_obs ~ arraydist(Normal.(log_y_pred, sigma))\nend\n\nnothing\n</pre> ## Cell 3 ##  using Turing @model function fit_ode(log_y_obs, times, prob)     r ~ Uniform(0.5, 1.0)     K ~ Uniform(1e6, 4e7)      P0 ~ Uniform(1e5, 3e5)          sigma ~ truncated(Normal(0, 3), 0, Inf)      sol = solve(prob, Tsit5();                 u0 = [P0], p = [r, K],                  saveat = times)      log_y_pred = log.(Array(sol)[1, :] .+ 1e-9)     log_y_obs ~ arraydist(Normal.(log_y_pred, sigma)) end  nothing <p>This block is where we declare the ODE with some starter values, call the model, and run the sampling.</p> <p>To start,</p> <p>u0: Initial state value(s). Make the inital value(s) from the observed data. p: Initial parameter value(s). Make these a value in the specified range of the prior you declared. Here we chose the low end of the uniform distribution. tspan:  first and last values from your observed times. prob: The prebuild ODE.</p> <p>Next is the call to run the sampling. Here are what the arguments mean.</p> <p>model: The model created above. NUTS(1000, .95): This is telling you to use the NUTS sampler. 1000 is the amount of 'warm up' iterations it will do under the hood. .95 is the acceptance rate of the 'improved' solutions. MCMCSerial(): Tells the solver to run the chains sequentially. 1000: number of posterior samples to be generated. 3: number of chains. progress: either hides or shows the progress bar. Up to you.</p> <p>Here we aren't going to run the sampling for time purposes. Instead we have the chain saved VIA Julia serialization and will call it to show the postprocessing.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 4 ##\n\nusing DifferentialEquations\n\nu0 = [y_obs[1]]\np = [0.5, 1e6]\ntspan = (times[1], times[end])\nprob = ODEProblem(ode, u0, tspan, p)\n\nlog_y_obs = log.(y_obs .+ 1e-9)\nmodel    = fit_ode(log_y_obs, times, prob)\n# chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)\n\nusing Serialization\nchain = deserialize(\"chains/case_study_2_p1.jls\")::Chains \n\nnothing\n</pre> ## Cell 4 ##  using DifferentialEquations  u0 = [y_obs[1]] p = [0.5, 1e6] tspan = (times[1], times[end]) prob = ODEProblem(ode, u0, tspan, p)  log_y_obs = log.(y_obs .+ 1e-9) model    = fit_ode(log_y_obs, times, prob) # chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)  using Serialization chain = deserialize(\"chains/case_study_2_p1.jls\")::Chains   nothing <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[\u00a0]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :r     =&gt; Uniform(0.5, 1.0),\n    :K     =&gt; Uniform(1e6, 4e7),\n    :P0    =&gt; Uniform(1e5, 3e5),\n    :sigma =&gt; truncated(Normal(0, 3), 0, Inf),\n)\n\norder = [:r, :K, :P0, :sigma]\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :r     =&gt; Uniform(0.5, 1.0),     :K     =&gt; Uniform(1e6, 4e7),     :P0    =&gt; Uniform(1e5, 3e5),     :sigma =&gt; truncated(Normal(0, 3), 0, Inf), )  order = [:r, :K, :P0, :sigma] plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities <p>Now we interpret the results. Remember the ODE's from before.</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P $$</p> <p>Take note of x-axis values at the peaks of the PDFs and the variables they represent. The x-axis values at those peaks are what the model found to be the optimal value for the equation. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space well enough.</p> In\u00a0[\u00a0]: Copied! <pre>init_syms = [:P0]\nparam_syms = [:r, :K]\nt_obs = times\ny_obs = y_obs\n\nplt = overlay_posterior_on_observed(\n    chain, ode, t_obs, y_obs;\n    init_syms=init_syms,\n    param_syms=param_syms,\n    which_states=[1],     # choose states to plot\n    n_draws=150,            # how many posterior paths to overlay\n    plot_ribbon=true,       # median \u00b1 CI band\n    legend=:topleft,\n    ribbon_q=(0.1, 0.9),    # CI limits\n    logy=false\n)\ndisplay(plt)\n</pre> init_syms = [:P0] param_syms = [:r, :K] t_obs = times y_obs = y_obs  plt = overlay_posterior_on_observed(     chain, ode, t_obs, y_obs;     init_syms=init_syms,     param_syms=param_syms,     which_states=[1],     # choose states to plot     n_draws=150,            # how many posterior paths to overlay     plot_ribbon=true,       # median \u00b1 CI band     legend=:topleft,     ribbon_q=(0.1, 0.9),    # CI limits     logy=false ) display(plt) <p>This plot is showing how well the range of estimated chains fit the observed data.</p> <p>CONGRATULATIONS! you just ran you first MCMC!</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 5 ##\n\nusing CSV, DataFrames\n\ncells = CSV.read(\"../../case_study_2/python/data/total_cells.csv\", DataFrame)\ndeath = CSV.read(\"../../case_study_2/python/data/death_percentage.csv\", DataFrame)\n\ncells_times  = cells[end-14:end, :1]\ncells_obs    = cells[end-14:end, :2] * 1e6\n\ndeath_times = death[1:15, :1]\ndeath_obs   = death[1:15, :2] .* (cells_obs ./ 100)\n\nnothing\n</pre> ## Cell 5 ##  using CSV, DataFrames  cells = CSV.read(\"../../case_study_2/python/data/total_cells.csv\", DataFrame) death = CSV.read(\"../../case_study_2/python/data/death_percentage.csv\", DataFrame)  cells_times  = cells[end-14:end, :1] cells_obs    = cells[end-14:end, :2] * 1e6  death_times = death[1:15, :1] death_obs   = death[1:15, :2] .* (cells_obs ./ 100)  nothing <p>ODE for living cells accounting for cell death</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P - \\delta P $$</p> <p>ODE for dead cells</p> <p>$$ \\frac{dD}{dt} = \\delta P $$</p> <p>We follow the same rules as before when creating this ODE. It's very easy to create more complex models with multiple equations. Remember your odering! Equations should be in the same order as the states they correspond to. BE CAREFUL WITH THE ORDER OF EVERYTHING. IT ALL MATTERS.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 6 ##\n\nfunction ode(du, u, p, t)\n    P = u[1]\n    D = u[2]\n\n    r = p[1]\n    K = p[2]\n    delta = p[3]\n\n    du[1] = r * (1 - P / K) * P - delta * P\n    du[2] = delta * P\n\n    return nothing\nend\n\nnothing\n</pre> ## Cell 6 ##  function ode(du, u, p, t)     P = u[1]     D = u[2]      r = p[1]     K = p[2]     delta = p[3]      du[1] = r * (1 - P / K) * P - delta * P     du[2] = delta * P      return nothing end  nothing <p>Here is the model. Everything here should make sense. If something is confusing see the explination above cell 4. Still note that we have two likelihoods at the bottom since we are estimating two states.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 7 ##\n\nusing Turing\n\n@model function fit_ode(logP_obs, logD_obs, times, prob)\n    r ~ Uniform(0.5, 1.0)\n    K ~ Uniform(1e6, 4e7)\n    delta ~ Uniform(0.0, 0.15)\n\n    P0 ~ Uniform(1e5, 3e5)\n    D0 ~ Uniform(1e4, 7e4)\n\n    sigma_live ~ truncated(Normal(0, 3), 0, Inf)\n    sigma_dead ~ truncated(Normal(0, 3), 0, Inf)\n    \n    # solve exactly at data times\n    sol = solve(prob, Tsit5();\n                u0 = [P0, D0], p = [r, K, delta], \n                saveat = times)\n    S = Array(sol) \n\n    logP_pred = log.((S[1, :] + S[2, :]) .+ 1e-9) # total!! since that's what's done in PyMC\n    logD_pred = log.(S[2, :] .+ 1e-9)\n\n    logP_obs ~ arraydist(Normal.(logP_pred, sigma_live))\n    logD_obs ~ arraydist(Normal.(logD_pred, sigma_dead))\n\nend\n\nnothing\n</pre> ## Cell 7 ##  using Turing  @model function fit_ode(logP_obs, logD_obs, times, prob)     r ~ Uniform(0.5, 1.0)     K ~ Uniform(1e6, 4e7)     delta ~ Uniform(0.0, 0.15)      P0 ~ Uniform(1e5, 3e5)     D0 ~ Uniform(1e4, 7e4)      sigma_live ~ truncated(Normal(0, 3), 0, Inf)     sigma_dead ~ truncated(Normal(0, 3), 0, Inf)          # solve exactly at data times     sol = solve(prob, Tsit5();                 u0 = [P0, D0], p = [r, K, delta],                  saveat = times)     S = Array(sol)       logP_pred = log.((S[1, :] + S[2, :]) .+ 1e-9) # total!! since that's what's done in PyMC     logD_pred = log.(S[2, :] .+ 1e-9)      logP_obs ~ arraydist(Normal.(logP_pred, sigma_live))     logD_obs ~ arraydist(Normal.(logD_pred, sigma_dead))  end  nothing <p>Everything here is that same as before! See the explination above cell 5 if you forgot something.</p> In\u00a0[\u00a0]: Copied! <pre>## Cell 8 ##\n\nusing DifferentialEquations\n\nu0 = [cells_obs[1], death_obs[1]]\np = [0.5, 1e6, 0.0] \ntspan = (cells_times[1], cells_times[end])\nprob = ODEProblem(ode, u0, tspan, p)\n\nlog_cells_obs = log.(cells_obs .+ 1e-9)\nlog_death_obs = log.(death_obs .+ 1e-9)\nmodel    = fit_ode(log_cells_obs, log_death_obs, cells_times, prob)\n# chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)\n\nusing Serialization\nchain = deserialize(\"chains/case_study_2_p2.jls\")::Chains \n\nnothing\n</pre> ## Cell 8 ##  using DifferentialEquations  u0 = [cells_obs[1], death_obs[1]] p = [0.5, 1e6, 0.0]  tspan = (cells_times[1], cells_times[end]) prob = ODEProblem(ode, u0, tspan, p)  log_cells_obs = log.(cells_obs .+ 1e-9) log_death_obs = log.(death_obs .+ 1e-9) model    = fit_ode(log_cells_obs, log_death_obs, cells_times, prob) # chain    = sample(model, NUTS(1000, .95), MCMCSerial(), 1000, 3; progress=false)  using Serialization chain = deserialize(\"chains/case_study_2_p2.jls\")::Chains   nothing   In\u00a0[\u00a0]: Copied! <pre>priors = Dict{Symbol,Distribution}(\n    :r     =&gt; Uniform(0.5, 1.0),\n    :K     =&gt; Uniform(1e6, 4e7),\n    :delta =&gt; Uniform(0.0, 0.15),\n    :P0    =&gt; Uniform(1e5, 3e5),\n    :D0    =&gt; Uniform(1e4, 7e4),\n    :sigma_live =&gt; truncated(Normal(0, 3), 0, Inf),\n    :sigma_dead =&gt; truncated(Normal(0, 3), 0, Inf)\n)\n\norder = [:r, :K, :delta, :P0, :D0, :sigma_live, :sigma_dead]\n\nplot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities\n</pre> priors = Dict{Symbol,Distribution}(     :r     =&gt; Uniform(0.5, 1.0),     :K     =&gt; Uniform(1e6, 4e7),     :delta =&gt; Uniform(0.0, 0.15),     :P0    =&gt; Uniform(1e5, 3e5),     :D0    =&gt; Uniform(1e4, 7e4),     :sigma_live =&gt; truncated(Normal(0, 3), 0, Inf),     :sigma_dead =&gt; truncated(Normal(0, 3), 0, Inf) )  order = [:r, :K, :delta, :P0, :D0, :sigma_live, :sigma_dead]  plot_trace_with_priors(chain; priors=priors, var_order=order, per_chain_density=true)  # also per-chain densities In\u00a0[\u00a0]: Copied! <pre>init_syms = [:P0, :D0]\nparam_syms = [:r, :K, :delta]\nt_obs = cells_times\ny_obs = hcat(cells_obs, death_obs)\n\nplt = overlay_posterior_on_observed(\n    chain, ode, t_obs, y_obs;\n    init_syms=init_syms,\n    param_syms=param_syms,\n    which_states=[1, 2],     # choose states to plot\n    pred_transforms=[u -&gt; u[1] + u[2], u -&gt; u[2]], # column1=total(P+D), column2=dead(D)\n    legend=:topleft,\n    n_draws=150,            # how many posterior paths to overlay\n    plot_ribbon=true,       # median \u00b1 CI band\n    ribbon_q=(0.1, 0.9),    # CI limits\n    logy=false\n)\ndisplay(plt)\n</pre> init_syms = [:P0, :D0] param_syms = [:r, :K, :delta] t_obs = cells_times y_obs = hcat(cells_obs, death_obs)  plt = overlay_posterior_on_observed(     chain, ode, t_obs, y_obs;     init_syms=init_syms,     param_syms=param_syms,     which_states=[1, 2],     # choose states to plot     pred_transforms=[u -&gt; u[1] + u[2], u -&gt; u[2]], # column1=total(P+D), column2=dead(D)     legend=:topleft,     n_draws=150,            # how many posterior paths to overlay     plot_ribbon=true,       # median \u00b1 CI band     ribbon_q=(0.1, 0.9),    # CI limits     logy=false ) display(plt) In\u00a0[47]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"education/julia/case_study_2/#case-study-2-logistic-growth","title":"Case Study 2: Logistic Growth\u00b6","text":"<p>We are going to establish a logistic model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! You want to read your data into a DataFrame. Here we are reading from a CSV but it can and will change based on the data you have. It's also smart to handle any necessary data conversions during this step. You may not need to and that's just fine! It all depends on what you are looking for.</p>"},{"location":"education/julia/case_study_2/#case-study-2-logistic-growth-and-death","title":"Case Study 2: Logistic Growth and Death\u00b6","text":"<p>Now on to a more complex model. We will be using the same data as before as well as some data on dead cells. We will also being using two ODEs!</p>"},{"location":"education/julia/case_study_2/#the-end","title":"The End\u00b6","text":""},{"location":"education/python/case_study_1/","title":"Case Study 1 \u2014 Birth\u2013death dynamics","text":"In\u00a0[1]: Copied! <pre># For user-defined post processing and plotting functions\nimport sys\nsys.path.insert(0, \"../..\")\nfrom utils.plot_utils_v2 import *\n</pre> # For user-defined post processing and plotting functions import sys sys.path.insert(0, \"../..\") from utils.plot_utils_v2 import * <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using PYMC. A python software package allowing you to fit complex models with ease.</p> In\u00a0[2]: Copied! <pre>## Cell 1 ##\n\nimport pandas as pd\ndata = pd.read_csv(\"../../case_study_1/python/data/phaeocystis_control.csv\")\ntime = data['times'].values\nobs = data['cells'].values\n</pre> ## Cell 1 ##  import pandas as pd data = pd.read_csv(\"../../case_study_1/python/data/phaeocystis_control.csv\") time = data['times'].values obs = data['cells'].values <p>Here is the first thing that needs to be done. We will setup the logistic ODE that will adapt to the data. To do this, you will use variables that PyMC gives you by 'default'. Let me explain. Note that in Cell 2 there are 3 parameters being passed in (y, t, params). You will also not see these variables being explicitly passed in this way anywhere. Here is what happens. When you create the ODE function in Cell 2, it gets passed to the the 'cell_model' in Cell 3. Now look to where the 'cell_model' is called in cell 4. There it passes priors into Y0 and theta. The connection is Y0 goes to y, and theta goes to params. The order you hand over the priors matters so call them in the correct order when creating the ODE equation. Be sure to keep this in mind and take note of the position of you priors when you pass them, and the indicies used in y and params to setup your variables and inital conditions!</p> <p>mum: Growth Rate y: Initial Value</p> <p>$$ \\frac{dy}{dt} = \\mu y $$</p> In\u00a0[3]: Copied! <pre>## Cell 2 ##\n\ndef ode(y, t, params):\n    mum = params[0]\n    return [mum * y[0]]\n</pre> ## Cell 2 ##  def ode(y, t, params):     mum = params[0]     return [mum * y[0]] <p>Here is the model we spoke about earlier. Here is what goes into each variable.</p> <p>func: the ODE function you just created times: the time steps from your data n_states: possible states count n_theta: Unique parameter count t0: inital time step (should almost always be 0)</p> In\u00a0[4]: Copied! <pre>## Cell 3 ##\n\nimport pymc as pm\ncell_model = pm.ode.DifferentialEquation(\n    func=ode,\n    times=data['times'].values,\n    n_states=1,\n    n_theta=1,\n    t0=0\n)\n</pre> ## Cell 3 ##  import pymc as pm cell_model = pm.ode.DifferentialEquation(     func=ode,     times=data['times'].values,     n_states=1,     n_theta=1,     t0=0 ) <pre>c:\\Users\\Whisk\\anaconda3\\envs\\MCMC_Modeling\\Lib\\site-packages\\pytensor\\link\\c\\cmodule.py:2968: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.\nThis usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.\nAlternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == 'NUMBA'` or passing `mode='NUMBA'` when compiling a PyTensor function.\nFor more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library\n  warnings.warn(\n</pre> <p>Here is where we assign out priors. Choosing priors is an interesting combination of art and science. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. For $\\mu$ we chose a truncatednormal prior. This says, we believe there is a normal distribution about some x, however, it cannot be above y or below z. For N0, we chose a lognormal distribution. This says we believe our value is around some x given a normalesque strictly positive distribution. For sigma, we chose a halfnormal distribution. This is usually used for your error or standard deviation. This is saying we know we can not have negative error, and we believe the error will be closer to 0 than farther away, however, we are not explicitly capping it off in the positive direction.</p> <p></p> <p>Next is getting the current ODE results. We spoke about how to pass the priors in previously. Be careful during this step!</p> <p>The final function is the Likelihood of the model. This is where we see how well the model results estimatied the data.</p> <p>We name the random variable 'y_obs'.</p> <p>For $\\mu$, this is a bit complicated so we'll break this down.</p> <ul> <li>y_hat[:,0] pulls the predicited trajectory for live cells from the ODE solution, note it's position in y_hat matches it's priors position when passed to cell_model.</li> <li>pm.math.log() puts predictions on a log scale which matches the log transformed observed data.</li> </ul> <p>Sigma: The prior you just created for it! Simple. Observered: The true cell data you read in earlier.</p> In\u00a0[5]: Copied! <pre>## Cell 4 ##\n\nimport numpy as np\nwith pm.Model() as model:\n    mum = pm.TruncatedNormal('mum', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)\n    N0 = pm.Lognormal('N0', mu=np.log(obs[0]), sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", 1)\n\n    y_hat = cell_model(y0=[N0], theta=[mum])\n    pm.Normal(\"Y_obs\", \n              mu=pm.math.log(y_hat[:, 0]), \n              sigma=sigma, \n              observed=np.log(obs))\n</pre> ## Cell 4 ##  import numpy as np with pm.Model() as model:     mum = pm.TruncatedNormal('mum', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)     N0 = pm.Lognormal('N0', mu=np.log(obs[0]), sigma=0.1)     sigma = pm.HalfNormal(\"sigma\", 1)      y_hat = cell_model(y0=[N0], theta=[mum])     pm.Normal(\"Y_obs\",                mu=pm.math.log(y_hat[:, 0]),                sigma=sigma,                observed=np.log(obs)) <p>When you are finally ready to run the model you will do so in the form shown between the '''. Here are what the variables being passed into pm.sample mean.</p> <p>draws: poterior values to be generated. tune: Some under the hood PyMC magic that 'warms it up'. Keep this between 500 and 2000 depending on how complex your model is. chains / cores: Amount of chains you want to compute in parallel. Keep these values the same for most basic usage. return_inferencedata: returns an ARVIZ inferencedata object which we'll use for easy reading of our output. target_accept: the % of times you accept the improved likelihood at each step of the chain. This is your exploration vs exploitation balance.</p> <p>To save time, we are just going to read in the pre-computed trace and print so you can see the output. We will use ARVIZ and Matplotlib to do so.</p> In\u00a0[6]: Copied! <pre>## CELL 5 ##\n\n'''\nwith model:\n    trace = pm.sample(draws=1000, tune=1000, target_accept=0.95, chains=4, return_inferencedata=True)\n'''\nimport arviz as az\nimport matplotlib.pyplot as plt\n\ntrace = az.from_netcdf(\"chains/case_study_1_p1_trace.nc\")\n</pre> ## CELL 5 ##  ''' with model:     trace = pm.sample(draws=1000, tune=1000, target_accept=0.95, chains=4, return_inferencedata=True) ''' import arviz as az import matplotlib.pyplot as plt  trace = az.from_netcdf(\"chains/case_study_1_p1_trace.nc\") <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[7]: Copied! <pre>plot_trace(\ntrace=trace,\nmodel=model,\nvar_names_map={'N0':'Initial density/ml','mum': 'Growth Rate \u03bc', 'sigma': 'Std Dev \u03c3'},\nvar_order=['mum','N0','sigma'],\nfontname='Arial',\nfontsize=12,\nnum_prior_samples=2000,\nsave_path='figures/normal_growth_chains.png'\n)\n</pre> plot_trace( trace=trace, model=model, var_names_map={'N0':'Initial density/ml','mum': 'Growth Rate \u03bc', 'sigma': 'Std Dev \u03c3'}, var_order=['mum','N0','sigma'], fontname='Arial', fontsize=12, num_prior_samples=2000, save_path='figures/normal_growth_chains.png' )  <pre>Sampling: [N0, Y_obs, mum, sigma]\n</pre> Out[7]: <pre>array([[&lt;Axes: title={'center': 'Growth Rate \u03bc'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Growth Rate \u03bc'}&gt;],\n       [&lt;Axes: title={'center': 'Initial density/ml'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Initial density/ml'}&gt;],\n       [&lt;Axes: title={'center': 'Std Dev \u03c3'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Std Dev \u03c3'}&gt;]], dtype=object)</pre> <p>Now we interpret the results. Remember the ODE's from before.</p> <p>$$ \\frac{dy}{dt} = \\mu y $$</p> <p>Take note of x-axis values at the peaks of the PDFs and the variables they represent. The x-axis values at those peaks are what the model found to be the optimal value for the equation. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space well enough.</p> In\u00a0[8]: Copied! <pre>dataset_postprocessing = {\n\"Cells\": [\n        {\"time\": time, \"values\": obs},  # replicate 1\n    ]\n}\n\ndef ode_solution2data(solution):\n    total = solution[:, 0]  # Assuming the first column is the total cell count\n    return {\n        \"total\": total\n    }\n\nposterior_dynamics(\ndataset=dataset_postprocessing,\ntrace=trace,\nmodel=model,\nn_plots=100,\nburn_in=50,\nnum_variables=1,\node_fn=ode,\node2data_fn=ode_solution2data,\nsave_path=\"figures/vardi_logistic_growth_dynamics.png\",\n# the key of this dict is the variable name in the dataset_postprocessing\nvar_properties={\n    \"Cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}\n},\nsuptitle=\"Posterior Predictive Dynamics\",\ncolor_lines='green'\n)\n</pre> dataset_postprocessing = { \"Cells\": [         {\"time\": time, \"values\": obs},  # replicate 1     ] }  def ode_solution2data(solution):     total = solution[:, 0]  # Assuming the first column is the total cell count     return {         \"total\": total     }  posterior_dynamics( dataset=dataset_postprocessing, trace=trace, model=model, n_plots=100, burn_in=50, num_variables=1, ode_fn=ode, ode2data_fn=ode_solution2data, save_path=\"figures/vardi_logistic_growth_dynamics.png\", # the key of this dict is the variable name in the dataset_postprocessing var_properties={     \"Cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True} }, suptitle=\"Posterior Predictive Dynamics\", color_lines='green' ) Out[8]: <pre>(&lt;Figure size 500x500 with 1 Axes&gt;,\n [&lt;Axes: title={'center': 'Total'}, xlabel='Time (days)', ylabel='Total cell density (/ml)'&gt;])</pre> <p>This plot is showing how well the range of estimated chains fit the observed data.</p> <p>CONGRATULATIONS! you just ran you first MCMC!</p> In\u00a0[9]: Copied! <pre>## Cell 6 ##\n\nimport pandas as pd\ndata = pd.read_csv(\"../../case_study_1/python/data/phaeocystis_control.csv\")\ntime = data['times'].values\nobs = data['cells'].values\n</pre> ## Cell 6 ##  import pandas as pd data = pd.read_csv(\"../../case_study_1/python/data/phaeocystis_control.csv\") time = data['times'].values obs = data['cells'].values <p>$$ \\frac{dy}{dt} = (\\mu - \\delta)\\, y $$</p> <p>Here we are simply subtracting a delta from mu so that delta can simulate death. In short, we don't always need death (or something else comperable) data. We can simply make our ODE expressive about the data!</p> In\u00a0[10]: Copied! <pre>## Cell 7 ##\n\ndef ode(y, t, params):\n    mum,delta = params[0],params[1]\n    return [(mum-delta) * y[0]]\n</pre> ## Cell 7 ##  def ode(y, t, params):     mum,delta = params[0],params[1]     return [(mum-delta) * y[0]] <p>Here we have our new model setup. We have two unique parameters (mum, delta) so we'll set n_theta to 2.</p> In\u00a0[11]: Copied! <pre>## Cell 8 ##\n\nimport pymc as pm\ncell_model = pm.ode.DifferentialEquation(\n    func=ode,\n    times=data['times'].values,\n    n_states=1,\n    n_theta=2,\n    t0=0\n)\n</pre> ## Cell 8 ##  import pymc as pm cell_model = pm.ode.DifferentialEquation(     func=ode,     times=data['times'].values,     n_states=1,     n_theta=2,     t0=0 ) <p>Here is the model. Everything here should make sense. If something is confusing see the explination above cell 4. Still note that we have two likelihoods at the bottom since we are estimating two states.</p> In\u00a0[12]: Copied! <pre>## Cell 9 ##\n\nimport numpy as np\nwith pm.Model() as model:\n    mum = pm.TruncatedNormal('mum', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)\n    delta = pm.TruncatedNormal('delta', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)\n    N0 = pm.Lognormal('N0', mu=np.log(obs[0]), sigma=0.1)\n    sigma = pm.HalfNormal(\"sigma\", 1)\n\n    y_hat = cell_model(y0=[N0], theta=[mum, delta])\n    \n    pm.Normal(\"Y_obs\", mu=pm.math.log(y_hat[:, 0]), sigma=sigma, observed=np.log(obs))\n</pre> ## Cell 9 ##  import numpy as np with pm.Model() as model:     mum = pm.TruncatedNormal('mum', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)     delta = pm.TruncatedNormal('delta', mu=0.5, sigma=0.3, lower=0.0, upper=1.0)     N0 = pm.Lognormal('N0', mu=np.log(obs[0]), sigma=0.1)     sigma = pm.HalfNormal(\"sigma\", 1)      y_hat = cell_model(y0=[N0], theta=[mum, delta])          pm.Normal(\"Y_obs\", mu=pm.math.log(y_hat[:, 0]), sigma=sigma, observed=np.log(obs)) <p>Everything here is the same as before! See explination above cell 5 if you forgot something.</p> In\u00a0[13]: Copied! <pre>## Cell 10 ##\n\n'''\nwith model:\n    trace = pm.sample(draws=1000, tune=1000, target_accept=0.95, chains=4, return_inferencedata=True)\n'''\nimport arviz as az\nimport matplotlib.pyplot as plt\n\ntrace = az.from_netcdf(\"chains/case_study_1_p2_trace.nc\")\n</pre> ## Cell 10 ##  ''' with model:     trace = pm.sample(draws=1000, tune=1000, target_accept=0.95, chains=4, return_inferencedata=True) ''' import arviz as az import matplotlib.pyplot as plt  trace = az.from_netcdf(\"chains/case_study_1_p2_trace.nc\") In\u00a0[14]: Copied! <pre>plot_trace(\ntrace=trace,\nmodel=model,\nvar_names_map={'N0':'Initial density/ml','mum': 'Growth Rate \u03bc (/day)', 'delta': 'Death Rate \u03b4 (/day)','sigma': 'Std Dev \u03c3'},\nvar_order=['mum','delta','N0','sigma'],\nfontname='Arial',\nfontsize=12,\nnum_prior_samples=2000,\nsave_path='figures/normal_growthdeath_chains.png'\n)\n</pre> plot_trace( trace=trace, model=model, var_names_map={'N0':'Initial density/ml','mum': 'Growth Rate \u03bc (/day)', 'delta': 'Death Rate \u03b4 (/day)','sigma': 'Std Dev \u03c3'}, var_order=['mum','delta','N0','sigma'], fontname='Arial', fontsize=12, num_prior_samples=2000, save_path='figures/normal_growthdeath_chains.png' )  <pre>Sampling: [N0, Y_obs, delta, mum, sigma]\n</pre> Out[14]: <pre>array([[&lt;Axes: title={'center': 'Growth Rate \u03bc (/day)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Growth Rate \u03bc (/day)'}&gt;],\n       [&lt;Axes: title={'center': 'Death Rate \u03b4 (/day)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Death Rate \u03b4 (/day)'}&gt;],\n       [&lt;Axes: title={'center': 'Initial density/ml'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Initial density/ml'}&gt;],\n       [&lt;Axes: title={'center': 'Std Dev \u03c3'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Std Dev \u03c3'}&gt;]], dtype=object)</pre> In\u00a0[15]: Copied! <pre>dataset_postprocessing = {\n\"Cells\": [\n        {\"time\": time, \"values\": obs},  # replicate 1\n    ]\n}\n\ndef ode_solution2data(solution):    \n    total = solution[:, 0]  # Assuming the first column is the total cell count\n    return {\n        \"total\": total\n    }\n\nposterior_dynamics(\ndataset=dataset_postprocessing,\ntrace=trace,\nmodel=model,\nn_plots=100,\nburn_in=50,\nnum_variables=1,\node_fn=ode,\node2data_fn=ode_solution2data,\nsave_path=\"figures/vardi_logistic_growthdeath_dynamics.png\",\n# the key of this dict is the variable name in the dataset_postprocessing\nvar_properties={\n    \"Cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}\n},\nsuptitle=\"Posterior Predictive Dynamics\",\ncolor_lines='red'\n)\n</pre> dataset_postprocessing = { \"Cells\": [         {\"time\": time, \"values\": obs},  # replicate 1     ] }  def ode_solution2data(solution):         total = solution[:, 0]  # Assuming the first column is the total cell count     return {         \"total\": total     }  posterior_dynamics( dataset=dataset_postprocessing, trace=trace, model=model, n_plots=100, burn_in=50, num_variables=1, ode_fn=ode, ode2data_fn=ode_solution2data, save_path=\"figures/vardi_logistic_growthdeath_dynamics.png\", # the key of this dict is the variable name in the dataset_postprocessing var_properties={     \"Cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True} }, suptitle=\"Posterior Predictive Dynamics\", color_lines='red' ) Out[15]: <pre>(&lt;Figure size 500x500 with 1 Axes&gt;,\n [&lt;Axes: title={'center': 'Total'}, xlabel='Time (days)', ylabel='Total cell density (/ml)'&gt;])</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"education/python/case_study_1/#case-study-1-exponential-growth","title":"Case Study 1: Exponential Growth\u00b6","text":"<p>We are going to establish an exponential model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! Pandas is the defacto python package for reading in data. It's also smart to handle any necessary data conversions during this step. You may not need to and that's just fine! It all depends on what you are looking for.</p>"},{"location":"education/python/case_study_1/#case-study-1-exponential-growth-and-death","title":"Case Study 1: Exponential Growth and Death\u00b6","text":"<p>Now on to a more complex model. We will be using the same data as before.</p>"},{"location":"education/python/case_study_1/#the-end","title":"The End\u00b6","text":""},{"location":"education/python/case_study_2/","title":"Case Study 2 \u2014 Priors, noise models, diagnostics","text":"In\u00a0[1]: Copied! <pre># For user-defined post processing and plotting functions\nimport sys\nsys.path.insert(0, \"../..\")\nfrom utils.plot_utils_v2 import *\n</pre> # For user-defined post processing and plotting functions import sys sys.path.insert(0, \"../..\") from utils.plot_utils_v2 import * <p>Welcome to the \"how to\" of Markov Chain Monte Carlo using PYMC. A python software package allowing you to fit complex models with ease.</p> In\u00a0[2]: Copied! <pre>## Cell 1 ##\n\nimport pandas as pd\n\n# import alive\ndataset = pd.read_csv(\"../../case_study_2/python/data/total_cells.csv\")\ncells = dataset.tail(15)\ncells_time = cells['Time (days)'].values\ncells_density = 1e6*cells[' Density (1e6/ml)'].values\n</pre> ## Cell 1 ##  import pandas as pd  # import alive dataset = pd.read_csv(\"../../case_study_2/python/data/total_cells.csv\") cells = dataset.tail(15) cells_time = cells['Time (days)'].values cells_density = 1e6*cells[' Density (1e6/ml)'].values <p>Here is the first thing that needs to be done. We will setup the logistic ODE that will adapt to the data. To do this, you will use variables that PyMC gives you by 'default'. Let me explain. Note that in Cell 2 there are 3 parameters being passed in (y, t, params). You will also not see these variables being explicitly passed in this way anywhere. Here is what happens. When you create the ODE function in Cell 2, it gets passed to the the 'cell_model' in Cell 3. Now look to where the 'cell_model' is called in cell 4. There it passes priors into Y0 and theta. The connection is Y0 goes to y, and theta goes to params. The order you hand over the priors matters so call them in the correct order when creating the ODE equation. Be sure to keep this in mind and take note of the position of you priors when you pass them, and the indicies used in y and params to setup your variables and inital conditions!</p> <p>P: Initial Value r: Growith Rate K: Carrying Capacity</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P $$</p> In\u00a0[3]: Copied! <pre>## Cell 2 ##\n\ndef logistic_growth(y, t, params):\n    P = y[0]\n    \n    r = params[0]\n    K = params[1]\n    \n    return r * (1 - P / K) * P\n</pre> ## Cell 2 ##  def logistic_growth(y, t, params):     P = y[0]          r = params[0]     K = params[1]          return r * (1 - P / K) * P  <p>Here is the model we spoke about earlier. Here is what goes into each variable.</p> <p>func: the ODE function you just created times: the time steps from your data n_states: possible states count n_theta: Unique parameter count t0: inital time step (should almost always be 0)</p> In\u00a0[4]: Copied! <pre>## Cell 3 ##\n\nimport pymc as pm\ncell_model = pm.ode.DifferentialEquation(\n    func=logistic_growth,\n    times=cells_time,\n    n_states=1,\n    n_theta=2,\n    t0=0\n)\n</pre> ## Cell 3 ##  import pymc as pm cell_model = pm.ode.DifferentialEquation(     func=logistic_growth,     times=cells_time,     n_states=1,     n_theta=2,     t0=0 ) <pre>c:\\Users\\Whisk\\anaconda3\\envs\\MCMC_Modeling\\Lib\\site-packages\\pytensor\\link\\c\\cmodule.py:2968: UserWarning: PyTensor could not link to a BLAS installation. Operations that might benefit from BLAS will be severely degraded.\nThis usually happens when PyTensor is installed via pip. We recommend it be installed via conda/mamba/pixi instead.\nAlternatively, you can use an experimental backend such as Numba or JAX that perform their own BLAS optimizations, by setting `pytensor.config.mode == 'NUMBA'` or passing `mode='NUMBA'` when compiling a PyTensor function.\nFor more options and details see https://pytensor.readthedocs.io/en/latest/troubleshooting.html#how-do-i-configure-test-my-blas-library\n  warnings.warn(\n</pre> <p>Here is where we assign our priors. Choosing priors is an interesting combination of art and science. You have all kinds of options. Uniform, normal, lognormal, halfnormal. It all depends on what you need. With these uniform priors we are essentially saying. We believe the proper value for the model will be equally likely within this range and WILL NOT be outside this range. Your half normal distribution is a PDF that is only posotive and extends to infinity. This is usually used for your error or standard deviation. This is saying we know we can not have negative error, and we believe the error will be closer to 0 than farther away, however, we are not explicitly capping it off in the positive direction. Below are the priors visualized.</p> <p></p> <p>Next is getting the current ODE results. We spoke about how to pass the priors in previously. Be careful during this step!</p> <p>The final function is the Likelihood of the model. This is where we see how well the model results estimatied the data.</p> <p>We name the random variable 'y_total'.</p> <p>For mu, this is a bit complicated so we'll break this down.</p> <ul> <li>y_hat[:,0] pulls the predicited trajectory for live cells from the ODE solution, note it's position in y_hat matches it's priors position when passed to cell_model.</li> <li>pm.math.clip() prevents taking log(0) by setting a minimum positive value to near 0.</li> <li>pm.math.log() puts predictions on a log scale which matches the log transformed observed data.</li> </ul> <p>Sigma: The prior you just created for it! Simple. Observered: The true cell data you read in earlier.</p> In\u00a0[5]: Copied! <pre>## Cell 4 ##\n\nimport numpy as np\nwith pm.Model() as model:\n    r = pm.Uniform(r\"$r$ (growth rate)\", lower=0.5, upper=1)\n    K = pm.Uniform(r\"$K$ (carrying capacity)\" , lower=1e6, upper=4e7)\n    P0 = pm.Uniform(r\"$P_0$ (init. live)\", lower=1e5, upper=3e5)\n    sigma_live = pm.HalfNormal(r\"$\\sigma_L$\", 3)\n\n    y_hat = cell_model(y0=[P0], theta=[r,K])\n    \n    pm.Normal(\"Y_total\", \n              mu=pm.math.log(pm.math.clip(y_hat[:, 0], 1e-8, np.inf)),\n              sigma=sigma_live,\n              observed =  np.log(cells_density))\n</pre> ## Cell 4 ##  import numpy as np with pm.Model() as model:     r = pm.Uniform(r\"$r$ (growth rate)\", lower=0.5, upper=1)     K = pm.Uniform(r\"$K$ (carrying capacity)\" , lower=1e6, upper=4e7)     P0 = pm.Uniform(r\"$P_0$ (init. live)\", lower=1e5, upper=3e5)     sigma_live = pm.HalfNormal(r\"$\\sigma_L$\", 3)      y_hat = cell_model(y0=[P0], theta=[r,K])          pm.Normal(\"Y_total\",                mu=pm.math.log(pm.math.clip(y_hat[:, 0], 1e-8, np.inf)),               sigma=sigma_live,               observed =  np.log(cells_density))  <p>When you are finally ready to run the model you will do so in the form shown between the '''. Here are what the variables being passed into pm.sample mean.</p> <p>draws: poterior values to be generated. tune: Some under the hood PyMC magic that 'warms it up'. Keep this between 500 and 2000 depending on how complex your model is. chains / cores: Amount of chains you want to compute in parallel. Keep these values the same for most basic usage. return_inferencedata: returns an ARVIZ inferencedata object which we'll use for easy reading of our output. target_accept: the % of times you accept the improved likelihood at each step of the chain. This is your exploration vs exploitation balance.</p> <p>To save time, we are just going to read in the pre-computed trace and print so you can see the output. We will use ARVIZ and Matplotlib to do so.</p> In\u00a0[6]: Copied! <pre>## Cell 5 ##\n\n'''\nwith model:\n    trace = pm.sample(draws=2000, tune=500, chains=3, return_inferencedata=True, target_accept=0.95, cores = 3) \n'''\nimport arviz as az\nimport matplotlib.pyplot as plt\n\ntrace = az.from_netcdf(\"chains/case_study_2_p1_trace.nc\")\n</pre> ## Cell 5 ##  ''' with model:     trace = pm.sample(draws=2000, tune=500, chains=3, return_inferencedata=True, target_accept=0.95, cores = 3)  ''' import arviz as az import matplotlib.pyplot as plt  trace = az.from_netcdf(\"chains/case_study_2_p1_trace.nc\") <p>To interpret the output we have some user-defined postprocessing and plotting functions. They are in our GitHub repo.</p> In\u00a0[7]: Copied! <pre>plot_trace(\ntrace=trace,\nmodel=model,\nfontname='Arial',\nfontsize=12,\nnum_prior_samples=200,\nsave_path='figures/vardi_logistic_growth_chains.png'\n)\n</pre> plot_trace( trace=trace, model=model, fontname='Arial', fontsize=12, num_prior_samples=200, save_path='figures/vardi_logistic_growth_chains.png' ) <pre>Sampling: [$K$ (carrying capacity), $P_0$ (init. live), $\\sigma_L$, $r$ (growth rate), Y_total]\n</pre> Out[7]: <pre>array([[&lt;Axes: title={'center': '$r$ (growth rate)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$r$ (growth rate)'}&gt;],\n       [&lt;Axes: title={'center': '$K$ (carrying capacity)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$K$ (carrying capacity)'}&gt;],\n       [&lt;Axes: title={'center': '$P_0$ (init. live)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$P_0$ (init. live)'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\sigma_L$'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$\\\\sigma_L$'}&gt;]], dtype=object)</pre> <p>Now we interpret the results. Remember the ODE's from before.</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P $$</p> <p>Take note of x-axis values at the peaks of the PDFs and the variables they represent. The x-axis values at those peaks are what the model found to be the optimal value for the equation. The right hand column is a frequency plot. This is just to make sure the model is exploring the parameter space well enough.</p> In\u00a0[8]: Copied! <pre>dataset_postprocessing = {\n\"Total cells\": [\n        {\"time\": cells_time, \"values\":  cells_density},  # replicate 1\n    ]\n}\n\ndef ode_solution2data(solution):\n    total = solution[:, 0]  # Assuming the first column is the total cell count\n    return {\n        \"total\": total\n    }\n\nposterior_dynamics(\ndataset=dataset_postprocessing,\ntrace=trace,\nmodel=model,\nn_plots=100,\nburn_in=50,\nnum_variables=1,\node_fn=logistic_growth,\node2data_fn=ode_solution2data,\nsave_path=\"figures/vardi_logistic_growth_dynamics.png\",\nvar_properties={\n    \"Total cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True}\n},\nsuptitle=\"Posterior Predictive Dynamics\"\n)\n</pre> dataset_postprocessing = { \"Total cells\": [         {\"time\": cells_time, \"values\":  cells_density},  # replicate 1     ] }  def ode_solution2data(solution):     total = solution[:, 0]  # Assuming the first column is the total cell count     return {         \"total\": total     }  posterior_dynamics( dataset=dataset_postprocessing, trace=trace, model=model, n_plots=100, burn_in=50, num_variables=1, ode_fn=logistic_growth, ode2data_fn=ode_solution2data, save_path=\"figures/vardi_logistic_growth_dynamics.png\", var_properties={     \"Total cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\",\"sol_key\": \"total\",\"log\": True} }, suptitle=\"Posterior Predictive Dynamics\" ) Out[8]: <pre>(&lt;Figure size 500x500 with 1 Axes&gt;,\n [&lt;Axes: title={'center': 'Total'}, xlabel='Time (days)', ylabel='Total cell density (/ml)'&gt;])</pre> <p>This plot is showing how well the range of estimated chains fit the observed data.</p> <p>CONGRATULATIONS! you just ran you first MCMC!</p> In\u00a0[9]: Copied! <pre>## Cell 6 ##\n\nimport pandas as pd\n\n# import alive\ndataset = pd.read_csv(\"../../case_study_2/python/data/total_cells.csv\")\ncells = dataset.tail(15)\ncells_time = cells['Time (days)'].values\ncells_density = 1e6*cells[' Density (1e6/ml)'].values\n\n# import dead\ndeath_dataset = pd.read_csv(\"../../case_study_2/python/data/death_percentage.csv\")\ndeath = death_dataset.tail(15)\ndead_time = death['Time (days)'].values\ndead_density = death[' Dead percentage '].values*cells_density/100\n</pre> ## Cell 6 ##  import pandas as pd  # import alive dataset = pd.read_csv(\"../../case_study_2/python/data/total_cells.csv\") cells = dataset.tail(15) cells_time = cells['Time (days)'].values cells_density = 1e6*cells[' Density (1e6/ml)'].values  # import dead death_dataset = pd.read_csv(\"../../case_study_2/python/data/death_percentage.csv\") death = death_dataset.tail(15) dead_time = death['Time (days)'].values dead_density = death[' Dead percentage '].values*cells_density/100  <p>ODE for living cells accounting for cell death</p> <p>$$ \\frac{dP}{dt} = r \\left(1 - \\frac{P}{K}\\right) P - \\delta P $$</p> <p>ODE for dead cells</p> <p>$$ \\frac{dD}{dt} = \\delta P $$</p> <p>We follow the same rules as before when creating this ODE. The new part of this is how we return the equation to PYMC. Note that in our previous ODE we returned an equation of lists to to PyMC that looked like</p> <p>[] = [] * (1 = [] / []) * []</p> <p>Since PyMC expects a matrix as a return value, it was able to accept the 1-D column right there since that is still a matrix.</p> <p>We take advantage of this when we have multiple equations. All you have to do is expand the matrix.</p> <p>x[[],[]] x[0] = [] * (1 = [] / []) * [] - [] * [] x[1] = [] * []</p> In\u00a0[10]: Copied! <pre>## Cell 7 ##\n\n\ndef logistic_growth_death(y, t, params):\n    P = y[0]\n    r = params[0]\n    K = params[1]\n    delta = params[2]\n\n    dydt = []\n    dydt.append(r * (1 - P / K) * P - delta * P)\n    dydt.append(delta * P)\n    return dydt\n</pre> ## Cell 7 ##   def logistic_growth_death(y, t, params):     P = y[0]     r = params[0]     K = params[1]     delta = params[2]      dydt = []     dydt.append(r * (1 - P / K) * P - delta * P)     dydt.append(delta * P)     return dydt <p>Here we have our new model setup. We have 3 unique parameters (r, K, delta) so we'll set n_theta to 3. n_states will now be 2 since we now have two ODE's we are tracking two states (alive, dead).</p> <p>note: We will end up with 5 priors for the 5 variables just mentioned. Plus two more for sigma_live and sigma_dead. Totaling 7.</p> In\u00a0[11]: Copied! <pre>## Cell 8 ##\nimport pymc as pm\ncell_model = pm.ode.DifferentialEquation(\n    func=logistic_growth_death,\n    times=cells_time,\n    n_states=2,\n    n_theta=3, \n    t0=0\n)\n</pre> ## Cell 8 ## import pymc as pm cell_model = pm.ode.DifferentialEquation(     func=logistic_growth_death,     times=cells_time,     n_states=2,     n_theta=3,      t0=0 ) <p>This portion is new! When in doubt always create helper functions. They improve readability and modularity of your code. In practice it also helps debugging.</p> <p>Now that we have more information in our output, it will help to have a function break it down for us to make calling it easier later on. Here we just split the y_hat output into live and dead. Remember that the live in our model is at position 0 and dead is at 1. Additionally we want total and dead, not live and dead so we do a little addition and we're done. You can do whatever you want to here to help make the next block run cleaner.</p> In\u00a0[12]: Copied! <pre>## Cell 9 ##\n\ndef ode_solution2data(solution):\n    live = solution[:, 0]\n    dead = solution[:, 1]\n    total = live + dead\n    return {\n        \"total\": total,\n        \"dead\": dead\n    }\n</pre> ## Cell 9 ##  def ode_solution2data(solution):     live = solution[:, 0]     dead = solution[:, 1]     total = live + dead     return {         \"total\": total,         \"dead\": dead     } <p>Here is the model. Everything here should make sense. If something is confusing see the explination above cell 4. Still note that we have two likelihoods at the bottom since we are estimating two states.</p> In\u00a0[13]: Copied! <pre>## Cell 10 ##\nimport numpy as np\nwith pm.Model() as model:\n    r = pm.Uniform(r\"$r$ (growth rate)\", lower=0.5, upper=1)\n    K = pm.Uniform(r\"$K$ (carrying capacity)\" , lower=1e6, upper=4e7)\n    delta = pm.Uniform(r\"$\\delta$ (death rate)\", lower=0.0, upper=0.15)\n    P0 = pm.Uniform(r\"$P_0$ (init. live)\", lower=1e5, upper=3e5)\n    D0 = pm.Uniform(r\"$D_0$ (init. dead)\", lower=1e4, upper=7e4)\n\n    sigma_live = pm.HalfNormal(r\"$\\sigma_L$\", 3)\n    sigma_dead = pm.HalfNormal(r\"$\\sigma_D$\", 3)\n\n    y_hat = cell_model(y0=[P0,D0], theta=[r,K,delta])\n    y_hat_sol = ode_solution2data(y_hat)\n    total_solution = y_hat_sol['total']\n    dead_solution = y_hat_sol['dead']\n\n    pm.Normal(\"Y_live\", mu=pm.math.log(pm.math.clip(total_solution, 1e-8, np.inf)),sigma=sigma_live,\n            observed =  np.log(cells_density))\n\n    pm.Normal(\"Y_dead\", mu=pm.math.log(pm.math.clip(dead_solution, 1e-8, np.inf)),sigma=sigma_dead,\n            observed=np.log(dead_density))\n</pre> ## Cell 10 ## import numpy as np with pm.Model() as model:     r = pm.Uniform(r\"$r$ (growth rate)\", lower=0.5, upper=1)     K = pm.Uniform(r\"$K$ (carrying capacity)\" , lower=1e6, upper=4e7)     delta = pm.Uniform(r\"$\\delta$ (death rate)\", lower=0.0, upper=0.15)     P0 = pm.Uniform(r\"$P_0$ (init. live)\", lower=1e5, upper=3e5)     D0 = pm.Uniform(r\"$D_0$ (init. dead)\", lower=1e4, upper=7e4)      sigma_live = pm.HalfNormal(r\"$\\sigma_L$\", 3)     sigma_dead = pm.HalfNormal(r\"$\\sigma_D$\", 3)      y_hat = cell_model(y0=[P0,D0], theta=[r,K,delta])     y_hat_sol = ode_solution2data(y_hat)     total_solution = y_hat_sol['total']     dead_solution = y_hat_sol['dead']      pm.Normal(\"Y_live\", mu=pm.math.log(pm.math.clip(total_solution, 1e-8, np.inf)),sigma=sigma_live,             observed =  np.log(cells_density))      pm.Normal(\"Y_dead\", mu=pm.math.log(pm.math.clip(dead_solution, 1e-8, np.inf)),sigma=sigma_dead,             observed=np.log(dead_density)) <p>Everything here is that same as before! See the explination above cell 5 if you forgot something.</p> In\u00a0[14]: Copied! <pre>## Cell 11 ##\n'''\nwith model:\n    trace = pm.sample(draws=1000, tune=500, chains=3, return_inferencedata=True, target_accept=0.95, cores = 3) \n'''\nimport arviz as az\ntrace = az.from_netcdf(\"chains/case_study_2_p2_trace.nc\")\n</pre> ## Cell 11 ## ''' with model:     trace = pm.sample(draws=1000, tune=500, chains=3, return_inferencedata=True, target_accept=0.95, cores = 3)  ''' import arviz as az trace = az.from_netcdf(\"chains/case_study_2_p2_trace.nc\") In\u00a0[15]: Copied! <pre>plot_trace(\ntrace=trace,\nmodel=model,\nfontname='Arial',\nfontsize=12,\nnum_prior_samples=2000,\nsave_path='figures/vardi_growth_death_chains.png'\n)\n</pre> plot_trace( trace=trace, model=model, fontname='Arial', fontsize=12, num_prior_samples=2000, save_path='figures/vardi_growth_death_chains.png' )  <pre>Sampling: [$D_0$ (init. dead), $K$ (carrying capacity), $P_0$ (init. live), $\\delta$ (death rate), $\\sigma_D$, $\\sigma_L$, $r$ (growth rate), Y_dead, Y_live]\n</pre> Out[15]: <pre>array([[&lt;Axes: title={'center': '$r$ (growth rate)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$r$ (growth rate)'}&gt;],\n       [&lt;Axes: title={'center': '$K$ (carrying capacity)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$K$ (carrying capacity)'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\delta$ (death rate)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$\\\\delta$ (death rate)'}&gt;],\n       [&lt;Axes: title={'center': '$P_0$ (init. live)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$P_0$ (init. live)'}&gt;],\n       [&lt;Axes: title={'center': '$D_0$ (init. dead)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$D_0$ (init. dead)'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\sigma_L$'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$\\\\sigma_L$'}&gt;],\n       [&lt;Axes: title={'center': '$\\\\sigma_D$'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '$\\\\sigma_D$'}&gt;]], dtype=object)</pre> In\u00a0[16]: Copied! <pre>dataset_postprocessing = {\n\"Total cells\": [\n        {\"time\": cells_time, \"values\":  cells_density},  # replicate 1\n    ],\n\"Dead cells\": [\n        {\"time\": dead_time, \"values\": dead_density},  # replicate 1\n    ]\n}\n\ndef ode_solution2data(solution):\n    live = solution[:, 0]\n    dead = solution[:, 1]\n    total = live + dead\n    return {\n        \"total\": total,\n        \"dead\": dead\n    }\n\nposterior_dynamics(\ndataset=dataset_postprocessing,\ntrace=trace,\nmodel=model,\nn_plots=100,\nburn_in=50,\nnum_variables=2,\node_fn=logistic_growth_death,\node2data_fn=ode_solution2data,\nsave_path=\"figures/vardi_growth_death_dynamics.png\",\nvar_properties={\n    \"Total cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\", \"sol_key\": \"total\",\"log\": True},\n    \"Dead cells\": {\"label\": \"Dead\", \"color\": \"black\", \"ylabel\": \"Dead cell density (/ml)\", \"xlabel\":\"Time (days)\", \"sol_key\": \"dead\",\"log\": True},\n},\nsuptitle=\"Posterior Predictive Dynamics\"\n)\n</pre> dataset_postprocessing = { \"Total cells\": [         {\"time\": cells_time, \"values\":  cells_density},  # replicate 1     ], \"Dead cells\": [         {\"time\": dead_time, \"values\": dead_density},  # replicate 1     ] }  def ode_solution2data(solution):     live = solution[:, 0]     dead = solution[:, 1]     total = live + dead     return {         \"total\": total,         \"dead\": dead     }  posterior_dynamics( dataset=dataset_postprocessing, trace=trace, model=model, n_plots=100, burn_in=50, num_variables=2, ode_fn=logistic_growth_death, ode2data_fn=ode_solution2data, save_path=\"figures/vardi_growth_death_dynamics.png\", var_properties={     \"Total cells\": {\"label\": \"Total\", \"color\": \"black\", \"ylabel\": \"Total cell density (/ml)\", \"xlabel\":\"Time (days)\", \"sol_key\": \"total\",\"log\": True},     \"Dead cells\": {\"label\": \"Dead\", \"color\": \"black\", \"ylabel\": \"Dead cell density (/ml)\", \"xlabel\":\"Time (days)\", \"sol_key\": \"dead\",\"log\": True}, }, suptitle=\"Posterior Predictive Dynamics\" ) Out[16]: <pre>(&lt;Figure size 1000x500 with 2 Axes&gt;,\n array([&lt;Axes: title={'center': 'Total'}, xlabel='Time (days)', ylabel='Total cell density (/ml)'&gt;,\n        &lt;Axes: title={'center': 'Dead'}, xlabel='Time (days)', ylabel='Dead cell density (/ml)'&gt;],\n       dtype=object))</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"education/python/case_study_2/#case-study-2-logistic-growth","title":"Case Study 2: Logistic Growth\u00b6","text":"<p>We are going to establish a logistic model in this case study. The ODE and it's solution will be defined in the near future.</p> <p>First things first. Data! Pandas is the defacto python package for reading in data. It's also smart to handle any necessary data conversions during this step. You may not need to and that's just fine! It all depends on what you are looking for.</p>"},{"location":"education/python/case_study_2/#case-study-2-logistic-growth-and-death","title":"Case Study 2: Logistic Growth and Death\u00b6","text":"<p>Now on to a more complex model. We will be using the same data as before as well as some data on dead cells. We will also being using two ODEs!</p>"},{"location":"education/python/case_study_2/#the-end","title":"THE END\u00b6","text":""},{"location":"education/python/case_study_3/","title":"Case Study 3 \u2014 Posterior inference & interpretation","text":"In\u00a0[1]: Copied! <pre># For user-defined post processing and plotting functions\nimport sys\nsys.path.insert(0, \"../..\")\nfrom utils.plot_utils_v2 import *\n</pre> # For user-defined post processing and plotting functions import sys sys.path.insert(0, \"../..\") from utils.plot_utils_v2 import * In\u00a0[2]: Copied! <pre>import pandas as pd\ndataset = pd.read_csv(\"../../case_study_3/python/data/total_cells.csv\")\nehux_cells = dataset.tail(15)\nehux_total_time = ehux_cells['Time (days)'].values\nehux_total_density = 1e6*ehux_cells[' Density (1e6/ml)'].values\n\n\ndeath_dataset = pd.read_csv(\"../../case_study_3/python/data/death_percentage.csv\")\nehux_death = death_dataset.tail(15)\nehux_death_time = ehux_death['Time (days)'].values\nehux_dead_density = ehux_death[' Dead percentage '].values*ehux_total_density/100\n</pre> import pandas as pd dataset = pd.read_csv(\"../../case_study_3/python/data/total_cells.csv\") ehux_cells = dataset.tail(15) ehux_total_time = ehux_cells['Time (days)'].values ehux_total_density = 1e6*ehux_cells[' Density (1e6/ml)'].values   death_dataset = pd.read_csv(\"../../case_study_3/python/data/death_percentage.csv\") ehux_death = death_dataset.tail(15) ehux_death_time = ehux_death['Time (days)'].values ehux_dead_density = ehux_death[' Dead percentage '].values*ehux_total_density/100 In\u00a0[3]: Copied! <pre>def general_case(t, y, params):\n    N, P, D = y\n    mu_max = params[0]\n    Ks = params[1]\n    Qn = params[2]\n    delta = params[3]\n\n    P_m3 = P * 1e6  # cells/mL \u2192 cells/m\u00b3\n    mu = mu_max * N / (N + Ks)\n\n    dNdt = -Qn * mu * P_m3\n    dPdt = mu * P - delta * P\n    dDdt = delta * P\n\n    return [dNdt, dPdt, dDdt]\n</pre> def general_case(t, y, params):     N, P, D = y     mu_max = params[0]     Ks = params[1]     Qn = params[2]     delta = params[3]      P_m3 = P * 1e6  # cells/mL \u2192 cells/m\u00b3     mu = mu_max * N / (N + Ks)      dNdt = -Qn * mu * P_m3     dPdt = mu * P - delta * P     dDdt = delta * P      return [dNdt, dPdt, dDdt]  In\u00a0[4]: Copied! <pre>import pytensor.tensor as pt\nfrom pytensor.graph.op import Op\nfrom scipy.integrate import solve_ivp\n\nclass SolveIVPWrapper(Op):\n    itypes = [pt.dvector]  # theta + y0\n    otypes = [pt.dmatrix]  # solution: (len(t), 3)\n\n    def __init__(self, times):\n        self.times = times\n\n    def perform(self, node, inputs, outputs):\n        theta_y0, = inputs\n        theta = theta_y0[:4]\n        y0 = theta_y0[4:]\n\n        sol = solve_ivp(\n            fun=lambda t, y: general_case(t, y, theta),\n            t_span=(self.times[0], self.times[-1]),\n            y0=y0,\n            t_eval=self.times,\n            method=\"LSODA\"\n        )\n\n        if not sol.success:\n            raise RuntimeError(\"ODE solver failed:\", sol.message)\n\n        outputs[0][0] = sol.y.T  # shape: (time, 3)\n</pre> import pytensor.tensor as pt from pytensor.graph.op import Op from scipy.integrate import solve_ivp  class SolveIVPWrapper(Op):     itypes = [pt.dvector]  # theta + y0     otypes = [pt.dmatrix]  # solution: (len(t), 3)      def __init__(self, times):         self.times = times      def perform(self, node, inputs, outputs):         theta_y0, = inputs         theta = theta_y0[:4]         y0 = theta_y0[4:]          sol = solve_ivp(             fun=lambda t, y: general_case(t, y, theta),             t_span=(self.times[0], self.times[-1]),             y0=y0,             t_eval=self.times,             method=\"LSODA\"         )          if not sol.success:             raise RuntimeError(\"ODE solver failed:\", sol.message)          outputs[0][0] = sol.y.T  # shape: (time, 3) In\u00a0[5]: Copied! <pre># === Convert solution to observables ===\ndef ode_solution2data(solution):\n    live = solution[:, 1]\n    dead = solution[:, 2]\n    total = live + dead\n    return {\n        \"total\": total,\n        \"dead\": dead\n    }\n</pre> # === Convert solution to observables === def ode_solution2data(solution):     live = solution[:, 1]     dead = solution[:, 2]     total = live + dead     return {         \"total\": total,         \"dead\": dead     } In\u00a0[6]: Copied! <pre>import pymc as pm\nimport numpy as np\n\node_op = SolveIVPWrapper(ehux_total_time)\n\nwith pm.Model() as model:\n    mu_max = pm.Uniform(\"mu_max\", 0.4, 0.7)\n    Ks = pm.Uniform(\"Ks\", 0.05, 0.2)\n    Qn = pm.Uniform(\"Qn\", 1e-10, 7e-10)\n    delta = pm.Uniform(\"delta\", 0.01, 0.09)\n\n    N0 = pm.Deterministic(\"N0\", 1000 + ((500 / 1.8e-10) * (Qn - 3.2e-10)))\n    P0 = pm.LogNormal(\"P0\", mu=12.2175, sigma=0.1)\n    D0 = pm.LogNormal(\"D0\", mu=10.2804, sigma=0.1)\n\n    sigma_live = pm.HalfNormal(\"sigma_live\", 1)\n    sigma_dead = pm.HalfNormal(\"sigma_dead\", 1)\n\n    # Solve ODE\n    sol = ode_op(pt.stack([mu_max, Ks, Qn, delta, N0, P0, D0]))\n\n    total = sol[:, 1] + sol[:, 2]\n    dead = sol[:, 2]\n\n    pm.Normal(\"Y_total\", mu=pt.log(total), sigma=sigma_live, observed=np.log(ehux_total_density))\n    pm.Normal(\"Y_dead\", mu=pt.log(dead), sigma=sigma_dead, observed=np.log(ehux_dead_density))\n</pre> import pymc as pm import numpy as np  ode_op = SolveIVPWrapper(ehux_total_time)  with pm.Model() as model:     mu_max = pm.Uniform(\"mu_max\", 0.4, 0.7)     Ks = pm.Uniform(\"Ks\", 0.05, 0.2)     Qn = pm.Uniform(\"Qn\", 1e-10, 7e-10)     delta = pm.Uniform(\"delta\", 0.01, 0.09)      N0 = pm.Deterministic(\"N0\", 1000 + ((500 / 1.8e-10) * (Qn - 3.2e-10)))     P0 = pm.LogNormal(\"P0\", mu=12.2175, sigma=0.1)     D0 = pm.LogNormal(\"D0\", mu=10.2804, sigma=0.1)      sigma_live = pm.HalfNormal(\"sigma_live\", 1)     sigma_dead = pm.HalfNormal(\"sigma_dead\", 1)      # Solve ODE     sol = ode_op(pt.stack([mu_max, Ks, Qn, delta, N0, P0, D0]))      total = sol[:, 1] + sol[:, 2]     dead = sol[:, 2]      pm.Normal(\"Y_total\", mu=pt.log(total), sigma=sigma_live, observed=np.log(ehux_total_density))     pm.Normal(\"Y_dead\", mu=pt.log(dead), sigma=sigma_dead, observed=np.log(ehux_dead_density)) In\u00a0[7]: Copied! <pre>def run_inference(model, draws=10000, tune=5000, chains=3, cores=3, threshold_for_slice=5,target_accept=0.9):\n    with model:\n        # Count number of continuous variables (excluding transformed ones)\n        num_params = len(model.free_RVs)\n        #var_names = [v.name for v in model.free_RVs]\n        \n        if num_params &gt; threshold_for_slice:\n            print(f\"Using Slice sampler (parameters: {num_params})\")\n            #step = pm.Slice()\n            step = pm.Metropolis()\n        else:\n            print(f\"Using NUTS sampler (parameters: {num_params})\")\n            step = pm.NUTS(target_accept=target_accept)\n        \n        trace = pm.sample(draws=draws, tune=tune, chains=chains, step=step,\n                          return_inferencedata=True, cores=cores)\n\n    return trace\n</pre>   def run_inference(model, draws=10000, tune=5000, chains=3, cores=3, threshold_for_slice=5,target_accept=0.9):     with model:         # Count number of continuous variables (excluding transformed ones)         num_params = len(model.free_RVs)         #var_names = [v.name for v in model.free_RVs]                  if num_params &gt; threshold_for_slice:             print(f\"Using Slice sampler (parameters: {num_params})\")             #step = pm.Slice()             step = pm.Metropolis()         else:             print(f\"Using NUTS sampler (parameters: {num_params})\")             step = pm.NUTS(target_accept=target_accept)                  trace = pm.sample(draws=draws, tune=tune, chains=chains, step=step,                           return_inferencedata=True, cores=cores)      return trace In\u00a0[8]: Copied! <pre>'''\nwith model:\n    trace = pm.sample(draws=10000, \n                      tune=5000, \n                      chains=3, \n                      step=pm.Metropolis(),\n                      return_inferencedata=True, \n                      cores=3)\n'''\nimport arviz as az\ntrace = az.from_netcdf(\"chains/case_study_3.nc\")\n</pre> ''' with model:     trace = pm.sample(draws=10000,                        tune=5000,                        chains=3,                        step=pm.Metropolis(),                       return_inferencedata=True,                        cores=3) ''' import arviz as az trace = az.from_netcdf(\"chains/case_study_3.nc\") In\u00a0[9]: Copied! <pre>plot_trace(\n    trace=trace,\n    model=model,\n    fontname='Arial',\n    fontsize=12,\n    num_prior_samples=2000,\n    var_names_map={'mu_max': 'Maximum Growth Rate \u03bc (/day)', 'delta': 'Death Rate \u03b4 (/day)', 'Qn': 'Nutrient Quota Qn (ml/cell)', 'P0': 'Initial Live Density (/ml)', 'D0': 'Initial Dead Density (/ml)','sigma_live': '\u03c3 for live cells', 'sigma_dead': '\u03c3 for dead cells'},\n    var_order=['mu_max','delta','Qn','P0','D0','sigma_live','sigma_dead'],\n    save_path='figures/vardi_general_chains_reparam.png'\n    )\n</pre> plot_trace(     trace=trace,     model=model,     fontname='Arial',     fontsize=12,     num_prior_samples=2000,     var_names_map={'mu_max': 'Maximum Growth Rate \u03bc (/day)', 'delta': 'Death Rate \u03b4 (/day)', 'Qn': 'Nutrient Quota Qn (ml/cell)', 'P0': 'Initial Live Density (/ml)', 'D0': 'Initial Dead Density (/ml)','sigma_live': '\u03c3 for live cells', 'sigma_dead': '\u03c3 for dead cells'},     var_order=['mu_max','delta','Qn','P0','D0','sigma_live','sigma_dead'],     save_path='figures/vardi_general_chains_reparam.png'     ) <pre>Sampling: [D0, Ks, P0, Qn, Y_dead, Y_total, delta, mu_max, sigma_dead, sigma_live]\n</pre> Out[9]: <pre>array([[&lt;Axes: title={'center': 'Maximum Growth Rate \u03bc (/day)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Maximum Growth Rate \u03bc (/day)'}&gt;],\n       [&lt;Axes: title={'center': 'Death Rate \u03b4 (/day)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Death Rate \u03b4 (/day)'}&gt;],\n       [&lt;Axes: title={'center': 'Nutrient Quota Qn (ml/cell)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Nutrient Quota Qn (ml/cell)'}&gt;],\n       [&lt;Axes: title={'center': 'Initial Live Density (/ml)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Initial Live Density (/ml)'}&gt;],\n       [&lt;Axes: title={'center': 'Initial Dead Density (/ml)'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': 'Initial Dead Density (/ml)'}&gt;],\n       [&lt;Axes: title={'center': '\u03c3 for live cells'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '\u03c3 for live cells'}&gt;],\n       [&lt;Axes: title={'center': '\u03c3 for dead cells'}, ylabel='Density'&gt;,\n        &lt;Axes: title={'center': '\u03c3 for dead cells'}&gt;]], dtype=object)</pre> <p>Quick mental model</p> <p>SciPy: y[state, time]</p> <p>cell_model: y[time, state]</p> In\u00a0[10]: Copied! <pre># --- fast, clean version (no behavior change) ---\nimport numpy as np\nimport matplotlib.pyplot as plt\nfrom matplotlib.collections import LineCollection\nimport xarray as xr\n\nn_samples = 500\nburn_in = 0\n\n# ---------- 1) Pre-compute once, avoid .flatten() in the loop ----------\nposterior = trace.posterior.stack(draws=(\"chain\", \"draw\"))\n\nif \"N0\" not in posterior:\n    Qn_vals = posterior[\"Qn\"].values\n    N0_vals = 1000 + ((500 / 1.8e-10) * (Qn_vals - 3.2e-10))\n    posterior[\"N0\"] = xr.DataArray(N0_vals, dims=posterior[\"Qn\"].dims, coords=posterior[\"Qn\"].coords)\n\n# Pull 1-D numpy views once (zero copy where possible)\nto_1d = lambda name: np.asarray(posterior[name].values).reshape(-1)\nmu_max_all = to_1d(\"mu_max\")\nKs_all     = to_1d(\"Ks\")\nQn_all     = to_1d(\"Qn\")\ndelta_all  = to_1d(\"delta\")\nN0_all     = to_1d(\"N0\")\nP0_all     = to_1d(\"P0\")\nD0_all     = to_1d(\"D0\")\n\navailable_draws = mu_max_all.shape[0]\nassert burn_in + n_samples &lt;= available_draws, \"Not enough samples\"\n\n# Pick the exact indices we\u2019ll use (no Python increments in the loop)\nidxs = np.arange(burn_in, burn_in + n_samples)\n\n# ---------- 2) Time grid computed once ----------\nt_min = float(min(ehux_total_time.min(), ehux_death_time.min()))\nt_max = float(max(ehux_total_time.max(), ehux_death_time.max()))\nt_eval = np.linspace(t_min, t_max, 200)\n\n# ---------- 3) Solve ODEs with args= (avoids per-iter lambda) ----------\n# general_case signature must be: general_case(t, y, theta)\n# We'll pass theta via args=(theta,) into solve_ivp.\n\nfrom scipy.integrate import solve_ivp\n\n# Choose a method suited to your system (try 'BDF' if stiff; otherwise 'DOP853' is fast/accurate)\nMETHOD = \"BDF\"         # try \"DOP853\" if non-stiff\nRTOL, ATOL = 1e-6, 1e-6  # consider 1e-5 / 1e-5 for more speed\n\n# ---------- 4) Collect curves, plot once via LineCollection (much cheaper) ----------\ntotal_segments = []\ndead_segments  = []\nnutr_segments  = []\n\nfor idx in idxs:\n    theta = [mu_max_all[idx], Ks_all[idx], Qn_all[idx], delta_all[idx]]\n    y0    = [N0_all[idx],     P0_all[idx], D0_all[idx]]\n\n    sol = solve_ivp(\n        fun=general_case,\n        t_span=(t_eval[0], t_eval[-1]),\n        y0=y0,\n        t_eval=t_eval,\n        rtol=RTOL,\n        atol=ATOL,\n        method=METHOD,\n        args=(theta,),\n        dense_output=False,    # faster; you already supply t_eval\n        vectorized=False       # set True only if general_case supports vectorization\n    )\n    if not sol.success:\n        continue\n\n    s = ode_solution2data(sol.y.T)  # expects shape (len(t_eval), nstates)\n\n    # Build polyline segments for a single efficient draw call per subplot\n    total_segments.append(np.column_stack((t_eval, s[\"total\"]))[:, None, :])\n    dead_segments.append (np.column_stack((t_eval, s[\"dead\"])) [:, None, :])\n    nutr_segments.append (np.column_stack((t_eval, sol.y[0]))   [:, None, :])\n\n# If nothing solved, bail early\nif len(total_segments) == 0:\n    raise RuntimeError(\"All ODE solves failed.\")\n\ntotal_segments = np.concatenate(total_segments, axis=1).transpose(1,0,2)  # (n_lines, n_pts, 2)\ndead_segments  = np.concatenate(dead_segments,  axis=1).transpose(1,0,2)\nnutr_segments  = np.concatenate(nutr_segments,  axis=1).transpose(1,0,2)\n\n# ---------- 5) Plot: one collection per panel instead of hundreds of Line2D ----------\nfig, axes = plt.subplots(1, 3, figsize=(12, 4), sharex=True)\nlabels = [\"Total cells (/ml)\", \"Dead cells (/ml)\", \"Nutrients\"]\n\n# Total\nlc0 = LineCollection(total_segments, linewidths=1.0, alpha=0.05)\naxes[0].add_collection(lc0)\naxes[0].autoscale_view()\n\n# Dead\nlc1 = LineCollection(dead_segments, linewidths=1.0, alpha=0.05)\naxes[1].add_collection(lc1)\naxes[1].autoscale_view()\n\n# Nutrients\nlc2 = LineCollection(nutr_segments, linewidths=1.0, alpha=0.05)\naxes[2].add_collection(lc2)\naxes[2].autoscale_view()\n\n# Observed data overlays (single scatter calls)\naxes[0].scatter(ehux_total_time, ehux_total_density, c=\"k\", s=20, zorder=3)\naxes[1].scatter(ehux_death_time, ehux_dead_density, c=\"k\", s=20, zorder=3)\n\n# Titles and formatting (no legend spam)\nfor i, ax in enumerate(axes):\n    ax.set_title(labels[i])\n    ax.set_xlabel(\"Time (days)\", fontsize=12)\n    ax.set_ylabel(\"Density\" if i &lt; 2 else r\"mmol N $m^{-3}$\", fontsize=12)\n    ax.set_yscale('log' if i &lt; 2 else 'linear')\n    ax.tick_params(labelsize=11)\n\nplt.tight_layout()\nplt.show()\n</pre> # --- fast, clean version (no behavior change) --- import numpy as np import matplotlib.pyplot as plt from matplotlib.collections import LineCollection import xarray as xr  n_samples = 500 burn_in = 0  # ---------- 1) Pre-compute once, avoid .flatten() in the loop ---------- posterior = trace.posterior.stack(draws=(\"chain\", \"draw\"))  if \"N0\" not in posterior:     Qn_vals = posterior[\"Qn\"].values     N0_vals = 1000 + ((500 / 1.8e-10) * (Qn_vals - 3.2e-10))     posterior[\"N0\"] = xr.DataArray(N0_vals, dims=posterior[\"Qn\"].dims, coords=posterior[\"Qn\"].coords)  # Pull 1-D numpy views once (zero copy where possible) to_1d = lambda name: np.asarray(posterior[name].values).reshape(-1) mu_max_all = to_1d(\"mu_max\") Ks_all     = to_1d(\"Ks\") Qn_all     = to_1d(\"Qn\") delta_all  = to_1d(\"delta\") N0_all     = to_1d(\"N0\") P0_all     = to_1d(\"P0\") D0_all     = to_1d(\"D0\")  available_draws = mu_max_all.shape[0] assert burn_in + n_samples &lt;= available_draws, \"Not enough samples\"  # Pick the exact indices we\u2019ll use (no Python increments in the loop) idxs = np.arange(burn_in, burn_in + n_samples)  # ---------- 2) Time grid computed once ---------- t_min = float(min(ehux_total_time.min(), ehux_death_time.min())) t_max = float(max(ehux_total_time.max(), ehux_death_time.max())) t_eval = np.linspace(t_min, t_max, 200)  # ---------- 3) Solve ODEs with args= (avoids per-iter lambda) ---------- # general_case signature must be: general_case(t, y, theta) # We'll pass theta via args=(theta,) into solve_ivp.  from scipy.integrate import solve_ivp  # Choose a method suited to your system (try 'BDF' if stiff; otherwise 'DOP853' is fast/accurate) METHOD = \"BDF\"         # try \"DOP853\" if non-stiff RTOL, ATOL = 1e-6, 1e-6  # consider 1e-5 / 1e-5 for more speed  # ---------- 4) Collect curves, plot once via LineCollection (much cheaper) ---------- total_segments = [] dead_segments  = [] nutr_segments  = []  for idx in idxs:     theta = [mu_max_all[idx], Ks_all[idx], Qn_all[idx], delta_all[idx]]     y0    = [N0_all[idx],     P0_all[idx], D0_all[idx]]      sol = solve_ivp(         fun=general_case,         t_span=(t_eval[0], t_eval[-1]),         y0=y0,         t_eval=t_eval,         rtol=RTOL,         atol=ATOL,         method=METHOD,         args=(theta,),         dense_output=False,    # faster; you already supply t_eval         vectorized=False       # set True only if general_case supports vectorization     )     if not sol.success:         continue      s = ode_solution2data(sol.y.T)  # expects shape (len(t_eval), nstates)      # Build polyline segments for a single efficient draw call per subplot     total_segments.append(np.column_stack((t_eval, s[\"total\"]))[:, None, :])     dead_segments.append (np.column_stack((t_eval, s[\"dead\"])) [:, None, :])     nutr_segments.append (np.column_stack((t_eval, sol.y[0]))   [:, None, :])  # If nothing solved, bail early if len(total_segments) == 0:     raise RuntimeError(\"All ODE solves failed.\")  total_segments = np.concatenate(total_segments, axis=1).transpose(1,0,2)  # (n_lines, n_pts, 2) dead_segments  = np.concatenate(dead_segments,  axis=1).transpose(1,0,2) nutr_segments  = np.concatenate(nutr_segments,  axis=1).transpose(1,0,2)  # ---------- 5) Plot: one collection per panel instead of hundreds of Line2D ---------- fig, axes = plt.subplots(1, 3, figsize=(12, 4), sharex=True) labels = [\"Total cells (/ml)\", \"Dead cells (/ml)\", \"Nutrients\"]  # Total lc0 = LineCollection(total_segments, linewidths=1.0, alpha=0.05) axes[0].add_collection(lc0) axes[0].autoscale_view()  # Dead lc1 = LineCollection(dead_segments, linewidths=1.0, alpha=0.05) axes[1].add_collection(lc1) axes[1].autoscale_view()  # Nutrients lc2 = LineCollection(nutr_segments, linewidths=1.0, alpha=0.05) axes[2].add_collection(lc2) axes[2].autoscale_view()  # Observed data overlays (single scatter calls) axes[0].scatter(ehux_total_time, ehux_total_density, c=\"k\", s=20, zorder=3) axes[1].scatter(ehux_death_time, ehux_dead_density, c=\"k\", s=20, zorder=3)  # Titles and formatting (no legend spam) for i, ax in enumerate(axes):     ax.set_title(labels[i])     ax.set_xlabel(\"Time (days)\", fontsize=12)     ax.set_ylabel(\"Density\" if i &lt; 2 else r\"mmol N $m^{-3}$\", fontsize=12)     ax.set_yscale('log' if i &lt; 2 else 'linear')     ax.tick_params(labelsize=11)  plt.tight_layout() plt.show()  In\u00a0[\u00a0]: Copied! <pre>\n</pre>"}]}